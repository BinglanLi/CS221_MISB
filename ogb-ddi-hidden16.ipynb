{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6beeae3c",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-06-01T05:43:08.028581Z",
     "iopub.status.busy": "2025-06-01T05:43:08.028386Z",
     "iopub.status.idle": "2025-06-01T05:43:08.034092Z",
     "shell.execute_reply": "2025-06-01T05:43:08.033501Z"
    },
    "papermill": {
     "duration": 0.010627,
     "end_time": "2025-06-01T05:43:08.035144",
     "exception": false,
     "start_time": "2025-06-01T05:43:08.024517",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists('training_outputs/'):\n",
    "    os.makedirs('training_outputs/') # directory for saving visualizations and model checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f9b6554",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-06-01T05:43:08.041056Z",
     "iopub.status.busy": "2025-06-01T05:43:08.040849Z",
     "iopub.status.idle": "2025-06-01T06:43:29.544791Z",
     "shell.execute_reply": "2025-06-01T06:43:29.544051Z"
    },
    "papermill": {
     "duration": 3621.508449,
     "end_time": "2025-06-01T06:43:29.546381",
     "exception": false,
     "start_time": "2025-06-01T05:43:08.037932",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==2.2.2\r\n",
      "  Downloading torch-2.2.2-cp311-cp311-manylinux1_x86_64.whl.metadata (25 kB)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (3.18.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (4.13.2)\r\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (1.13.1)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (2025.3.2)\r\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2.2)\r\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2.2)\r\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2.2)\r\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.2)\r\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2.2)\r\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2.2)\r\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2.2)\r\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2.2)\r\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2.2)\r\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.2)\r\n",
      "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\r\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2.2)\r\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\r\n",
      "Collecting triton==2.2.0 (from torch==2.2.2)\r\n",
      "  Downloading triton-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.2) (12.9.41)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.2.2) (3.0.2)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.2.2) (1.3.0)\r\n",
      "Downloading torch-2.2.2-cp311-cp311-manylinux1_x86_64.whl (755.6 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.6/755.6 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading triton-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch\r\n",
      "  Attempting uninstall: triton\r\n",
      "    Found existing installation: triton 3.2.0\r\n",
      "    Uninstalling triton-3.2.0:\r\n",
      "      Successfully uninstalled triton-3.2.0\r\n",
      "  Attempting uninstall: nvidia-nvtx-cu12\r\n",
      "    Found existing installation: nvidia-nvtx-cu12 12.4.127\r\n",
      "    Uninstalling nvidia-nvtx-cu12-12.4.127:\r\n",
      "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\r\n",
      "  Attempting uninstall: nvidia-nccl-cu12\r\n",
      "    Found existing installation: nvidia-nccl-cu12 2.21.5\r\n",
      "    Uninstalling nvidia-nccl-cu12-2.21.5:\r\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\r\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\r\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\r\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\r\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\r\n",
      "  Attempting uninstall: nvidia-curand-cu12\r\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.10.19\r\n",
      "    Uninstalling nvidia-curand-cu12-10.3.10.19:\r\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\r\n",
      "  Attempting uninstall: nvidia-cufft-cu12\r\n",
      "    Found existing installation: nvidia-cufft-cu12 11.4.0.6\r\n",
      "    Uninstalling nvidia-cufft-cu12-11.4.0.6:\r\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\r\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\r\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.4.127\r\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.4.127:\r\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.4.127\r\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\r\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.4.127\r\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.4.127:\r\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.4.127\r\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\r\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.4.127\r\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.4.127:\r\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.4.127\r\n",
      "  Attempting uninstall: nvidia-cublas-cu12\r\n",
      "    Found existing installation: nvidia-cublas-cu12 12.9.0.13\r\n",
      "    Uninstalling nvidia-cublas-cu12-12.9.0.13:\r\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\r\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\r\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\r\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\r\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\r\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\r\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\r\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\r\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\r\n",
      "  Attempting uninstall: torch\r\n",
      "    Found existing installation: torch 2.6.0+cu124\r\n",
      "    Uninstalling torch-2.6.0+cu124:\r\n",
      "      Successfully uninstalled torch-2.6.0+cu124\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.2.2 which is incompatible.\r\n",
      "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.2.2 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvtx-cu12-12.1.105 torch-2.2.2 triton-2.2.0\r\n",
      "Collecting torch-cluster==1.6.3\r\n",
      "  Downloading torch_cluster-1.6.3.tar.gz (54 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-cluster==1.6.3) (1.15.2)\r\n",
      "Requirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy->torch-cluster==1.6.3) (1.26.4)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy->torch-cluster==1.6.3) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy->torch-cluster==1.6.3) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy->torch-cluster==1.6.3) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy->torch-cluster==1.6.3) (2025.1.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy->torch-cluster==1.6.3) (2022.1.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy->torch-cluster==1.6.3) (2.4.1)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.5,>=1.23.5->scipy->torch-cluster==1.6.3) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.5,>=1.23.5->scipy->torch-cluster==1.6.3) (2022.1.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2.5,>=1.23.5->scipy->torch-cluster==1.6.3) (1.3.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2.5,>=1.23.5->scipy->torch-cluster==1.6.3) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2.5,>=1.23.5->scipy->torch-cluster==1.6.3) (2024.2.0)\r\n",
      "Building wheels for collected packages: torch-cluster\r\n",
      "  Building wheel for torch-cluster (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for torch-cluster: filename=torch_cluster-1.6.3-cp311-cp311-linux_x86_64.whl size=2043683 sha256=982ea8ce4f6775f7e99cb6b365bfc1c91c4875e8aa9691d5101f1cee08bfbe40\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/ef/de/7d/a4211822af99147b93800e9e204f0be21294e3c0b95b3b861a\r\n",
      "Successfully built torch-cluster\r\n",
      "Installing collected packages: torch-cluster\r\n",
      "Successfully installed torch-cluster-1.6.3\r\n",
      "Collecting torch-geometric==2.6.1\r\n",
      "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.6.1) (3.11.18)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.6.1) (2025.3.2)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.6.1) (3.1.6)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.6.1) (1.26.4)\r\n",
      "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.6.1) (7.0.0)\r\n",
      "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.6.1) (3.0.9)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.6.1) (2.32.3)\r\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.6.1) (4.67.1)\r\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.6.1) (2.6.1)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.6.1) (1.3.2)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.6.1) (25.3.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.6.1) (1.6.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.6.1) (6.4.3)\r\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.6.1) (0.3.1)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.6.1) (1.20.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric==2.6.1) (3.0.2)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric==2.6.1) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric==2.6.1) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric==2.6.1) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric==2.6.1) (2025.1.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric==2.6.1) (2022.1.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric==2.6.1) (2.4.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric==2.6.1) (3.4.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric==2.6.1) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric==2.6.1) (2.4.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric==2.6.1) (2025.4.26)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torch-geometric==2.6.1) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torch-geometric==2.6.1) (2022.1.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torch-geometric==2.6.1) (1.3.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torch-geometric==2.6.1) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torch-geometric==2.6.1) (2024.2.0)\r\n",
      "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: torch-geometric\r\n",
      "Successfully installed torch-geometric-2.6.1\r\n",
      "Collecting torch-scatter==2.1.2\r\n",
      "  Downloading torch_scatter-2.1.2.tar.gz (108 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.0/108.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Building wheels for collected packages: torch-scatter\r\n",
      "  Building wheel for torch-scatter (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for torch-scatter: filename=torch_scatter-2.1.2-cp311-cp311-linux_x86_64.whl size=3856295 sha256=549057d23ea37312bbfd8b8e00dfbdb81aca274197a00d57057e7714b42b81a8\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/b8/d4/0e/a80af2465354ea7355a2c153b11af2da739cfcf08b6c0b28e2\r\n",
      "Successfully built torch-scatter\r\n",
      "Installing collected packages: torch-scatter\r\n",
      "Successfully installed torch-scatter-2.1.2\r\n",
      "Collecting torch-sparse==0.6.18\r\n",
      "  Downloading torch_sparse-0.6.18.tar.gz (209 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.0/210.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-sparse==0.6.18) (1.15.2)\r\n",
      "Requirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy->torch-sparse==0.6.18) (1.26.4)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy->torch-sparse==0.6.18) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy->torch-sparse==0.6.18) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy->torch-sparse==0.6.18) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy->torch-sparse==0.6.18) (2025.1.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy->torch-sparse==0.6.18) (2022.1.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy->torch-sparse==0.6.18) (2.4.1)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.5,>=1.23.5->scipy->torch-sparse==0.6.18) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.5,>=1.23.5->scipy->torch-sparse==0.6.18) (2022.1.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2.5,>=1.23.5->scipy->torch-sparse==0.6.18) (1.3.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2.5,>=1.23.5->scipy->torch-sparse==0.6.18) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2.5,>=1.23.5->scipy->torch-sparse==0.6.18) (2024.2.0)\r\n",
      "Building wheels for collected packages: torch-sparse\r\n",
      "  Building wheel for torch-sparse (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for torch-sparse: filename=torch_sparse-0.6.18-cp311-cp311-linux_x86_64.whl size=2669679 sha256=fd6f191cce36e66f148a6c5e710b717c47746714f43a1076492c3b4a3625abaf\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/75/e2/1e/299c596063839303657c211f587f05591891cc6cf126d94d21\r\n",
      "Successfully built torch-sparse\r\n",
      "Installing collected packages: torch-sparse\r\n",
      "Successfully installed torch-sparse-0.6.18\r\n",
      "Collecting torch-spline-conv==1.2.2\r\n",
      "  Downloading torch_spline_conv-1.2.2.tar.gz (25 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Building wheels for collected packages: torch-spline-conv\r\n",
      "  Building wheel for torch-spline-conv (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for torch-spline-conv: filename=torch_spline_conv-1.2.2-cp311-cp311-linux_x86_64.whl size=551998 sha256=b1510f7b001579a3ef7d8470d630ccfe15fa9335573f682615e7bc632d050773\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/25/16/8a/a98b0173c4fbbc7aa1c4929b46d2eb08d1475c5c7b54e289b6\r\n",
      "Successfully built torch-spline-conv\r\n",
      "Installing collected packages: torch-spline-conv\r\n",
      "Successfully installed torch-spline-conv-1.2.2\r\n",
      "Collecting ogb\r\n",
      "  Downloading ogb-1.3.6-py3-none-any.whl.metadata (6.2 kB)\r\n",
      "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from ogb) (2.2.2)\r\n",
      "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from ogb) (1.26.4)\r\n",
      "Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.11/dist-packages (from ogb) (4.67.1)\r\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from ogb) (1.2.2)\r\n",
      "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from ogb) (2.2.3)\r\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from ogb) (1.17.0)\r\n",
      "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.11/dist-packages (from ogb) (2.4.0)\r\n",
      "Collecting outdated>=0.2.0 (from ogb)\r\n",
      "  Downloading outdated-0.2.2-py2.py3-none-any.whl.metadata (4.7 kB)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.0->ogb) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.0->ogb) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.0->ogb) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.0->ogb) (2025.1.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.0->ogb) (2022.1.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.0->ogb) (2.4.1)\r\n",
      "Requirement already satisfied: setuptools>=44 in /usr/local/lib/python3.11/dist-packages (from outdated>=0.2.0->ogb) (75.2.0)\r\n",
      "Collecting littleutils (from outdated>=0.2.0->ogb)\r\n",
      "  Downloading littleutils-0.2.4-py3-none-any.whl.metadata (679 bytes)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from outdated>=0.2.0->ogb) (2.32.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24.0->ogb) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24.0->ogb) (2025.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24.0->ogb) (2025.2)\r\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.0->ogb) (1.15.2)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.0->ogb) (1.5.0)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.0->ogb) (3.6.0)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (3.18.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (4.13.2)\r\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (1.13.1)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (2025.3.2)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (8.9.2.26)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (12.1.3.1)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (11.0.2.54)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (10.3.2.106)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (11.4.5.107)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (12.1.0.106)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (2.19.3)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (12.1.105)\r\n",
      "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (2.2.0)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.6.0->ogb) (12.9.41)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.6.0->ogb) (3.0.2)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.16.0->ogb) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.16.0->ogb) (2022.1.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.16.0->ogb) (1.3.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.16.0->ogb) (2024.2.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->outdated>=0.2.0->ogb) (3.4.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->outdated>=0.2.0->ogb) (3.10)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->outdated>=0.2.0->ogb) (2025.4.26)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch>=1.6.0->ogb) (1.3.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.16.0->ogb) (2024.2.0)\r\n",
      "Downloading ogb-1.3.6-py3-none-any.whl (78 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\r\n",
      "Downloading littleutils-0.2.4-py3-none-any.whl (8.1 kB)\r\n",
      "Installing collected packages: littleutils, outdated, ogb\r\n",
      "Successfully installed littleutils-0.2.4 ogb-1.3.6 outdated-0.2.2\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy) (2025.1.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy) (2022.1.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy) (2.4.1)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2022.1.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy) (1.3.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy) (2024.2.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==2.2.2\n",
    "!pip install torch-cluster==1.6.3\n",
    "!pip install torch-geometric==2.6.1\n",
    "!pip install torch-scatter==2.1.2\n",
    "!pip install torch-sparse==0.6.18\n",
    "!pip install torch-spline-conv==1.2.2\n",
    "\n",
    "!pip install ogb\n",
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c243561",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-06-01T06:43:29.641082Z",
     "iopub.status.busy": "2025-06-01T06:43:29.640800Z",
     "iopub.status.idle": "2025-06-01T06:43:39.638133Z",
     "shell.execute_reply": "2025-06-01T06:43:39.637306Z"
    },
    "papermill": {
     "duration": 10.045072,
     "end_time": "2025-06-01T06:43:39.639606",
     "exception": false,
     "start_time": "2025-06-01T06:43:29.594534",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Sequential, Linear, BatchNorm1d, ReLU\n",
    "\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GCNConv, SAGEConv, GINConv, GATConv\n",
    "\n",
    "from ogb.linkproppred import PygLinkPropPredDataset, Evaluator\n",
    "# import custom dataset_pyg to avoid weights_only errors\n",
    "# from dataset_pyg import PygLinkPropPredDataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data.data import DataEdgeAttr, GlobalStorage, DataTensorAttr\n",
    "\n",
    "from torch_geometric.nn import Node2Vec\n",
    "\n",
    "import pandas as pd\n",
    "import shutil, os\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "\n",
    "#from logger import Logger\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a4256ca",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-06-01T06:43:39.733111Z",
     "iopub.status.busy": "2025-06-01T06:43:39.732320Z",
     "iopub.status.idle": "2025-06-01T06:43:39.736625Z",
     "shell.execute_reply": "2025-06-01T06:43:39.735976Z"
    },
    "papermill": {
     "duration": 0.051303,
     "end_time": "2025-06-01T06:43:39.737616",
     "exception": false,
     "start_time": "2025-06-01T06:43:39.686313",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# If you use GPU, the device should be cuda\n",
    "print('Device: {}'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f83abe2",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-06-01T06:43:39.829526Z",
     "iopub.status.busy": "2025-06-01T06:43:39.829069Z",
     "iopub.status.idle": "2025-06-01T06:43:39.835317Z",
     "shell.execute_reply": "2025-06-01T06:43:39.834615Z"
    },
    "papermill": {
     "duration": 0.053431,
     "end_time": "2025-06-01T06:43:39.836491",
     "exception": false,
     "start_time": "2025-06-01T06:43:39.783060",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class using PyG's GATConv layer\n",
    "class GAN(torch.nn.Module):\n",
    "    ''' Define graph isomorphic network. '''\n",
    "    def __init__(self, in_channels, hidden_size, out_channels, in_head,\n",
    "                 num_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.out_head = 1\n",
    "        self.dropout = dropout\n",
    "        # Initialize 2 GATConv layers\n",
    "        self.conv1 = GATConv(in_channels, hidden_size, heads=in_head,\n",
    "                             dropout=self.dropout)\n",
    "        self.conv2 = GATConv(hidden_size*in_head, out_channels, concat=False,\n",
    "                             heads=self.out_head, dropout=self.dropout)\n",
    "\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.conv1.reset_parameters()\n",
    "        self.conv2.reset_parameters()\n",
    "\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # Execute conv -> relu -> dropout sequence\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f10091c5",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-06-01T06:43:39.928023Z",
     "iopub.status.busy": "2025-06-01T06:43:39.927293Z",
     "iopub.status.idle": "2025-06-01T06:43:39.933497Z",
     "shell.execute_reply": "2025-06-01T06:43:39.932809Z"
    },
    "papermill": {
     "duration": 0.053261,
     "end_time": "2025-06-01T06:43:39.934523",
     "exception": false,
     "start_time": "2025-06-01T06:43:39.881262",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class in order to predict whether a link exists between two nodes using\n",
    "# their embeddings, x_i and x_j\n",
    "class LinkPredictor(torch.nn.Module):\n",
    "    ''' Neural network which predicts whether a link (interaction) exists between 2 nodes i,j\n",
    "    given their embeddings x_i, x_j.\n",
    "    '''\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n",
    "                 dropout):\n",
    "        super(LinkPredictor, self).__init__()\n",
    "\n",
    "        self.lins = torch.nn.ModuleList()\n",
    "        self.lins.append(torch.nn.Linear(in_channels, hidden_channels))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.lins.append(torch.nn.Linear(hidden_channels, hidden_channels))\n",
    "        self.lins.append(torch.nn.Linear(hidden_channels, out_channels))\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for lin in self.lins:\n",
    "            lin.reset_parameters()\n",
    "\n",
    "    def forward(self, x_i, x_j):\n",
    "        x = x_i * x_j # hadamard product\n",
    "        for lin in self.lins[:-1]: # linear layer -> relu -> dropout\n",
    "            x = lin(x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.lins[-1](x)\n",
    "        return torch.sigmoid(x) # sigmoid activation outputs probability that a given edge exists for all node pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83d5fdda",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-06-01T06:43:40.026391Z",
     "iopub.status.busy": "2025-06-01T06:43:40.025751Z",
     "iopub.status.idle": "2025-06-01T06:43:40.038555Z",
     "shell.execute_reply": "2025-06-01T06:43:40.037997Z"
    },
    "papermill": {
     "duration": 0.060061,
     "end_time": "2025-06-01T06:43:40.039720",
     "exception": false,
     "start_time": "2025-06-01T06:43:39.979659",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, predictor, x, adj_t, split_edge, optimizer, batch_size):\n",
    "\n",
    "    row, col, _ = adj_t.coo()\n",
    "    edge_index = torch.stack([col, row], dim=0)\n",
    "\n",
    "    model.train()\n",
    "    predictor.train()\n",
    "\n",
    "    pos_train_edge = split_edge['train']['edge'].to(x.device)\n",
    "\n",
    "    total_loss = total_examples = 0\n",
    "    for perm in DataLoader(range(pos_train_edge.size(0)), batch_size,\n",
    "                           shuffle=True):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        h = model(x, adj_t)\n",
    "\n",
    "        edge = pos_train_edge[perm].t()\n",
    "\n",
    "        # computes the loss for positive edges\n",
    "        pos_out = predictor(h[edge[0]], h[edge[1]])\n",
    "        pos_loss = -torch.log(pos_out + 1e-15).mean()\n",
    "\n",
    "        # samples negative edges from the graph\n",
    "        edge = negative_sampling(edge_index, num_nodes=x.size(0),\n",
    "                                 num_neg_samples=perm.size(0), method='dense')\n",
    "\n",
    "        # computes the loss for negative edges\n",
    "        neg_out = predictor(h[edge[0]], h[edge[1]])\n",
    "        neg_loss = -torch.log(1 - neg_out + 1e-15).mean()\n",
    "\n",
    "        loss = pos_loss + neg_loss\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(x, 1.0)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        torch.nn.utils.clip_grad_norm_(predictor.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        num_examples = pos_out.size(0)\n",
    "        total_loss += loss.item() * num_examples\n",
    "        total_examples += num_examples\n",
    "\n",
    "    return total_loss / total_examples\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, predictor, x, adj_t, split_edge, evaluator, batch_size):\n",
    "    model.eval()\n",
    "    predictor.eval()\n",
    "\n",
    "    h = model(x, adj_t)\n",
    "\n",
    "    pos_train_edge = split_edge['eval_train']['edge'].to(x.device)\n",
    "    pos_valid_edge = split_edge['valid']['edge'].to(x.device)\n",
    "    neg_valid_edge = split_edge['valid']['edge_neg'].to(x.device)\n",
    "    pos_test_edge = split_edge['test']['edge'].to(x.device)\n",
    "    neg_test_edge = split_edge['test']['edge_neg'].to(x.device)\n",
    "\n",
    "    # store what the link predictor outputs for each positive and negative\n",
    "    # edge in order to compute the hits@K\n",
    "    pos_train_preds = []\n",
    "    for perm in DataLoader(range(pos_train_edge.size(0)), batch_size):\n",
    "        edge = pos_train_edge[perm].t()\n",
    "        pos_train_preds += [predictor(h[edge[0]], h[edge[1]]).squeeze().cpu()]\n",
    "    pos_train_pred = torch.cat(pos_train_preds, dim=0)\n",
    "\n",
    "    pos_valid_preds = []\n",
    "    for perm in DataLoader(range(pos_valid_edge.size(0)), batch_size):\n",
    "        edge = pos_valid_edge[perm].t()\n",
    "        pos_valid_preds += [predictor(h[edge[0]], h[edge[1]]).squeeze().cpu()]\n",
    "    pos_valid_pred = torch.cat(pos_valid_preds, dim=0)\n",
    "\n",
    "    neg_valid_preds = []\n",
    "    for perm in DataLoader(range(neg_valid_edge.size(0)), batch_size):\n",
    "        edge = neg_valid_edge[perm].t()\n",
    "        neg_valid_preds += [predictor(h[edge[0]], h[edge[1]]).squeeze().cpu()]\n",
    "    neg_valid_pred = torch.cat(neg_valid_preds, dim=0)\n",
    "\n",
    "    pos_test_preds = []\n",
    "    for perm in DataLoader(range(pos_test_edge.size(0)), batch_size):\n",
    "        edge = pos_test_edge[perm].t()\n",
    "        pos_test_preds += [predictor(h[edge[0]], h[edge[1]]).squeeze().cpu()]\n",
    "    pos_test_pred = torch.cat(pos_test_preds, dim=0)\n",
    "\n",
    "    neg_test_preds = []\n",
    "    for perm in DataLoader(range(neg_test_edge.size(0)), batch_size):\n",
    "        edge = neg_test_edge[perm].t()\n",
    "        neg_test_preds += [predictor(h[edge[0]], h[edge[1]]).squeeze().cpu()]\n",
    "    neg_test_pred = torch.cat(neg_test_preds, dim=0)\n",
    "\n",
    "    # compute the hits@K for training, validation, and test\n",
    "    results = {}\n",
    "    for K in [10, 20]:\n",
    "        evaluator.K = K\n",
    "        train_hits = evaluator.eval({\n",
    "            'y_pred_pos': pos_train_pred,\n",
    "            'y_pred_neg': neg_valid_pred,\n",
    "        })[f'hits@{K}']\n",
    "        valid_hits = evaluator.eval({\n",
    "            'y_pred_pos': pos_valid_pred,\n",
    "            'y_pred_neg': neg_valid_pred,\n",
    "        })[f'hits@{K}']\n",
    "        test_hits = evaluator.eval({\n",
    "            'y_pred_pos': pos_test_pred,\n",
    "            'y_pred_neg': neg_test_pred,\n",
    "        })[f'hits@{K}']\n",
    "\n",
    "        results[f'Hits@{K}'] = (train_hits, valid_hits, test_hits)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3aca305",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-06-01T06:43:40.134240Z",
     "iopub.status.busy": "2025-06-01T06:43:40.133990Z",
     "iopub.status.idle": "2025-06-01T06:43:40.142649Z",
     "shell.execute_reply": "2025-06-01T06:43:40.141965Z"
    },
    "papermill": {
     "duration": 0.055659,
     "end_time": "2025-06-01T06:43:40.143713",
     "exception": false,
     "start_time": "2025-06-01T06:43:40.088054",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(model, emb, gnn_args, predictor, model_name):\n",
    "  '''\n",
    "  Train specified GNN model. Model and embeddings should be initialized.\n",
    "  Save model after every run.\n",
    "  '''\n",
    "  train_hits_arr, val_hits_arr, test_hits_arr = [], [], []\n",
    "\n",
    "  evaluator = Evaluator(name='ogbl-ddi')\n",
    "  for run in range(2):\n",
    "    max_valhits, train_hits_run, test_hits_run = float('-inf'), 0, 0\n",
    "\n",
    "    torch.nn.init.xavier_uniform_(emb.weight)\n",
    "    model.reset_parameters()\n",
    "    predictor.reset_parameters()\n",
    "    optimizer = torch.optim.Adam(\n",
    "        list(model.parameters()) + list(emb.parameters()) +\n",
    "        list(predictor.parameters()), lr=gnn_args['lr'])\n",
    "\n",
    "    for epoch in range(1, 1 + gnn_args['epochs']):\n",
    "        loss = train(model, predictor, emb.weight, adj_t, split_edge,\n",
    "                      optimizer, gnn_args['batch_size'])\n",
    "\n",
    "        if epoch % gnn_args['eval_steps'] == 0:\n",
    "            results = test(model, predictor, emb.weight, adj_t, split_edge,\n",
    "                            evaluator, gnn_args['batch_size'])\n",
    "\n",
    "\n",
    "            if epoch % gnn_args['log_steps'] == 0:\n",
    "                for key, result in results.items():\n",
    "                    train_hits, valid_hits, test_hits = result\n",
    "                    print(key)\n",
    "                    print(f'Run: {run + 1:02d}, '\n",
    "                          f'Epoch: {epoch:02d}, '\n",
    "                          f'Loss: {loss:.4f}, '\n",
    "                          f'Train: {100 * train_hits:.2f}%, '\n",
    "                          f'Valid: {100 * valid_hits:.2f}%, '\n",
    "                          f'Test: {100 * test_hits:.2f}%')\n",
    "                print('---')\n",
    "\n",
    "            # check val-hits@20\n",
    "            train_hits, valid_hits, test_hits = results['Hits@20']\n",
    "            if valid_hits >= max_valhits: # if validhits20 is higher than max, save ckpt\n",
    "              max_valhits = valid_hits\n",
    "              train_hits_run = train_hits\n",
    "              test_hits_run = test_hits\n",
    "              # Save model checkpoint for current run.\n",
    "              model_path = f\"training_outputs/{model_name}.pt\"\n",
    "              emb_path = f'training_outputs/{model_name}_init_emb.pt'\n",
    "              save_model_ckpt(model, emb, optimizer, predictor, loss, emb_path, model_path)\n",
    "    train_hits_arr.append(train_hits_run)\n",
    "    test_hits_arr.append(test_hits_run)\n",
    "    val_hits_arr.append(max_valhits)\n",
    "\n",
    "\n",
    "  # Print overall stats arrays for best model based on val hits@20\n",
    "  print(\"Val_hits@20: \", val_hits_arr)\n",
    "  print(\"Test_hits@20: \", test_hits_arr)\n",
    "  print(\"Train_hits@20: \", train_hits_arr)\n",
    "\n",
    "  # Print best model stats (based on val hits@20)\n",
    "  val_max = max(val_hits_arr)\n",
    "  print(\"Best model val hits@20: \", max(val_hits_arr))\n",
    "  max_idx = val_hits_arr.index(val_max)\n",
    "  print('Best model test hits@20: ', test_hits_arr[max_idx])\n",
    "  print('Best model train hits@20: ', val_hits_arr[max_idx])\n",
    "\n",
    "  # convert to numpy array\n",
    "  val_hits_arr = np.array(val_hits_arr)\n",
    "  test_hits_arr = np.array(test_hits_arr)\n",
    "  train_hits_arr = np.array(train_hits_arr)\n",
    "\n",
    "  # Print average stats + variance\n",
    "  print(f\"Average best train hits@20: {np.mean(train_hits_arr)}; var: {np.var(train_hits_arr)}\")\n",
    "  print(f\"Average best val hits@20: {np.mean(val_hits_arr)}; var: {np.var(val_hits_arr)}\")\n",
    "  print(f\"Average best test hits@20: {np.mean(test_hits_arr)}; var: {np.var(test_hits_arr)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aedf7272",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-06-01T06:43:40.247537Z",
     "iopub.status.busy": "2025-06-01T06:43:40.247225Z",
     "iopub.status.idle": "2025-06-01T06:43:40.251938Z",
     "shell.execute_reply": "2025-06-01T06:43:40.251146Z"
    },
    "papermill": {
     "duration": 0.06525,
     "end_time": "2025-06-01T06:43:40.253473",
     "exception": false,
     "start_time": "2025-06-01T06:43:40.188223",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_model_ckpt(model, emb, optimizer, predictor, loss, emb_path, model_path):\n",
    "  ''' Save model and embedding checkpoints. '''\n",
    "  EPOCH = 100\n",
    "  # Save model params\n",
    "  torch.save({\n",
    "            'epoch': EPOCH,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'predictor_state_dict': predictor.state_dict(),\n",
    "            'loss': loss,\n",
    "            }, model_path)\n",
    "  # Also save initial embedding (just in case)\n",
    "  torch.save(emb.weight.data.cpu(), emb_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02283dc9",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-06-01T06:43:40.356779Z",
     "iopub.status.busy": "2025-06-01T06:43:40.356498Z",
     "iopub.status.idle": "2025-06-01T06:43:42.437253Z",
     "shell.execute_reply": "2025-06-01T06:43:42.436430Z"
    },
    "papermill": {
     "duration": 2.127605,
     "end_time": "2025-06-01T06:43:42.438681",
     "exception": false,
     "start_time": "2025-06-01T06:43:40.311076",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://snap.stanford.edu/ogb/data/linkproppred/ddi.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloaded 0.04 GB: 100%|██████████| 46/46 [00:00<00:00, 52.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dataset/ddi.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading necessary files...\n",
      "This might take a while.\n",
      "Processing graphs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 56.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting graphs into PyG objects...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 3618.90it/s]\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving...\n",
      "Task type: link prediction\n"
     ]
    }
   ],
   "source": [
    "# loaded with transform parameter set as such in order to obtain the adj_t matrix\n",
    "# required for the GNN layers\n",
    "# torch.serialization.add_safe_globals([DataEdgeAttr])\n",
    "# torch.serialization.add_safe_globals([DataTensorAttr])\n",
    "# torch.serialization.add_safe_globals([GlobalStorage])\n",
    "dataset = PygLinkPropPredDataset(name='ogbl-ddi', transform=T.ToSparseTensor()) # loading ogb-ddi\n",
    "print('Task type: {}'.format(dataset.task_type))\n",
    "graph = dataset[0]\n",
    "adj_t = graph.adj_t.to(device) # loads all edges in graph into sparse adj_t matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3837b147",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-06-01T06:43:42.535158Z",
     "iopub.status.busy": "2025-06-01T06:43:42.534914Z",
     "iopub.status.idle": "2025-06-01T06:43:42.557826Z",
     "shell.execute_reply": "2025-06-01T06:43:42.557308Z"
    },
    "papermill": {
     "duration": 0.071981,
     "end_time": "2025-06-01T06:43:42.558871",
     "exception": false,
     "start_time": "2025-06-01T06:43:42.486890",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dataset reloaded without the transform parameter to obtain the data in the\n",
    "# correct format\n",
    "dataset = PygLinkPropPredDataset(name='ogbl-ddi')\n",
    "data = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42e0a343",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-06-01T06:43:42.656938Z",
     "iopub.status.busy": "2025-06-01T06:43:42.656713Z",
     "iopub.status.idle": "2025-06-01T06:43:42.762194Z",
     "shell.execute_reply": "2025-06-01T06:43:42.761395Z"
    },
    "papermill": {
     "duration": 0.15557,
     "end_time": "2025-06-01T06:43:42.763545",
     "exception": false,
     "start_time": "2025-06-01T06:43:42.607975",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# getting the train, validation, and test edge splits\n",
    "split_edge = dataset.get_edge_split()\n",
    "train_edges = split_edge['train']['edge']\n",
    "torch.manual_seed(70) # picking random samples to evaluate on\n",
    "idx = torch.randperm(split_edge['train']['edge'].size(0))\n",
    "idx = idx[:split_edge['valid']['edge'].size(0)]\n",
    "split_edge['eval_train'] = {'edge': split_edge['train']['edge'][idx]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f2f9af3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T06:43:42.859596Z",
     "iopub.status.busy": "2025-06-01T06:43:42.859353Z",
     "iopub.status.idle": "2025-06-01T06:43:42.863368Z",
     "shell.execute_reply": "2025-06-01T06:43:42.862679Z"
    },
    "papermill": {
     "duration": 0.052617,
     "end_time": "2025-06-01T06:43:42.864507",
     "exception": false,
     "start_time": "2025-06-01T06:43:42.811890",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gnn_args = { # define GNN hyperparams\n",
    "    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    'hidden_size': 16,\n",
    "    'dropout': 0.5,\n",
    "    'epochs': 100,\n",
    "    'weight_decay': 1e-5,\n",
    "    'lr': 0.005,\n",
    "    'attn_size': 32,\n",
    "    'attn_head': 1,\n",
    "    'num_layers':2,\n",
    "    'log_steps':1,\n",
    "    'eval_steps':5,\n",
    "    'runs':10,\n",
    "    'batch_size': 1024,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e96f2e0c",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-06-01T06:43:42.959106Z",
     "iopub.status.busy": "2025-06-01T06:43:42.958863Z",
     "iopub.status.idle": "2025-06-01T08:56:00.021796Z",
     "shell.execute_reply": "2025-06-01T08:56:00.021097Z"
    },
    "papermill": {
     "duration": 7937.1617,
     "end_time": "2025-06-01T08:56:00.073317",
     "exception": false,
     "start_time": "2025-06-01T06:43:42.911617",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hits@10\n",
      "Run: 01, Epoch: 05, Loss: 0.6390, Train: 13.96%, Valid: 13.31%, Test: 5.98%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 05, Loss: 0.6390, Train: 20.27%, Valid: 19.62%, Test: 9.93%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 10, Loss: 0.5835, Train: 12.90%, Valid: 12.38%, Test: 4.15%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 10, Loss: 0.5835, Train: 17.36%, Valid: 16.80%, Test: 9.07%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 15, Loss: 0.5541, Train: 13.88%, Valid: 13.28%, Test: 7.03%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 15, Loss: 0.5541, Train: 21.19%, Valid: 20.48%, Test: 9.72%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 20, Loss: 0.5378, Train: 17.54%, Valid: 16.59%, Test: 5.35%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 20, Loss: 0.5378, Train: 21.03%, Valid: 19.99%, Test: 6.87%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 25, Loss: 0.5236, Train: 14.96%, Valid: 14.16%, Test: 4.83%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 25, Loss: 0.5236, Train: 20.76%, Valid: 19.76%, Test: 6.62%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 30, Loss: 0.5163, Train: 13.78%, Valid: 12.94%, Test: 3.84%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 30, Loss: 0.5163, Train: 16.08%, Valid: 15.16%, Test: 6.66%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 35, Loss: 0.5082, Train: 15.42%, Valid: 14.63%, Test: 5.52%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 35, Loss: 0.5082, Train: 18.42%, Valid: 17.47%, Test: 8.46%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 40, Loss: 0.5010, Train: 14.51%, Valid: 13.69%, Test: 6.59%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 40, Loss: 0.5010, Train: 17.74%, Valid: 16.82%, Test: 9.31%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 45, Loss: 0.4970, Train: 15.95%, Valid: 15.08%, Test: 7.58%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 45, Loss: 0.4970, Train: 19.22%, Valid: 18.20%, Test: 9.85%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 50, Loss: 0.4934, Train: 13.22%, Valid: 12.53%, Test: 5.62%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 50, Loss: 0.4934, Train: 18.88%, Valid: 17.93%, Test: 7.77%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 55, Loss: 0.4912, Train: 9.01%, Valid: 8.50%, Test: 3.72%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 55, Loss: 0.4912, Train: 14.87%, Valid: 14.09%, Test: 6.49%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 60, Loss: 0.4878, Train: 12.07%, Valid: 11.29%, Test: 3.75%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 60, Loss: 0.4878, Train: 17.69%, Valid: 16.68%, Test: 7.28%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 65, Loss: 0.4869, Train: 15.07%, Valid: 14.20%, Test: 6.36%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 65, Loss: 0.4869, Train: 20.02%, Valid: 18.95%, Test: 9.65%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 70, Loss: 0.4836, Train: 16.23%, Valid: 15.28%, Test: 5.60%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 70, Loss: 0.4836, Train: 21.91%, Valid: 20.88%, Test: 8.85%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 75, Loss: 0.4841, Train: 14.24%, Valid: 13.39%, Test: 4.55%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 75, Loss: 0.4841, Train: 19.75%, Valid: 18.67%, Test: 8.56%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 80, Loss: 0.4825, Train: 16.83%, Valid: 15.91%, Test: 4.64%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 80, Loss: 0.4825, Train: 20.59%, Valid: 19.60%, Test: 7.90%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 85, Loss: 0.4822, Train: 19.01%, Valid: 17.88%, Test: 5.45%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 85, Loss: 0.4822, Train: 22.53%, Valid: 21.42%, Test: 7.99%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 90, Loss: 0.4821, Train: 18.57%, Valid: 17.48%, Test: 6.42%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 90, Loss: 0.4821, Train: 22.97%, Valid: 21.86%, Test: 9.31%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 95, Loss: 0.4837, Train: 16.58%, Valid: 15.65%, Test: 6.05%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 95, Loss: 0.4837, Train: 23.69%, Valid: 22.54%, Test: 9.49%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 100, Loss: 0.4830, Train: 16.29%, Valid: 15.40%, Test: 4.79%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 100, Loss: 0.4830, Train: 21.27%, Valid: 20.22%, Test: 8.33%\n",
      "---\n",
      "Hits@10\n",
      "Run: 02, Epoch: 05, Loss: 0.6216, Train: 9.47%, Valid: 9.13%, Test: 3.27%\n",
      "Hits@20\n",
      "Run: 02, Epoch: 05, Loss: 0.6216, Train: 11.84%, Valid: 11.43%, Test: 4.53%\n",
      "---\n",
      "Hits@10\n",
      "Run: 02, Epoch: 10, Loss: 0.5630, Train: 17.64%, Valid: 17.07%, Test: 8.92%\n",
      "Hits@20\n",
      "Run: 02, Epoch: 10, Loss: 0.5630, Train: 20.30%, Valid: 19.79%, Test: 12.48%\n",
      "---\n",
      "Hits@10\n",
      "Run: 02, Epoch: 15, Loss: 0.5344, Train: 18.00%, Valid: 17.16%, Test: 7.08%\n",
      "Hits@20\n",
      "Run: 02, Epoch: 15, Loss: 0.5344, Train: 22.12%, Valid: 21.43%, Test: 11.10%\n",
      "---\n",
      "Hits@10\n",
      "Run: 02, Epoch: 20, Loss: 0.5266, Train: 13.67%, Valid: 12.99%, Test: 5.82%\n",
      "Hits@20\n",
      "Run: 02, Epoch: 20, Loss: 0.5266, Train: 18.02%, Valid: 17.41%, Test: 8.55%\n",
      "---\n",
      "Hits@10\n",
      "Run: 02, Epoch: 25, Loss: 0.5207, Train: 14.58%, Valid: 13.65%, Test: 4.72%\n",
      "Hits@20\n",
      "Run: 02, Epoch: 25, Loss: 0.5207, Train: 20.22%, Valid: 19.18%, Test: 8.00%\n",
      "---\n",
      "Hits@10\n",
      "Run: 02, Epoch: 30, Loss: 0.5173, Train: 18.03%, Valid: 17.20%, Test: 5.02%\n",
      "Hits@20\n",
      "Run: 02, Epoch: 30, Loss: 0.5173, Train: 22.44%, Valid: 21.65%, Test: 9.02%\n",
      "---\n",
      "Hits@10\n",
      "Run: 02, Epoch: 35, Loss: 0.5135, Train: 20.82%, Valid: 20.15%, Test: 11.68%\n",
      "Hits@20\n",
      "Run: 02, Epoch: 35, Loss: 0.5135, Train: 25.69%, Valid: 25.03%, Test: 16.01%\n",
      "---\n",
      "Hits@10\n",
      "Run: 02, Epoch: 40, Loss: 0.5125, Train: 18.41%, Valid: 17.57%, Test: 4.84%\n",
      "Hits@20\n",
      "Run: 02, Epoch: 40, Loss: 0.5125, Train: 21.63%, Valid: 20.89%, Test: 10.20%\n",
      "---\n",
      "Hits@10\n",
      "Run: 02, Epoch: 45, Loss: 0.5115, Train: 21.28%, Valid: 20.39%, Test: 8.79%\n",
      "Hits@20\n",
      "Run: 02, Epoch: 45, Loss: 0.5115, Train: 25.97%, Valid: 25.17%, Test: 14.82%\n",
      "---\n",
      "Hits@10\n",
      "Run: 02, Epoch: 50, Loss: 0.5073, Train: 24.45%, Valid: 23.25%, Test: 6.16%\n",
      "Hits@20\n",
      "Run: 02, Epoch: 50, Loss: 0.5073, Train: 27.82%, Valid: 26.71%, Test: 15.39%\n",
      "---\n",
      "Hits@10\n",
      "Run: 02, Epoch: 55, Loss: 0.5068, Train: 21.33%, Valid: 20.32%, Test: 5.04%\n",
      "Hits@20\n",
      "Run: 02, Epoch: 55, Loss: 0.5068, Train: 27.59%, Valid: 26.58%, Test: 14.88%\n",
      "---\n",
      "Hits@10\n",
      "Run: 02, Epoch: 60, Loss: 0.5058, Train: 21.39%, Valid: 20.32%, Test: 5.76%\n",
      "Hits@20\n",
      "Run: 02, Epoch: 60, Loss: 0.5058, Train: 25.37%, Valid: 24.17%, Test: 11.73%\n",
      "---\n",
      "Hits@10\n",
      "Run: 02, Epoch: 65, Loss: 0.5029, Train: 14.00%, Valid: 13.05%, Test: 4.23%\n",
      "Hits@20\n",
      "Run: 02, Epoch: 65, Loss: 0.5029, Train: 25.38%, Valid: 24.65%, Test: 13.52%\n",
      "---\n",
      "Hits@10\n",
      "Run: 02, Epoch: 70, Loss: 0.4989, Train: 22.11%, Valid: 21.21%, Test: 7.91%\n",
      "Hits@20\n",
      "Run: 02, Epoch: 70, Loss: 0.4989, Train: 25.52%, Valid: 24.68%, Test: 15.13%\n",
      "---\n",
      "Hits@10\n",
      "Run: 02, Epoch: 75, Loss: 0.5002, Train: 16.52%, Valid: 15.39%, Test: 4.24%\n",
      "Hits@20\n",
      "Run: 02, Epoch: 75, Loss: 0.5002, Train: 24.10%, Valid: 22.82%, Test: 8.37%\n",
      "---\n",
      "Hits@10\n",
      "Run: 02, Epoch: 80, Loss: 0.4967, Train: 16.35%, Valid: 15.30%, Test: 3.61%\n",
      "Hits@20\n",
      "Run: 02, Epoch: 80, Loss: 0.4967, Train: 21.63%, Valid: 20.39%, Test: 9.69%\n",
      "---\n",
      "Hits@10\n",
      "Run: 02, Epoch: 85, Loss: 0.4956, Train: 23.55%, Valid: 22.33%, Test: 8.58%\n",
      "Hits@20\n",
      "Run: 02, Epoch: 85, Loss: 0.4956, Train: 32.54%, Valid: 31.31%, Test: 16.22%\n",
      "---\n",
      "Hits@10\n",
      "Run: 02, Epoch: 90, Loss: 0.4889, Train: 18.64%, Valid: 17.63%, Test: 3.14%\n",
      "Hits@20\n",
      "Run: 02, Epoch: 90, Loss: 0.4889, Train: 26.80%, Valid: 25.71%, Test: 13.45%\n",
      "---\n",
      "Hits@10\n",
      "Run: 02, Epoch: 95, Loss: 0.4879, Train: 17.95%, Valid: 16.87%, Test: 3.93%\n",
      "Hits@20\n",
      "Run: 02, Epoch: 95, Loss: 0.4879, Train: 25.80%, Valid: 24.52%, Test: 10.90%\n",
      "---\n",
      "Hits@10\n",
      "Run: 02, Epoch: 100, Loss: 0.4872, Train: 18.32%, Valid: 17.16%, Test: 3.96%\n",
      "Hits@20\n",
      "Run: 02, Epoch: 100, Loss: 0.4872, Train: 25.24%, Valid: 24.08%, Test: 8.10%\n",
      "---\n",
      "Val_hits@20:  [0.225351901654818, 0.3130744855381342]\n",
      "Test_hits@20:  [0.09488422267003274, 0.16215568323981752]\n",
      "Train_hits@20:  [0.2368584677389148, 0.3253826157960581]\n",
      "Best model val hits@20:  0.3130744855381342\n",
      "Best model test hits@20:  0.16215568323981752\n",
      "Best model train hits@20:  0.3130744855381342\n",
      "Average best train hits@20: 0.28112054176748646; var: 0.0019591311973107565\n",
      "Average best val hits@20: 0.2692131935964761; var: 0.0019238129307913626\n",
      "Average best test hits@20: 0.12851995295492513; var: 0.001131362351798027\n"
     ]
    }
   ],
   "source": [
    "# TRAIN GAN with random features\n",
    "gan_model = GAN(gnn_args['hidden_size'], gnn_args['hidden_size'],\n",
    "                gnn_args['hidden_size'], gnn_args['attn_head'],\n",
    "                gnn_args['num_layers'], gnn_args['dropout']).to(device)\n",
    "predictor = LinkPredictor(gnn_args['hidden_size'], gnn_args['hidden_size'], 1,\n",
    "                          gnn_args['num_layers'], gnn_args['dropout']).to(device)\n",
    "gan_emb_rand = torch.nn.Embedding(dataset.data.num_nodes, gnn_args['hidden_size']).to(device)\n",
    "train_model(gan_model, gan_emb_rand, gnn_args, predictor, 'gan_rand_feat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f189e6f",
   "metadata": {
    "editable": false,
    "papermill": {
     "duration": 0.049225,
     "end_time": "2025-06-01T08:56:00.187589",
     "exception": false,
     "start_time": "2025-06-01T08:56:00.138364",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 11578.797373,
   "end_time": "2025-06-01T08:56:02.767856",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-01T05:43:03.970483",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
