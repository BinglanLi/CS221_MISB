{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch==2.2.2\n!pip install torch-cluster==1.6.3\n!pip install torch-geometric==2.6.1\n!pip install torch-scatter==2.1.2\n!pip install torch-sparse==0.6.18\n!pip install torch-spline-conv==1.2.2","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-29T04:39:10.685940Z","iopub.execute_input":"2025-05-29T04:39:10.686163Z","iopub.status.idle":"2025-05-29T05:40:19.154758Z","shell.execute_reply.started":"2025-05-29T04:39:10.686146Z","shell.execute_reply":"2025-05-29T05:40:19.154036Z"}},"outputs":[{"name":"stdout","text":"Collecting torch==2.2.2\n  Downloading torch-2.2.2-cp311-cp311-manylinux1_x86_64.whl.metadata (25 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (4.13.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (1.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (2025.3.2)\nCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2.2)\n  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2.2)\n  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2.2)\n  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.2)\n  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2.2)\n  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2.2)\n  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2.2)\n  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2.2)\n  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2.2)\n  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.2)\n  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2.2)\n  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\nCollecting triton==2.2.0 (from torch==2.2.2)\n  Downloading triton-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.2) (12.9.41)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.2.2) (3.0.2)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.2.2) (1.3.0)\nDownloading torch-2.2.2-cp311-cp311-manylinux1_x86_64.whl (755.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.6/755.6 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m88.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading triton-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch\n  Attempting uninstall: triton\n    Found existing installation: triton 3.2.0\n    Uninstalling triton-3.2.0:\n      Successfully uninstalled triton-3.2.0\n  Attempting uninstall: nvidia-nvtx-cu12\n    Found existing installation: nvidia-nvtx-cu12 12.4.127\n    Uninstalling nvidia-nvtx-cu12-12.4.127:\n      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n  Attempting uninstall: nvidia-nccl-cu12\n    Found existing installation: nvidia-nccl-cu12 2.21.5\n    Uninstalling nvidia-nccl-cu12-2.21.5:\n      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\n    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.10.19\n    Uninstalling nvidia-curand-cu12-10.3.10.19:\n      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.4.0.6\n    Uninstalling nvidia-cufft-cu12-11.4.0.6:\n      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.4.127\n    Uninstalling nvidia-cuda-runtime-cu12-12.4.127:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.4.127\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.4.127\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.4.127:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.4.127\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.4.127\n    Uninstalling nvidia-cuda-cupti-cu12-12.4.127:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.4.127\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.9.0.13\n    Uninstalling nvidia-cublas-cu12-12.9.0.13:\n      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\n    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: torch\n    Found existing installation: torch 2.6.0+cu124\n    Uninstalling torch-2.6.0+cu124:\n      Successfully uninstalled torch-2.6.0+cu124\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntorchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.2.2 which is incompatible.\ntorchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvtx-cu12-12.1.105 torch-2.2.2 triton-2.2.0\nCollecting torch-cluster==1.6.3\n  Downloading torch_cluster-1.6.3.tar.gz (54 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-cluster==1.6.3) (1.15.2)\nRequirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy->torch-cluster==1.6.3) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy->torch-cluster==1.6.3) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy->torch-cluster==1.6.3) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy->torch-cluster==1.6.3) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy->torch-cluster==1.6.3) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy->torch-cluster==1.6.3) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy->torch-cluster==1.6.3) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.5,>=1.23.5->scipy->torch-cluster==1.6.3) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.5,>=1.23.5->scipy->torch-cluster==1.6.3) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2.5,>=1.23.5->scipy->torch-cluster==1.6.3) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2.5,>=1.23.5->scipy->torch-cluster==1.6.3) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2.5,>=1.23.5->scipy->torch-cluster==1.6.3) (2024.2.0)\nBuilding wheels for collected packages: torch-cluster\n  Building wheel for torch-cluster (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for torch-cluster: filename=torch_cluster-1.6.3-cp311-cp311-linux_x86_64.whl size=2037594 sha256=ed7f9dd312342ca6962c37a66e572b200571d0c61fba78189bc0844052d70ffc\n  Stored in directory: /root/.cache/pip/wheels/ef/de/7d/a4211822af99147b93800e9e204f0be21294e3c0b95b3b861a\nSuccessfully built torch-cluster\nInstalling collected packages: torch-cluster\nSuccessfully installed torch-cluster-1.6.3\nCollecting torch-geometric==2.6.1\n  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.6.1) (3.11.18)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.6.1) (2025.3.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.6.1) (3.1.6)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.6.1) (1.26.4)\nRequirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.6.1) (7.0.0)\nRequirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.6.1) (3.0.9)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.6.1) (2.32.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.6.1) (4.67.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.6.1) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.6.1) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.6.1) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.6.1) (1.6.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.6.1) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.6.1) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.6.1) (1.20.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric==2.6.1) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric==2.6.1) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric==2.6.1) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric==2.6.1) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric==2.6.1) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric==2.6.1) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric==2.6.1) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric==2.6.1) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric==2.6.1) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric==2.6.1) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric==2.6.1) (2025.4.26)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torch-geometric==2.6.1) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torch-geometric==2.6.1) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torch-geometric==2.6.1) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torch-geometric==2.6.1) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torch-geometric==2.6.1) (2024.2.0)\nDownloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: torch-geometric\nSuccessfully installed torch-geometric-2.6.1\nCollecting torch-scatter==2.1.2\n  Downloading torch_scatter-2.1.2.tar.gz (108 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.0/108.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nBuilding wheels for collected packages: torch-scatter\n  Building wheel for torch-scatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for torch-scatter: filename=torch_scatter-2.1.2-cp311-cp311-linux_x86_64.whl size=3541198 sha256=4ea9a0cf68190a3792277c7ea4e838602d3ab66aad8f0f0af39c1ec6aed41295\n  Stored in directory: /root/.cache/pip/wheels/b8/d4/0e/a80af2465354ea7355a2c153b11af2da739cfcf08b6c0b28e2\nSuccessfully built torch-scatter\nInstalling collected packages: torch-scatter\nSuccessfully installed torch-scatter-2.1.2\nCollecting torch-sparse==0.6.18\n  Downloading torch_sparse-0.6.18.tar.gz (209 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.0/210.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-sparse==0.6.18) (1.15.2)\nRequirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy->torch-sparse==0.6.18) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy->torch-sparse==0.6.18) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy->torch-sparse==0.6.18) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy->torch-sparse==0.6.18) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy->torch-sparse==0.6.18) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy->torch-sparse==0.6.18) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy->torch-sparse==0.6.18) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.5,>=1.23.5->scipy->torch-sparse==0.6.18) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.5,>=1.23.5->scipy->torch-sparse==0.6.18) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2.5,>=1.23.5->scipy->torch-sparse==0.6.18) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2.5,>=1.23.5->scipy->torch-sparse==0.6.18) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2.5,>=1.23.5->scipy->torch-sparse==0.6.18) (2024.2.0)\nBuilding wheels for collected packages: torch-sparse\n  Building wheel for torch-sparse (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for torch-sparse: filename=torch_sparse-0.6.18-cp311-cp311-linux_x86_64.whl size=2773984 sha256=fbf0dfefd08bbbb66662afb9167f075c47cc5a9e6f92c3d751270a18436cd0c4\n  Stored in directory: /root/.cache/pip/wheels/75/e2/1e/299c596063839303657c211f587f05591891cc6cf126d94d21\nSuccessfully built torch-sparse\nInstalling collected packages: torch-sparse\nSuccessfully installed torch-sparse-0.6.18\nCollecting torch-spline-conv==1.2.2\n  Downloading torch_spline_conv-1.2.2.tar.gz (25 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nBuilding wheels for collected packages: torch-spline-conv\n  Building wheel for torch-spline-conv (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for torch-spline-conv: filename=torch_spline_conv-1.2.2-cp311-cp311-linux_x86_64.whl size=525857 sha256=d7a1c35d686ccd61a4b2ef7b2cd624e33b7b9c7f90a3a3382e35f3ad95cfd414\n  Stored in directory: /root/.cache/pip/wheels/25/16/8a/a98b0173c4fbbc7aa1c4929b46d2eb08d1475c5c7b54e289b6\nSuccessfully built torch-spline-conv\nInstalling collected packages: torch-spline-conv\nSuccessfully installed torch-spline-conv-1.2.2\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install ogb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T05:40:19.156179Z","iopub.execute_input":"2025-05-29T05:40:19.156418Z","iopub.status.idle":"2025-05-29T05:40:22.794313Z","shell.execute_reply.started":"2025-05-29T05:40:19.156393Z","shell.execute_reply":"2025-05-29T05:40:22.793363Z"}},"outputs":[{"name":"stdout","text":"Collecting ogb\n  Downloading ogb-1.3.6-py3-none-any.whl.metadata (6.2 kB)\nRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from ogb) (2.2.2)\nRequirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from ogb) (1.26.4)\nRequirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.11/dist-packages (from ogb) (4.67.1)\nRequirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from ogb) (1.2.2)\nRequirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from ogb) (2.2.3)\nRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from ogb) (1.17.0)\nRequirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.11/dist-packages (from ogb) (2.4.0)\nCollecting outdated>=0.2.0 (from ogb)\n  Downloading outdated-0.2.2-py2.py3-none-any.whl.metadata (4.7 kB)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.0->ogb) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.0->ogb) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.0->ogb) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.0->ogb) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.0->ogb) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.0->ogb) (2.4.1)\nRequirement already satisfied: setuptools>=44 in /usr/local/lib/python3.11/dist-packages (from outdated>=0.2.0->ogb) (75.2.0)\nCollecting littleutils (from outdated>=0.2.0->ogb)\n  Downloading littleutils-0.2.4-py3-none-any.whl.metadata (679 bytes)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from outdated>=0.2.0->ogb) (2.32.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24.0->ogb) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24.0->ogb) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24.0->ogb) (2025.2)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.0->ogb) (1.15.2)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.0->ogb) (1.5.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.0->ogb) (3.6.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (4.13.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (1.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (12.1.105)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (12.1.105)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (12.1.105)\nRequirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (8.9.2.26)\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (12.1.3.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (11.0.2.54)\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (10.3.2.106)\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (11.4.5.107)\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (12.1.0.106)\nRequirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (2.19.3)\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (12.1.105)\nRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (2.2.0)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.6.0->ogb) (12.9.41)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.6.0->ogb) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.16.0->ogb) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.16.0->ogb) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.16.0->ogb) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.16.0->ogb) (2024.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->outdated>=0.2.0->ogb) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->outdated>=0.2.0->ogb) (3.10)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->outdated>=0.2.0->ogb) (2025.4.26)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch>=1.6.0->ogb) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.16.0->ogb) (2024.2.0)\nDownloading ogb-1.3.6-py3-none-any.whl (78 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\nDownloading littleutils-0.2.4-py3-none-any.whl (8.1 kB)\nInstalling collected packages: littleutils, outdated, ogb\nSuccessfully installed littleutils-0.2.4 ogb-1.3.6 outdated-0.2.2\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Importing necessary dependencies in order to import our dataset, create our\n# GCN models, and evaluate the models\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn import Sequential, Linear, BatchNorm1d, ReLU\n\nfrom torch_geometric.utils import negative_sampling\nfrom torch_geometric.data import Data\n\nimport torch_geometric.transforms as T\nfrom torch_geometric.nn import GCNConv, SAGEConv, GINConv, GATConv\n\nfrom ogb.linkproppred import PygLinkPropPredDataset, Evaluator\nfrom torch_geometric.data import DataLoader\nfrom torch_geometric.data.data import DataEdgeAttr, GlobalStorage\n\nfrom torch_geometric.nn import Node2Vec\n\nimport pandas as pd\nimport shutil, os\nimport os.path as osp\nimport numpy as np\n\n#from logger import Logger\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nimport random","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T05:40:22.795439Z","iopub.execute_input":"2025-05-29T05:40:22.796259Z","iopub.status.idle":"2025-05-29T05:40:30.058456Z","shell.execute_reply.started":"2025-05-29T05:40:22.796228Z","shell.execute_reply":"2025-05-29T05:40:30.057726Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\n# If you use GPU, the device should be cuda\nprint('Device: {}'.format(device))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T05:40:30.060082Z","iopub.execute_input":"2025-05-29T05:40:30.060466Z","iopub.status.idle":"2025-05-29T05:40:30.064730Z","shell.execute_reply.started":"2025-05-29T05:40:30.060448Z","shell.execute_reply":"2025-05-29T05:40:30.064024Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# loaded with transform parameter set as such in order to obtain the adj_t matrix\n# required for the GNN layers\n# torch.serialization.add_safe_globals([DataEdgeAttr])\n# torch.serialization.add_safe_globals([GlobalStorage])\ndataset = PygLinkPropPredDataset(name='ogbl-ddi', root ='./', transform=T.ToSparseTensor()) # loading ogb-ddi\nprint('Task type: {}'.format(dataset.task_type))\ngraph = dataset[0]\nadj_t = graph.adj_t.to(device) # loads all edges in graph into sparse adj_t matrix","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T05:40:30.065505Z","iopub.execute_input":"2025-05-29T05:40:30.065751Z","iopub.status.idle":"2025-05-29T05:40:33.547804Z","shell.execute_reply.started":"2025-05-29T05:40:30.065729Z","shell.execute_reply":"2025-05-29T05:40:33.547042Z"}},"outputs":[{"name":"stdout","text":"Downloading http://snap.stanford.edu/ogb/data/linkproppred/ddi.zip\n","output_type":"stream"},{"name":"stderr","text":"Downloaded 0.04 GB: 100%|██████████| 46/46 [00:02<00:00, 22.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ./ddi.zip\n","output_type":"stream"},{"name":"stderr","text":"Processing...\n","output_type":"stream"},{"name":"stdout","text":"Loading necessary files...\nThis might take a while.\nProcessing graphs...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00, 64.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Converting graphs into PyG objects...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00, 3587.94it/s]","output_type":"stream"},{"name":"stdout","text":"Saving...\nTask type: link prediction\n","output_type":"stream"},{"name":"stderr","text":"\nDone!\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# getting the train, validation, and test edge splits\nsplit_edge = dataset.get_edge_split()\ntrain_edges = split_edge['train']['edge']\ntorch.manual_seed(70) # picking random samples to evaluate on\nidx = torch.randperm(split_edge['train']['edge'].size(0))\nidx = idx[:split_edge['valid']['edge'].size(0)]\nsplit_edge['eval_train'] = {'edge': split_edge['train']['edge'][idx]}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T05:40:33.548644Z","iopub.execute_input":"2025-05-29T05:40:33.548922Z","iopub.status.idle":"2025-05-29T05:40:33.647631Z","shell.execute_reply.started":"2025-05-29T05:40:33.548897Z","shell.execute_reply":"2025-05-29T05:40:33.646662Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"train_edges_node2vec = train_edges.T # transpose to get the right dimension\n# Initialize edge-induced subgraph with only train edges (edge-induced subgraph)\ndata_node2vec = Data(edge_index=train_edges_node2vec)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T05:40:33.648466Z","iopub.execute_input":"2025-05-29T05:40:33.648773Z","iopub.status.idle":"2025-05-29T05:40:33.652755Z","shell.execute_reply.started":"2025-05-29T05:40:33.648741Z","shell.execute_reply":"2025-05-29T05:40:33.651944Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def save_embedding(model, filepath): # function to save embedding to specified filepath\n    torch.save(model.embedding.weight.data.cpu(), filepath)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T05:40:33.653729Z","iopub.execute_input":"2025-05-29T05:40:33.654071Z","iopub.status.idle":"2025-05-29T05:40:33.662551Z","shell.execute_reply.started":"2025-05-29T05:40:33.654045Z","shell.execute_reply":"2025-05-29T05:40:33.661917Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import os\nif not os.path.exists('/kaggle/working/training_outputs/'):\n    os.makedirs('/kaggle/working/training_outputs/') # directory for saving visualizations and model checkpoints","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T05:40:33.663403Z","iopub.execute_input":"2025-05-29T05:40:33.663766Z","iopub.status.idle":"2025-05-29T05:40:33.672774Z","shell.execute_reply.started":"2025-05-29T05:40:33.663742Z","shell.execute_reply":"2025-05-29T05:40:33.672016Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# class in order to predict whether a link exists between two nodes using\n# their embeddings, x_i and x_j\nclass LinkPredictor(torch.nn.Module):\n    ''' Neural network which predicts whether a link (interaction) exists between 2 nodes i,j\n    given their embeddings x_i, x_j.\n    '''\n    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n                 dropout):\n        super(LinkPredictor, self).__init__()\n\n        self.lins = torch.nn.ModuleList()\n        self.lins.append(torch.nn.Linear(in_channels, hidden_channels))\n        for _ in range(num_layers - 2):\n            self.lins.append(torch.nn.Linear(hidden_channels, hidden_channels))\n        self.lins.append(torch.nn.Linear(hidden_channels, out_channels))\n\n        self.dropout = dropout\n\n    def reset_parameters(self):\n        for lin in self.lins:\n            lin.reset_parameters()\n\n    def forward(self, x_i, x_j):\n        x = x_i * x_j # hadamard product\n        for lin in self.lins[:-1]: # linear layer -> relu -> dropout\n            x = lin(x)\n            x = F.relu(x)\n            x = F.dropout(x, p=self.dropout, training=self.training)\n        x = self.lins[-1](x)\n        return torch.sigmoid(x) # sigmoid activation outputs probability that a given edge exists for all node pairs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T05:40:33.675050Z","iopub.execute_input":"2025-05-29T05:40:33.675225Z","iopub.status.idle":"2025-05-29T05:40:33.687540Z","shell.execute_reply.started":"2025-05-29T05:40:33.675212Z","shell.execute_reply":"2025-05-29T05:40:33.686822Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def train(model, predictor, x, adj_t, split_edge, optimizer, batch_size):\n\n    row, col, _ = adj_t.coo()\n    edge_index = torch.stack([col, row], dim=0)\n\n    model.train()\n    predictor.train()\n\n    pos_train_edge = split_edge['train']['edge'].to(x.device)\n\n    total_loss = total_examples = 0\n    for perm in DataLoader(range(pos_train_edge.size(0)), batch_size,\n                           shuffle=True):\n        optimizer.zero_grad()\n\n        h = model(x, adj_t)\n\n        edge = pos_train_edge[perm].t()\n\n        # computes the loss for positive edges\n        pos_out = predictor(h[edge[0]], h[edge[1]])\n        pos_loss = -torch.log(pos_out + 1e-15).mean()\n\n        # samples negative edges from the graph\n        edge = negative_sampling(edge_index, num_nodes=x.size(0),\n                                 num_neg_samples=perm.size(0), method='dense')\n\n        # computes the loss for negative edges\n        neg_out = predictor(h[edge[0]], h[edge[1]])\n        neg_loss = -torch.log(1 - neg_out + 1e-15).mean()\n\n        loss = pos_loss + neg_loss\n        loss.backward()\n\n        torch.nn.utils.clip_grad_norm_(x, 1.0)\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        torch.nn.utils.clip_grad_norm_(predictor.parameters(), 1.0)\n\n        optimizer.step()\n\n        num_examples = pos_out.size(0)\n        total_loss += loss.item() * num_examples\n        total_examples += num_examples\n\n    return total_loss / total_examples\n\n\n@torch.no_grad()\ndef test(model, predictor, x, adj_t, split_edge, evaluator, batch_size):\n    model.eval()\n    predictor.eval()\n\n    h = model(x, adj_t)\n\n    pos_train_edge = split_edge['eval_train']['edge'].to(x.device)\n    pos_valid_edge = split_edge['valid']['edge'].to(x.device)\n    neg_valid_edge = split_edge['valid']['edge_neg'].to(x.device)\n    pos_test_edge = split_edge['test']['edge'].to(x.device)\n    neg_test_edge = split_edge['test']['edge_neg'].to(x.device)\n\n    # store what the link predictor outputs for each positive and negative\n    # edge in order to compute the hits@K\n    pos_train_preds = []\n    for perm in DataLoader(range(pos_train_edge.size(0)), batch_size):\n        edge = pos_train_edge[perm].t()\n        pos_train_preds += [predictor(h[edge[0]], h[edge[1]]).squeeze().cpu()]\n    pos_train_pred = torch.cat(pos_train_preds, dim=0)\n\n    pos_valid_preds = []\n    for perm in DataLoader(range(pos_valid_edge.size(0)), batch_size):\n        edge = pos_valid_edge[perm].t()\n        pos_valid_preds += [predictor(h[edge[0]], h[edge[1]]).squeeze().cpu()]\n    pos_valid_pred = torch.cat(pos_valid_preds, dim=0)\n\n    neg_valid_preds = []\n    for perm in DataLoader(range(neg_valid_edge.size(0)), batch_size):\n        edge = neg_valid_edge[perm].t()\n        neg_valid_preds += [predictor(h[edge[0]], h[edge[1]]).squeeze().cpu()]\n    neg_valid_pred = torch.cat(neg_valid_preds, dim=0)\n\n    pos_test_preds = []\n    for perm in DataLoader(range(pos_test_edge.size(0)), batch_size):\n        edge = pos_test_edge[perm].t()\n        pos_test_preds += [predictor(h[edge[0]], h[edge[1]]).squeeze().cpu()]\n    pos_test_pred = torch.cat(pos_test_preds, dim=0)\n\n    neg_test_preds = []\n    for perm in DataLoader(range(neg_test_edge.size(0)), batch_size):\n        edge = neg_test_edge[perm].t()\n        neg_test_preds += [predictor(h[edge[0]], h[edge[1]]).squeeze().cpu()]\n    neg_test_pred = torch.cat(neg_test_preds, dim=0)\n\n    # compute the hits@K for training, validation, and test\n    results = {}\n    for K in [10, 20]:\n        evaluator.K = K\n        train_hits = evaluator.eval({\n            'y_pred_pos': pos_train_pred,\n            'y_pred_neg': neg_valid_pred,\n        })[f'hits@{K}']\n        valid_hits = evaluator.eval({\n            'y_pred_pos': pos_valid_pred,\n            'y_pred_neg': neg_valid_pred,\n        })[f'hits@{K}']\n        test_hits = evaluator.eval({\n            'y_pred_pos': pos_test_pred,\n            'y_pred_neg': neg_test_pred,\n        })[f'hits@{K}']\n\n        results[f'Hits@{K}'] = (train_hits, valid_hits, test_hits)\n\n    return results\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T05:40:33.688207Z","iopub.execute_input":"2025-05-29T05:40:33.688453Z","iopub.status.idle":"2025-05-29T05:40:33.705863Z","shell.execute_reply.started":"2025-05-29T05:40:33.688436Z","shell.execute_reply":"2025-05-29T05:40:33.705336Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"gnn_args = { # define GNN hyperparams\n    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n    'hidden_size': 256,\n    'dropout': 0.5,\n    'epochs': 100,\n    'weight_decay': 1e-5,\n    'lr': 0.005,\n    'attn_size': 32,\n    'num_layers':2,\n    'log_steps':1,\n    'eval_steps':5,\n    'runs':10,\n    'batch_size': 16*1024,\n    'attn_heads': 1,\n\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T05:56:23.137064Z","iopub.execute_input":"2025-05-29T05:56:23.137339Z","iopub.status.idle":"2025-05-29T05:56:23.141910Z","shell.execute_reply.started":"2025-05-29T05:56:23.137319Z","shell.execute_reply":"2025-05-29T05:56:23.141229Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"def train_model(model, emb, gnn_args, predictor, model_name):\n  '''\n  Train specified GNN model. Model and embeddings should be initialized.\n  Save model after every run.\n  '''\n  train_hits_arr, val_hits_arr, test_hits_arr = [], [], []\n\n  evaluator = Evaluator(name='ogbl-ddi')\n  for run in range(2):\n    max_valhits, train_hits_run, test_hits_run = float('-inf'), 0, 0\n\n    torch.nn.init.xavier_uniform_(emb.weight)\n    model.reset_parameters()\n    predictor.reset_parameters()\n    optimizer = torch.optim.Adam(\n        list(model.parameters()) + list(emb.parameters()) +\n        list(predictor.parameters()), lr=gnn_args['lr'])\n\n    for epoch in range(1, 1 + gnn_args['epochs']):\n        loss = train(model, predictor, emb.weight, adj_t, split_edge,\n                      optimizer, gnn_args['batch_size'])\n\n        if epoch % gnn_args['eval_steps'] == 0:\n            results = test(model, predictor, emb.weight, adj_t, split_edge,\n                            evaluator, gnn_args['batch_size'])\n\n\n            if epoch % gnn_args['log_steps'] == 0:\n                for key, result in results.items():\n                    train_hits, valid_hits, test_hits = result\n                    print(key)\n                    print(f'Run: {run + 1:02d}, '\n                          f'Epoch: {epoch:02d}, '\n                          f'Loss: {loss:.4f}, '\n                          f'Train: {100 * train_hits:.2f}%, '\n                          f'Valid: {100 * valid_hits:.2f}%, '\n                          f'Test: {100 * test_hits:.2f}%')\n                print('---')\n\n            # check val-hits@20\n            train_hits, valid_hits, test_hits = results['Hits@20']\n            if valid_hits >= max_valhits: # if validhits20 is higher than max, save ckpt\n              max_valhits = valid_hits\n              train_hits_run = train_hits\n              test_hits_run = test_hits\n              # Save model checkpoint for current run.\n              model_path = f\"training_outputs/{model_name}.pt\"\n              emb_path = f'training_outputs/{model_name}_init_emb.pt'\n              save_model_ckpt(model, emb, optimizer, predictor, loss, emb_path, model_path)\n    train_hits_arr.append(train_hits_run)\n    test_hits_arr.append(test_hits_run)\n    val_hits_arr.append(max_valhits)\n\n\n  # Print overall stats arrays for best model based on val hits@20\n  print(\"Val_hits@20: \", val_hits_arr)\n  print(\"Test_hits@20: \", test_hits_arr)\n  print(\"Train_hits@20: \", train_hits_arr)\n\n  # Print best model stats (based on val hits@20)\n  val_max = max(val_hits_arr)\n  print(\"Best model val hits@20: \", max(val_hits_arr))\n  max_idx = val_hits_arr.index(val_max)\n  print('Best model test hits@20: ', test_hits_arr[max_idx])\n  print('Best model train hits@20: ', val_hits_arr[max_idx])\n\n  # convert to numpy array\n  val_hits_arr = np.array(val_hits_arr)\n  test_hits_arr = np.array(test_hits_arr)\n  train_hits_arr = np.array(train_hits_arr)\n\n  # Print average stats + variance\n  print(f\"Average best train hits@20: {np.mean(train_hits_arr)}; var: {np.var(train_hits_arr)}\")\n  print(f\"Average best val hits@20: {np.mean(val_hits_arr)}; var: {np.var(val_hits_arr)}\")\n  print(f\"Average best test hits@20: {np.mean(test_hits_arr)}; var: {np.var(test_hits_arr)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T05:56:23.490823Z","iopub.execute_input":"2025-05-29T05:56:23.491086Z","iopub.status.idle":"2025-05-29T05:56:23.509593Z","shell.execute_reply.started":"2025-05-29T05:56:23.491068Z","shell.execute_reply":"2025-05-29T05:56:23.508814Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"def save_model_ckpt(model, emb, optimizer, predictor, loss, emb_path, model_path):\n  ''' Save model and embedding checkpoints. '''\n  EPOCH = 100\n  # Save model params\n  torch.save({\n            'epoch': EPOCH,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'predictor_state_dict': predictor.state_dict(),\n            'loss': loss,\n            }, model_path)\n  # Also save initial embedding (just in case)\n  torch.save(emb.weight.data.cpu(), emb_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T05:56:23.758812Z","iopub.execute_input":"2025-05-29T05:56:23.759629Z","iopub.status.idle":"2025-05-29T05:56:23.763519Z","shell.execute_reply.started":"2025-05-29T05:56:23.759603Z","shell.execute_reply":"2025-05-29T05:56:23.762918Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"def load_model_ckpt(curr_model, model_name, run):\n  ''' Load model checkpoint. '''\n  evaluator = Evaluator(name='ogbl-ddi')\n  model_path = f\"training_outputs/{model_name}.pt\"\n  emb_path = f'training_outputs/{model_name}_init_emb.pt'\n\n  # Load emb (init feature representations)\n  pretrained_weight = torch.load(emb_path, map_location='cpu').to(device)\n  pretrained_weight = pretrained_weight.cpu().data.numpy()\n  emb_after = torch.nn.Embedding(dataset.data.num_nodes, gnn_args['hidden_size']).to(device)\n  # Pretrained_weight is a numpy matrix of shape (num_embeddings, embedding_dim)\n  emb_after.weight.data.copy_(torch.from_numpy(pretrained_weight))\n\n  # Init optimizer and predictor objects\n  predictor = LinkPredictor(gnn_args['hidden_size'], gnn_args['hidden_size'], 1,\n                          gnn_args['num_layers'], gnn_args['dropout']).to(device)\n  optimizer = torch.optim.Adam(\n        list(curr_model.parameters()) + list(emb_after.parameters()) +\n        list(predictor.parameters()), lr=gnn_args['lr'])\n\n\n  # Load model, predictor, and optimizer params\n  checkpoint = torch.load(model_path)\n  curr_model.load_state_dict(checkpoint['model_state_dict'])\n  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n  predictor.load_state_dict(checkpoint['predictor_state_dict'])\n  epoch = checkpoint['epoch']\n  loss = checkpoint['loss']\n\n\n  # Save final embedding representation of all nodes\n  h = curr_model(emb_after.weight, adj_t)\n  final_emb_path = f'training_outputs/{model_name}_final_emb_{run}.pt'\n  torch.save(h, final_emb_path)\n\n  # Evaluate pretrained model\n  results = test(curr_model, predictor, emb_after.weight, adj_t, split_edge,\n                               evaluator, gnn_args['batch_size'])\n\n  # Print hits stats\n  for key, result in results.items():\n    print(key)\n    train_hits, valid_hits, test_hits = result\n    print(f'Train: {100 * train_hits:.2f}%, '\n                              f'Valid: {100 * valid_hits:.2f}%, '\n                              f'Test: {100 * test_hits:.2f}%')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T05:56:24.038652Z","iopub.execute_input":"2025-05-29T05:56:24.039091Z","iopub.status.idle":"2025-05-29T05:56:24.045704Z","shell.execute_reply.started":"2025-05-29T05:56:24.039072Z","shell.execute_reply":"2025-05-29T05:56:24.045169Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"class GAT(torch.nn.Module):\n    '''Define GAT network using PyG's GATConv layer.'''\n    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n                dropout=0.5, heads=1):\n        super(GAT, self).__init__()\n\n        self.convs = torch.nn.ModuleList()\n        self.convs.append(GATConv(in_channels, hidden_channels, heads=heads, dropout=dropout))\n        \n        for _ in range(num_layers - 2):\n            self.convs.append(\n                GATConv(hidden_channels * heads, hidden_channels, heads=heads, dropout=dropout)\n            )\n        \n        self.convs.append(GATConv(hidden_channels * heads, out_channels, heads=1, concat=False, dropout=dropout))\n\n        self.dropout = dropout\n\n    def reset_parameters(self):\n        for conv in self.convs:\n            conv.reset_parameters()\n\n    def forward(self, x, edge_index):\n        for conv in self.convs[:-1]:\n            x = conv(x, edge_index)\n            x = F.elu(x)\n            x = F.dropout(x, p=self.dropout, training=self.training)\n        x = self.convs[-1](x, edge_index)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T05:56:27.336067Z","iopub.execute_input":"2025-05-29T05:56:27.336344Z","iopub.status.idle":"2025-05-29T05:56:27.343226Z","shell.execute_reply.started":"2025-05-29T05:56:27.336324Z","shell.execute_reply":"2025-05-29T05:56:27.342539Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"# TRAIN gcn with random features\nmodel_name = 'gat_rand_feat'\ngat_model = GAT(in_channels = gnn_args['hidden_size'],hidden_channels = 32,out_channels = gnn_args['hidden_size'],\n              num_layers = gnn_args['num_layers'], dropout = gnn_args['dropout'], heads = 1).to(device)\ngat_predictor = LinkPredictor(gnn_args['hidden_size'], gnn_args['hidden_size'], 1,\n                          gnn_args['num_layers'], gnn_args['dropout']).to(device)\ngat_emb_rand = torch.nn.Embedding(dataset.data.num_nodes, gnn_args['hidden_size']).to(device)\ntrain_model(gat_model, gat_emb_rand, gnn_args, gat_predictor, model_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T05:56:27.734837Z","iopub.execute_input":"2025-05-29T05:56:27.735174Z","iopub.status.idle":"2025-05-29T06:46:06.016498Z","shell.execute_reply.started":"2025-05-29T05:56:27.735143Z","shell.execute_reply":"2025-05-29T06:46:06.015732Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n  warnings.warn(msg)\n/usr/local/lib/python3.11/dist-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n  warnings.warn(out)\n","output_type":"stream"},{"name":"stdout","text":"Hits@10\nRun: 01, Epoch: 05, Loss: 0.5427, Train: 16.26%, Valid: 15.43%, Test: 5.20%\nHits@20\nRun: 01, Epoch: 05, Loss: 0.5427, Train: 20.54%, Valid: 19.76%, Test: 9.42%\n---\nHits@10\nRun: 01, Epoch: 10, Loss: 0.4247, Train: 16.63%, Valid: 15.37%, Test: 4.57%\nHits@20\nRun: 01, Epoch: 10, Loss: 0.4247, Train: 31.17%, Valid: 29.67%, Test: 14.61%\n---\nHits@10\nRun: 01, Epoch: 15, Loss: 0.3803, Train: 21.70%, Valid: 19.39%, Test: 8.61%\nHits@20\nRun: 01, Epoch: 15, Loss: 0.3803, Train: 35.06%, Valid: 32.16%, Test: 12.96%\n---\nHits@10\nRun: 01, Epoch: 20, Loss: 0.3549, Train: 18.46%, Valid: 16.38%, Test: 1.17%\nHits@20\nRun: 01, Epoch: 20, Loss: 0.3549, Train: 34.16%, Valid: 30.97%, Test: 7.83%\n---\nHits@10\nRun: 01, Epoch: 25, Loss: 0.3341, Train: 24.10%, Valid: 21.13%, Test: 7.83%\nHits@20\nRun: 01, Epoch: 25, Loss: 0.3341, Train: 36.68%, Valid: 32.77%, Test: 15.51%\n---\nHits@10\nRun: 01, Epoch: 30, Loss: 0.3175, Train: 24.35%, Valid: 21.56%, Test: 6.13%\nHits@20\nRun: 01, Epoch: 30, Loss: 0.3175, Train: 30.29%, Valid: 26.76%, Test: 13.09%\n---\nHits@10\nRun: 01, Epoch: 35, Loss: 0.3097, Train: 24.89%, Valid: 21.82%, Test: 3.79%\nHits@20\nRun: 01, Epoch: 35, Loss: 0.3097, Train: 35.64%, Valid: 31.58%, Test: 7.80%\n---\nHits@10\nRun: 01, Epoch: 40, Loss: 0.3030, Train: 27.49%, Valid: 23.93%, Test: 3.48%\nHits@20\nRun: 01, Epoch: 40, Loss: 0.3030, Train: 38.67%, Valid: 34.12%, Test: 10.13%\n---\nHits@10\nRun: 01, Epoch: 45, Loss: 0.2988, Train: 23.53%, Valid: 20.46%, Test: 8.02%\nHits@20\nRun: 01, Epoch: 45, Loss: 0.2988, Train: 34.22%, Valid: 30.11%, Test: 15.45%\n---\nHits@10\nRun: 01, Epoch: 50, Loss: 0.2925, Train: 35.12%, Valid: 30.61%, Test: 10.34%\nHits@20\nRun: 01, Epoch: 50, Loss: 0.2925, Train: 45.79%, Valid: 40.31%, Test: 17.54%\n---\nHits@10\nRun: 01, Epoch: 55, Loss: 0.2882, Train: 36.13%, Valid: 31.63%, Test: 18.90%\nHits@20\nRun: 01, Epoch: 55, Loss: 0.2882, Train: 51.19%, Valid: 45.67%, Test: 25.36%\n---\nHits@10\nRun: 01, Epoch: 60, Loss: 0.2859, Train: 33.05%, Valid: 28.55%, Test: 7.75%\nHits@20\nRun: 01, Epoch: 60, Loss: 0.2859, Train: 44.13%, Valid: 38.56%, Test: 15.02%\n---\nHits@10\nRun: 01, Epoch: 65, Loss: 0.2816, Train: 44.66%, Valid: 38.95%, Test: 13.95%\nHits@20\nRun: 01, Epoch: 65, Loss: 0.2816, Train: 50.23%, Valid: 43.90%, Test: 25.92%\n---\nHits@10\nRun: 01, Epoch: 70, Loss: 0.2782, Train: 36.73%, Valid: 31.89%, Test: 4.90%\nHits@20\nRun: 01, Epoch: 70, Loss: 0.2782, Train: 44.04%, Valid: 38.42%, Test: 11.25%\n---\nHits@10\nRun: 01, Epoch: 75, Loss: 0.2749, Train: 36.85%, Valid: 31.75%, Test: 7.27%\nHits@20\nRun: 01, Epoch: 75, Loss: 0.2749, Train: 47.25%, Valid: 40.96%, Test: 15.80%\n---\nHits@10\nRun: 01, Epoch: 80, Loss: 0.2717, Train: 34.59%, Valid: 29.79%, Test: 5.92%\nHits@20\nRun: 01, Epoch: 80, Loss: 0.2717, Train: 50.84%, Valid: 44.44%, Test: 16.37%\n---\nHits@10\nRun: 01, Epoch: 85, Loss: 0.2700, Train: 35.77%, Valid: 30.77%, Test: 7.58%\nHits@20\nRun: 01, Epoch: 85, Loss: 0.2700, Train: 50.31%, Valid: 43.89%, Test: 18.11%\n---\nHits@10\nRun: 01, Epoch: 90, Loss: 0.2695, Train: 32.11%, Valid: 27.82%, Test: 7.48%\nHits@20\nRun: 01, Epoch: 90, Loss: 0.2695, Train: 46.33%, Valid: 40.32%, Test: 14.94%\n---\nHits@10\nRun: 01, Epoch: 95, Loss: 0.2649, Train: 34.10%, Valid: 29.55%, Test: 5.30%\nHits@20\nRun: 01, Epoch: 95, Loss: 0.2649, Train: 47.96%, Valid: 41.52%, Test: 10.27%\n---\nHits@10\nRun: 01, Epoch: 100, Loss: 0.2649, Train: 39.44%, Valid: 34.19%, Test: 8.86%\nHits@20\nRun: 01, Epoch: 100, Loss: 0.2649, Train: 55.25%, Valid: 48.29%, Test: 18.53%\n---\nHits@10\nRun: 02, Epoch: 05, Loss: 0.5467, Train: 22.28%, Valid: 21.73%, Test: 4.50%\nHits@20\nRun: 02, Epoch: 05, Loss: 0.5467, Train: 26.40%, Valid: 25.89%, Test: 12.91%\n---\nHits@10\nRun: 02, Epoch: 10, Loss: 0.4509, Train: 10.41%, Valid: 9.76%, Test: 1.86%\nHits@20\nRun: 02, Epoch: 10, Loss: 0.4509, Train: 19.73%, Valid: 18.92%, Test: 6.48%\n---\nHits@10\nRun: 02, Epoch: 15, Loss: 0.4006, Train: 0.01%, Valid: 0.01%, Test: 0.00%\nHits@20\nRun: 02, Epoch: 15, Loss: 0.4006, Train: 1.06%, Valid: 0.87%, Test: 0.10%\n---\nHits@10\nRun: 02, Epoch: 20, Loss: 0.3747, Train: 4.18%, Valid: 3.64%, Test: 0.48%\nHits@20\nRun: 02, Epoch: 20, Loss: 0.3747, Train: 13.69%, Valid: 12.58%, Test: 4.03%\n---\nHits@10\nRun: 02, Epoch: 25, Loss: 0.3554, Train: 5.58%, Valid: 4.89%, Test: 0.98%\nHits@20\nRun: 02, Epoch: 25, Loss: 0.3554, Train: 20.18%, Valid: 18.43%, Test: 2.50%\n---\nHits@10\nRun: 02, Epoch: 30, Loss: 0.3411, Train: 26.16%, Valid: 23.47%, Test: 6.53%\nHits@20\nRun: 02, Epoch: 30, Loss: 0.3411, Train: 32.74%, Valid: 29.74%, Test: 19.35%\n---\nHits@10\nRun: 02, Epoch: 35, Loss: 0.3290, Train: 23.24%, Valid: 20.80%, Test: 5.09%\nHits@20\nRun: 02, Epoch: 35, Loss: 0.3290, Train: 31.24%, Valid: 28.23%, Test: 13.23%\n---\nHits@10\nRun: 02, Epoch: 40, Loss: 0.3200, Train: 33.10%, Valid: 30.35%, Test: 3.76%\nHits@20\nRun: 02, Epoch: 40, Loss: 0.3200, Train: 42.73%, Valid: 39.72%, Test: 13.96%\n---\nHits@10\nRun: 02, Epoch: 45, Loss: 0.3122, Train: 26.66%, Valid: 23.75%, Test: 5.73%\nHits@20\nRun: 02, Epoch: 45, Loss: 0.3122, Train: 34.75%, Valid: 31.51%, Test: 17.21%\n---\nHits@10\nRun: 02, Epoch: 50, Loss: 0.3068, Train: 27.87%, Valid: 24.42%, Test: 9.01%\nHits@20\nRun: 02, Epoch: 50, Loss: 0.3068, Train: 40.23%, Valid: 35.86%, Test: 17.23%\n---\nHits@10\nRun: 02, Epoch: 55, Loss: 0.3016, Train: 24.75%, Valid: 22.63%, Test: 6.69%\nHits@20\nRun: 02, Epoch: 55, Loss: 0.3016, Train: 35.26%, Valid: 32.46%, Test: 11.82%\n---\nHits@10\nRun: 02, Epoch: 60, Loss: 0.2995, Train: 35.94%, Valid: 31.82%, Test: 13.27%\nHits@20\nRun: 02, Epoch: 60, Loss: 0.2995, Train: 46.63%, Valid: 41.99%, Test: 25.97%\n---\nHits@10\nRun: 02, Epoch: 65, Loss: 0.2935, Train: 37.15%, Valid: 33.73%, Test: 9.21%\nHits@20\nRun: 02, Epoch: 65, Loss: 0.2935, Train: 46.80%, Valid: 43.22%, Test: 16.08%\n---\nHits@10\nRun: 02, Epoch: 70, Loss: 0.2885, Train: 39.25%, Valid: 34.76%, Test: 11.72%\nHits@20\nRun: 02, Epoch: 70, Loss: 0.2885, Train: 49.04%, Valid: 43.86%, Test: 24.00%\n---\nHits@10\nRun: 02, Epoch: 75, Loss: 0.2888, Train: 22.93%, Valid: 19.79%, Test: 7.97%\nHits@20\nRun: 02, Epoch: 75, Loss: 0.2888, Train: 44.75%, Valid: 39.88%, Test: 21.86%\n---\nHits@10\nRun: 02, Epoch: 80, Loss: 0.2882, Train: 39.69%, Valid: 35.02%, Test: 8.49%\nHits@20\nRun: 02, Epoch: 80, Loss: 0.2882, Train: 52.24%, Valid: 47.39%, Test: 25.95%\n---\nHits@10\nRun: 02, Epoch: 85, Loss: 0.2808, Train: 39.52%, Valid: 34.35%, Test: 9.81%\nHits@20\nRun: 02, Epoch: 85, Loss: 0.2808, Train: 52.61%, Valid: 46.22%, Test: 21.01%\n---\nHits@10\nRun: 02, Epoch: 90, Loss: 0.2776, Train: 33.94%, Valid: 29.50%, Test: 10.94%\nHits@20\nRun: 02, Epoch: 90, Loss: 0.2776, Train: 50.37%, Valid: 44.45%, Test: 24.90%\n---\nHits@10\nRun: 02, Epoch: 95, Loss: 0.2774, Train: 43.88%, Valid: 38.37%, Test: 17.23%\nHits@20\nRun: 02, Epoch: 95, Loss: 0.2774, Train: 51.86%, Valid: 45.64%, Test: 24.22%\n---\nHits@10\nRun: 02, Epoch: 100, Loss: 0.2771, Train: 43.04%, Valid: 37.73%, Test: 4.27%\nHits@20\nRun: 02, Epoch: 100, Loss: 0.2771, Train: 49.38%, Valid: 43.58%, Test: 16.79%\n---\nVal_hits@20:  [0.4828937215800553, 0.47388923431893265]\nTest_hits@20:  [0.18534860550307516, 0.2594970372090584]\nTrain_hits@20:  [0.5525399096554773, 0.5224175774782941]\nBest model val hits@20:  0.4828937215800553\nBest model test hits@20:  0.18534860550307516\nBest model train hits@20:  0.4828937215800553\nAverage best train hits@20: 0.5374787435668857; var: 0.00022683872394814138\nAverage best val hits@20: 0.478391477949494; var: 2.0270197708929924e-05\nAverage best test hits@20: 0.22242282135606678; var: 0.0013744974811142159\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"# Loading GAT trained on Features\ngat_model = GAT(in_channels = gnn_args['hidden_size'],hidden_channels = 32,out_channels = gnn_args['hidden_size'],\n              num_layers = gnn_args['num_layers'], dropout = gnn_args['dropout'], heads = 1).to(device)\nload_model_ckpt(gat_model, model_name,0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T06:46:06.017789Z","iopub.execute_input":"2025-05-29T06:46:06.018486Z","iopub.status.idle":"2025-05-29T06:46:06.476880Z","shell.execute_reply.started":"2025-05-29T06:46:06.018457Z","shell.execute_reply":"2025-05-29T06:46:06.476167Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n  warnings.warn(msg)\n/usr/local/lib/python3.11/dist-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n  warnings.warn(out)\n","output_type":"stream"},{"name":"stdout","text":"Hits@10\nTrain: 39.69%, Valid: 35.02%, Test: 8.49%\nHits@20\nTrain: 52.24%, Valid: 47.39%, Test: 25.95%\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"from huggingface_hub import HfApi, HfFolder, upload_file, create_repo, hf_hub_download","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T06:53:00.562123Z","iopub.execute_input":"2025-05-29T06:53:00.562445Z","iopub.status.idle":"2025-05-29T06:53:00.566711Z","shell.execute_reply.started":"2025-05-29T06:53:00.562420Z","shell.execute_reply":"2025-05-29T06:53:00.565966Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"from huggingface_hub import login","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T06:53:08.383376Z","iopub.execute_input":"2025-05-29T06:53:08.383837Z","iopub.status.idle":"2025-05-29T06:53:08.391454Z","shell.execute_reply.started":"2025-05-29T06:53:08.383817Z","shell.execute_reply":"2025-05-29T06:53:08.390749Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"\nlogin()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T06:53:14.346078Z","iopub.execute_input":"2025-05-29T06:53:14.346365Z","iopub.status.idle":"2025-05-29T06:53:14.363177Z","shell.execute_reply.started":"2025-05-29T06:53:14.346344Z","shell.execute_reply":"2025-05-29T06:53:14.362323Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24f5844c5fa24145a331e8acd04f160f"}},"metadata":{}}],"execution_count":36},{"cell_type":"code","source":"file_path = hf_hub_download(\n    repo_id=\"Rzoro/ogb_ddi\",  # Replace with your model repo\n    filename=\"node2vec_state_dict.pth\",          # Replace with your model file\n    repo_type=\"model\"                         # Could be 'dataset' or 'space' too\n)\n\nprint(\"Downloaded to:\", file_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T06:53:57.298816Z","iopub.execute_input":"2025-05-29T06:53:57.299151Z","iopub.status.idle":"2025-05-29T06:53:57.560360Z","shell.execute_reply.started":"2025-05-29T06:53:57.299129Z","shell.execute_reply":"2025-05-29T06:53:57.559764Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"node2vec_state_dict.pth:   0%|          | 0.00/4.37M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82a618900f914bab8e1f7a20fdddc480"}},"metadata":{}},{"name":"stdout","text":"Downloaded to: /root/.cache/huggingface/hub/models--Rzoro--ogb_ddi/snapshots/73c5c4f0b4718e53e29226ecf164d2aeddd832a6/node2vec_state_dict.pth\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"# training function for node2vec using PyG's Node2Vec function\ndef train_Node2Vec(args, data, filepath):\n  model = Node2Vec(data.edge_index, args['embedding_dim'], args['walk_length'],\n                    args['context_size'], args['walks_per_node'],\n                    sparse=True).to(device)\n\n  loader = model.loader(batch_size=args['batch_size'], shuffle=True,\n                        num_workers=4)\n  optimizer = torch.optim.SparseAdam(list(model.parameters()), lr=args['lr'])\n\n  model.train()\n  for epoch in range(1, args['epochs'] + 1):\n      for i, (pos_rw, neg_rw) in enumerate(loader):\n          optimizer.zero_grad()\n          loss = model.loss(pos_rw.to(device), neg_rw.to(device))\n          loss.backward()\n          optimizer.step()\n\n          if (i + 1) % args['log_steps'] == 0:\n              print(f'Epoch: {epoch:02d}, Step: {i+1:03d}/{len(loader)}, '\n                    f'Loss: {loss:.4f}')\n\n          if (i + 1) % 100 == 0:  # Save model every 100 steps.\n              save_embedding(model, filepath)\n      save_embedding(model, filepath)\n  return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T06:54:15.227371Z","iopub.execute_input":"2025-05-29T06:54:15.227939Z","iopub.status.idle":"2025-05-29T06:54:15.233720Z","shell.execute_reply.started":"2025-05-29T06:54:15.227912Z","shell.execute_reply":"2025-05-29T06:54:15.233024Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"def load_node2vec(args, data, filepath):\n    model = Node2Vec(data.edge_index, args['embedding_dim'], args['walk_length'],\n                    args['context_size'], args['walks_per_node'],\n                    sparse=True).to(device)\n    loader = model.loader(batch_size=args['batch_size'], shuffle=True,\n                        num_workers=4)\n    optimizer = torch.optim.SparseAdam(list(model.parameters()), lr=args['lr'])\n\n    model.load_state_dict(torch.load(filepath))\n\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T06:54:24.012474Z","iopub.execute_input":"2025-05-29T06:54:24.012732Z","iopub.status.idle":"2025-05-29T06:54:24.017753Z","shell.execute_reply.started":"2025-05-29T06:54:24.012711Z","shell.execute_reply":"2025-05-29T06:54:24.017105Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"node2vec_args = {'device':0, 'embedding_dim':256, 'walk_length':40, 'context_size':20, 'walks_per_node':10,\n      'batch_size':256, 'lr':0.01, 'epochs':100, 'log_steps':1}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T06:55:31.430674Z","iopub.execute_input":"2025-05-29T06:55:31.430964Z","iopub.status.idle":"2025-05-29T06:55:31.434830Z","shell.execute_reply.started":"2025-05-29T06:55:31.430943Z","shell.execute_reply":"2025-05-29T06:55:31.434158Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"new_node2vec2 = load_node2vec(node2vec_args, data_node2vec, file_path)\npretrained_weight = new_node2vec2.embedding.weight.data.cpu()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T06:57:07.794370Z","iopub.execute_input":"2025-05-29T06:57:07.794941Z","iopub.status.idle":"2025-05-29T06:57:07.891372Z","shell.execute_reply.started":"2025-05-29T06:57:07.794906Z","shell.execute_reply":"2025-05-29T06:57:07.890581Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"upload_file(\n    path_or_fileobj=\"/kaggle/working/training_outputs/gat_rand_feat.pt\",\n    path_in_repo=\"gat_rand_feat.pt\",\n    repo_id=\"Rzoro/ogb_ddi\",\n    repo_type=\"model\"\n)\nupload_file(\n    path_or_fileobj=\"/kaggle/working/training_outputs/gat_rand_feat_final_emb_0.pt\",\n    path_in_repo=\"gat_rand_feat_final_emb_0.pt\",\n    repo_id=\"Rzoro/ogb_ddi\",\n    repo_type=\"model\"\n)\nupload_file(\n    path_or_fileobj=\"/kaggle/working/training_outputs/gat_rand_feat_init_emb.pt\",\n    path_in_repo=\"gat_rand_feat_init_emb.pt\",\n    repo_id=\"Rzoro/ogb_ddi\",\n    repo_type=\"model\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T06:56:30.795696Z","iopub.execute_input":"2025-05-29T06:56:30.796329Z","iopub.status.idle":"2025-05-29T06:56:33.670614Z","shell.execute_reply.started":"2025-05-29T06:56:30.796302Z","shell.execute_reply":"2025-05-29T06:56:33.669909Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"gat_rand_feat.pt:   0%|          | 0.00/9.75M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b0a87a58e74487583432e79bcb78710"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"gat_rand_feat_final_emb_0.pt:   0%|          | 0.00/4.37M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cea6ea286811406b951761ffe408f56d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"gat_rand_feat_init_emb.pt:   0%|          | 0.00/4.37M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"658fdfa6df6e40eeb8c0b0a2e0cabc26"}},"metadata":{}},{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/Rzoro/ogb_ddi/commit/652965602ec912a722be42da2edfbc99888b982c', commit_message='Upload gat_rand_feat_init_emb.pt with huggingface_hub', commit_description='', oid='652965602ec912a722be42da2edfbc99888b982c', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Rzoro/ogb_ddi', endpoint='https://huggingface.co', repo_type='model', repo_id='Rzoro/ogb_ddi'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":43},{"cell_type":"code","source":"# Train GAT with node2vec embedding features\nmodel_name = \"gat_node2vec_feat_256\"\ngat_model2 = GAT(in_channels = gnn_args['hidden_size'],hidden_channels = 32,out_channels = gnn_args['hidden_size'],\n              num_layers = gnn_args['num_layers'], dropout = gnn_args['dropout'], heads = 1).to(device)\n\ngat_predictor2 = LinkPredictor(gnn_args['hidden_size'], gnn_args['hidden_size'], 1,\n                          gnn_args['num_layers'], gnn_args['dropout']).to(device)\ngat_emb_node2vec = torch.nn.Embedding(dataset.data.num_nodes, gnn_args['hidden_size']).to(device)\ngat_emb_node2vec.weight.data.copy_(pretrained_weight)\ntrain_model(gat_model2, gat_emb_node2vec, gnn_args, gat_predictor2, model_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T06:59:15.498730Z","iopub.execute_input":"2025-05-29T06:59:15.499344Z","iopub.status.idle":"2025-05-29T07:50:01.093159Z","shell.execute_reply.started":"2025-05-29T06:59:15.499320Z","shell.execute_reply":"2025-05-29T07:50:01.092330Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n  warnings.warn(out)\n","output_type":"stream"},{"name":"stdout","text":"Hits@10\nRun: 01, Epoch: 05, Loss: 0.5395, Train: 20.90%, Valid: 19.73%, Test: 7.79%\nHits@20\nRun: 01, Epoch: 05, Loss: 0.5395, Train: 27.81%, Valid: 26.50%, Test: 10.66%\n---\nHits@10\nRun: 01, Epoch: 10, Loss: 0.4246, Train: 25.68%, Valid: 23.78%, Test: 9.45%\nHits@20\nRun: 01, Epoch: 10, Loss: 0.4246, Train: 31.74%, Valid: 29.56%, Test: 17.64%\n---\nHits@10\nRun: 01, Epoch: 15, Loss: 0.3880, Train: 31.81%, Valid: 29.09%, Test: 8.50%\nHits@20\nRun: 01, Epoch: 15, Loss: 0.3880, Train: 38.39%, Valid: 35.68%, Test: 17.95%\n---\nHits@10\nRun: 01, Epoch: 20, Loss: 0.3601, Train: 30.15%, Valid: 27.25%, Test: 8.23%\nHits@20\nRun: 01, Epoch: 20, Loss: 0.3601, Train: 37.02%, Valid: 33.92%, Test: 15.36%\n---\nHits@10\nRun: 01, Epoch: 25, Loss: 0.3376, Train: 27.04%, Valid: 24.03%, Test: 10.53%\nHits@20\nRun: 01, Epoch: 25, Loss: 0.3376, Train: 34.88%, Valid: 31.39%, Test: 19.16%\n---\nHits@10\nRun: 01, Epoch: 30, Loss: 0.3293, Train: 30.23%, Valid: 26.94%, Test: 8.02%\nHits@20\nRun: 01, Epoch: 30, Loss: 0.3293, Train: 37.78%, Valid: 34.03%, Test: 17.24%\n---\nHits@10\nRun: 01, Epoch: 35, Loss: 0.3194, Train: 28.20%, Valid: 24.94%, Test: 8.03%\nHits@20\nRun: 01, Epoch: 35, Loss: 0.3194, Train: 36.71%, Valid: 32.91%, Test: 24.02%\n---\nHits@10\nRun: 01, Epoch: 40, Loss: 0.3102, Train: 33.19%, Valid: 29.20%, Test: 11.41%\nHits@20\nRun: 01, Epoch: 40, Loss: 0.3102, Train: 42.58%, Valid: 37.54%, Test: 21.31%\n---\nHits@10\nRun: 01, Epoch: 45, Loss: 0.3054, Train: 24.64%, Valid: 21.52%, Test: 14.31%\nHits@20\nRun: 01, Epoch: 45, Loss: 0.3054, Train: 40.30%, Valid: 35.65%, Test: 22.14%\n---\nHits@10\nRun: 01, Epoch: 50, Loss: 0.2968, Train: 36.11%, Valid: 31.68%, Test: 13.77%\nHits@20\nRun: 01, Epoch: 50, Loss: 0.2968, Train: 44.09%, Valid: 38.98%, Test: 28.06%\n---\nHits@10\nRun: 01, Epoch: 55, Loss: 0.2901, Train: 25.74%, Valid: 22.26%, Test: 11.90%\nHits@20\nRun: 01, Epoch: 55, Loss: 0.2901, Train: 33.42%, Valid: 29.05%, Test: 21.36%\n---\nHits@10\nRun: 01, Epoch: 60, Loss: 0.2867, Train: 39.79%, Valid: 34.90%, Test: 20.76%\nHits@20\nRun: 01, Epoch: 60, Loss: 0.2867, Train: 52.13%, Valid: 46.08%, Test: 30.24%\n---\nHits@10\nRun: 01, Epoch: 65, Loss: 0.2821, Train: 42.60%, Valid: 37.31%, Test: 13.09%\nHits@20\nRun: 01, Epoch: 65, Loss: 0.2821, Train: 50.67%, Valid: 44.40%, Test: 20.55%\n---\nHits@10\nRun: 01, Epoch: 70, Loss: 0.2799, Train: 36.75%, Valid: 31.88%, Test: 7.67%\nHits@20\nRun: 01, Epoch: 70, Loss: 0.2799, Train: 43.44%, Valid: 37.87%, Test: 15.47%\n---\nHits@10\nRun: 01, Epoch: 75, Loss: 0.2795, Train: 36.25%, Valid: 31.47%, Test: 3.71%\nHits@20\nRun: 01, Epoch: 75, Loss: 0.2795, Train: 45.56%, Valid: 39.93%, Test: 12.83%\n---\nHits@10\nRun: 01, Epoch: 80, Loss: 0.2743, Train: 42.76%, Valid: 37.26%, Test: 10.19%\nHits@20\nRun: 01, Epoch: 80, Loss: 0.2743, Train: 51.57%, Valid: 45.22%, Test: 24.79%\n---\nHits@10\nRun: 01, Epoch: 85, Loss: 0.2733, Train: 42.45%, Valid: 37.19%, Test: 17.59%\nHits@20\nRun: 01, Epoch: 85, Loss: 0.2733, Train: 54.24%, Valid: 47.53%, Test: 31.14%\n---\nHits@10\nRun: 01, Epoch: 90, Loss: 0.2717, Train: 41.05%, Valid: 35.63%, Test: 13.44%\nHits@20\nRun: 01, Epoch: 90, Loss: 0.2717, Train: 53.03%, Valid: 46.52%, Test: 27.27%\n---\nHits@10\nRun: 01, Epoch: 95, Loss: 0.2682, Train: 35.98%, Valid: 31.17%, Test: 5.96%\nHits@20\nRun: 01, Epoch: 95, Loss: 0.2682, Train: 50.93%, Valid: 44.60%, Test: 13.99%\n---\nHits@10\nRun: 01, Epoch: 100, Loss: 0.2689, Train: 35.92%, Valid: 31.05%, Test: 5.00%\nHits@20\nRun: 01, Epoch: 100, Loss: 0.2689, Train: 49.22%, Valid: 43.08%, Test: 11.85%\n---\nHits@10\nRun: 02, Epoch: 05, Loss: 0.5857, Train: 7.41%, Valid: 6.79%, Test: 2.25%\nHits@20\nRun: 02, Epoch: 05, Loss: 0.5857, Train: 19.39%, Valid: 18.64%, Test: 4.89%\n---\nHits@10\nRun: 02, Epoch: 10, Loss: 0.4402, Train: 11.68%, Valid: 10.78%, Test: 5.86%\nHits@20\nRun: 02, Epoch: 10, Loss: 0.4402, Train: 21.65%, Valid: 20.28%, Test: 8.45%\n---\nHits@10\nRun: 02, Epoch: 15, Loss: 0.3939, Train: 24.91%, Valid: 22.47%, Test: 5.70%\nHits@20\nRun: 02, Epoch: 15, Loss: 0.3939, Train: 32.39%, Valid: 29.81%, Test: 13.32%\n---\nHits@10\nRun: 02, Epoch: 20, Loss: 0.3722, Train: 34.09%, Valid: 31.37%, Test: 6.96%\nHits@20\nRun: 02, Epoch: 20, Loss: 0.3722, Train: 40.88%, Valid: 38.05%, Test: 17.45%\n---\nHits@10\nRun: 02, Epoch: 25, Loss: 0.3467, Train: 28.82%, Valid: 25.96%, Test: 10.38%\nHits@20\nRun: 02, Epoch: 25, Loss: 0.3467, Train: 41.16%, Valid: 37.97%, Test: 20.05%\n---\nHits@10\nRun: 02, Epoch: 30, Loss: 0.3330, Train: 33.26%, Valid: 29.87%, Test: 9.84%\nHits@20\nRun: 02, Epoch: 30, Loss: 0.3330, Train: 43.58%, Valid: 39.88%, Test: 23.12%\n---\nHits@10\nRun: 02, Epoch: 35, Loss: 0.3191, Train: 25.72%, Valid: 22.64%, Test: 14.54%\nHits@20\nRun: 02, Epoch: 35, Loss: 0.3191, Train: 39.15%, Valid: 35.00%, Test: 25.33%\n---\nHits@10\nRun: 02, Epoch: 40, Loss: 0.3128, Train: 26.74%, Valid: 23.31%, Test: 18.85%\nHits@20\nRun: 02, Epoch: 40, Loss: 0.3128, Train: 36.88%, Valid: 32.49%, Test: 27.68%\n---\nHits@10\nRun: 02, Epoch: 45, Loss: 0.3039, Train: 31.52%, Valid: 27.39%, Test: 16.74%\nHits@20\nRun: 02, Epoch: 45, Loss: 0.3039, Train: 40.82%, Valid: 35.91%, Test: 25.84%\n---\nHits@10\nRun: 02, Epoch: 50, Loss: 0.2956, Train: 37.56%, Valid: 33.00%, Test: 17.16%\nHits@20\nRun: 02, Epoch: 50, Loss: 0.2956, Train: 45.33%, Valid: 40.12%, Test: 30.13%\n---\nHits@10\nRun: 02, Epoch: 55, Loss: 0.2935, Train: 30.01%, Valid: 26.27%, Test: 22.18%\nHits@20\nRun: 02, Epoch: 55, Loss: 0.2935, Train: 41.36%, Valid: 36.32%, Test: 29.02%\n---\nHits@10\nRun: 02, Epoch: 60, Loss: 0.2860, Train: 35.97%, Valid: 31.39%, Test: 14.28%\nHits@20\nRun: 02, Epoch: 60, Loss: 0.2860, Train: 43.80%, Valid: 38.32%, Test: 24.21%\n---\nHits@10\nRun: 02, Epoch: 65, Loss: 0.2828, Train: 36.02%, Valid: 31.25%, Test: 20.54%\nHits@20\nRun: 02, Epoch: 65, Loss: 0.2828, Train: 42.24%, Valid: 36.87%, Test: 32.06%\n---\nHits@10\nRun: 02, Epoch: 70, Loss: 0.2799, Train: 29.80%, Valid: 25.73%, Test: 19.79%\nHits@20\nRun: 02, Epoch: 70, Loss: 0.2799, Train: 36.77%, Valid: 31.99%, Test: 29.84%\n---\nHits@10\nRun: 02, Epoch: 75, Loss: 0.2779, Train: 35.17%, Valid: 30.38%, Test: 9.77%\nHits@20\nRun: 02, Epoch: 75, Loss: 0.2779, Train: 44.87%, Valid: 39.10%, Test: 22.49%\n---\nHits@10\nRun: 02, Epoch: 80, Loss: 0.2726, Train: 36.87%, Valid: 31.98%, Test: 12.87%\nHits@20\nRun: 02, Epoch: 80, Loss: 0.2726, Train: 48.71%, Valid: 42.51%, Test: 20.80%\n---\nHits@10\nRun: 02, Epoch: 85, Loss: 0.2694, Train: 34.05%, Valid: 29.52%, Test: 6.37%\nHits@20\nRun: 02, Epoch: 85, Loss: 0.2694, Train: 52.61%, Valid: 46.12%, Test: 14.51%\n---\nHits@10\nRun: 02, Epoch: 90, Loss: 0.2694, Train: 43.41%, Valid: 37.59%, Test: 11.48%\nHits@20\nRun: 02, Epoch: 90, Loss: 0.2694, Train: 51.02%, Valid: 44.45%, Test: 23.43%\n---\nHits@10\nRun: 02, Epoch: 95, Loss: 0.2683, Train: 34.12%, Valid: 29.42%, Test: 13.05%\nHits@20\nRun: 02, Epoch: 95, Loss: 0.2683, Train: 50.10%, Valid: 43.77%, Test: 23.98%\n---\nHits@10\nRun: 02, Epoch: 100, Loss: 0.2649, Train: 30.37%, Valid: 26.01%, Test: 6.27%\nHits@20\nRun: 02, Epoch: 100, Loss: 0.2649, Train: 41.26%, Valid: 35.56%, Test: 30.23%\n---\nVal_hits@20:  [0.47526762504775677, 0.46124399763276375]\nTest_hits@20:  [0.3113664796350261, 0.1451130804785413]\nTrain_hits@20:  [0.5423742780303995, 0.5261182569350283]\nBest model val hits@20:  0.47526762504775677\nBest model test hits@20:  0.3113664796350261\nBest model train hits@20:  0.47526762504775677\nAverage best train hits@20: 0.534246267482714; var: 6.606455546328809e-05\nAverage best val hits@20: 0.4682558113402603; var: 4.916553146863597e-05\nAverage best test hits@20: 0.2282397800567837; var: 0.006910048182771367\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"# Loading GAT trained on Features\ngat_model2 = GAT(in_channels = gnn_args['hidden_size'],hidden_channels = 32,out_channels = gnn_args['hidden_size'],\n              num_layers = gnn_args['num_layers'], dropout = gnn_args['dropout'], heads = 1).to(device)\nload_model_ckpt(gat_model2, model_name,0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T07:50:01.094440Z","iopub.execute_input":"2025-05-29T07:50:01.094651Z","iopub.status.idle":"2025-05-29T07:50:01.560120Z","shell.execute_reply.started":"2025-05-29T07:50:01.094635Z","shell.execute_reply":"2025-05-29T07:50:01.559348Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n  warnings.warn(msg)\n/usr/local/lib/python3.11/dist-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n  warnings.warn(out)\n","output_type":"stream"},{"name":"stdout","text":"Hits@10\nTrain: 34.05%, Valid: 29.52%, Test: 6.37%\nHits@20\nTrain: 52.61%, Valid: 46.12%, Test: 14.51%\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"upload_file(\n    path_or_fileobj=\"/kaggle/working/training_outputs/gat_node2vec_feat_256.pt\",\n    path_in_repo=\"gat_node2vec_feat_256.pt\",\n    repo_id=\"Rzoro/ogb_ddi\",\n    repo_type=\"model\"\n)\nupload_file(\n    path_or_fileobj=\"/kaggle/working/training_outputs/gat_node2vec_feat_256_final_emb_0.pt\",\n    path_in_repo=\"gat_node2vec_feat_256_final_emb_0.pt\",\n    repo_id=\"Rzoro/ogb_ddi\",\n    repo_type=\"model\"\n)\nupload_file(\n    path_or_fileobj=\"/kaggle/working/training_outputs/gat_node2vec_feat_256_init_emb.pt\",\n    path_in_repo=\"gat_node2vec_feat_256_init_emb.pt\",\n    repo_id=\"Rzoro/ogb_ddi\",\n    repo_type=\"model\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T08:01:41.278910Z","iopub.execute_input":"2025-05-29T08:01:41.279227Z","iopub.status.idle":"2025-05-29T08:01:44.329301Z","shell.execute_reply.started":"2025-05-29T08:01:41.279189Z","shell.execute_reply":"2025-05-29T08:01:44.328706Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"gat_node2vec_feat_256.pt:   0%|          | 0.00/9.75M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40c7ed252cbb4c139989f9388c465cb0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"gat_node2vec_feat_256_final_emb_0.pt:   0%|          | 0.00/4.37M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ddf89289ac245a6b4f9a326224603c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"gat_node2vec_feat_256_init_emb.pt:   0%|          | 0.00/4.37M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b55b22458bdd4e1b9990e0debbc154f7"}},"metadata":{}},{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/Rzoro/ogb_ddi/commit/4fb9fbe9622d2562751bad502b91e7498dd8a90f', commit_message='Upload gat_node2vec_feat_256_init_emb.pt with huggingface_hub', commit_description='', oid='4fb9fbe9622d2562751bad502b91e7498dd8a90f', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Rzoro/ogb_ddi', endpoint='https://huggingface.co', repo_type='model', repo_id='Rzoro/ogb_ddi'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":48},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}