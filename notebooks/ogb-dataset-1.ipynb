{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch==2.2.2\n!pip install torch-cluster==1.6.3\n!pip install torch-geometric==2.6.1\n!pip install torch-scatter==2.1.2\n!pip install torch-sparse==0.6.18\n!pip install torch-spline-conv==1.2.2","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-28T04:28:28.684750Z","iopub.execute_input":"2025-05-28T04:28:28.684950Z","iopub.status.idle":"2025-05-28T05:30:06.052970Z","shell.execute_reply.started":"2025-05-28T04:28:28.684935Z","shell.execute_reply":"2025-05-28T05:30:06.052027Z"}},"outputs":[{"name":"stdout","text":"Collecting torch==2.2.2\n  Downloading torch-2.2.2-cp311-cp311-manylinux1_x86_64.whl.metadata (25 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (4.13.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (1.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (2025.3.2)\nCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2.2)\n  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2.2)\n  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2.2)\n  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.2)\n  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2.2)\n  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2.2)\n  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2.2)\n  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2.2)\n  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2.2)\n  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.2)\n  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2.2)\n  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\nCollecting triton==2.2.0 (from torch==2.2.2)\n  Downloading triton-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.2) (12.9.41)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.2.2) (3.0.2)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.2.2) (1.3.0)\nDownloading torch-2.2.2-cp311-cp311-manylinux1_x86_64.whl (755.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.6/755.6 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m95.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m78.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading triton-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch\n  Attempting uninstall: triton\n    Found existing installation: triton 3.2.0\n    Uninstalling triton-3.2.0:\n      Successfully uninstalled triton-3.2.0\n  Attempting uninstall: nvidia-nvtx-cu12\n    Found existing installation: nvidia-nvtx-cu12 12.4.127\n    Uninstalling nvidia-nvtx-cu12-12.4.127:\n      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n  Attempting uninstall: nvidia-nccl-cu12\n    Found existing installation: nvidia-nccl-cu12 2.21.5\n    Uninstalling nvidia-nccl-cu12-2.21.5:\n      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\n    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.10.19\n    Uninstalling nvidia-curand-cu12-10.3.10.19:\n      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.4.0.6\n    Uninstalling nvidia-cufft-cu12-11.4.0.6:\n      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.4.127\n    Uninstalling nvidia-cuda-runtime-cu12-12.4.127:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.4.127\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.4.127\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.4.127:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.4.127\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.4.127\n    Uninstalling nvidia-cuda-cupti-cu12-12.4.127:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.4.127\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.9.0.13\n    Uninstalling nvidia-cublas-cu12-12.9.0.13:\n      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\n    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: torch\n    Found existing installation: torch 2.6.0+cu124\n    Uninstalling torch-2.6.0+cu124:\n      Successfully uninstalled torch-2.6.0+cu124\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntorchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.2.2 which is incompatible.\ntorchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvtx-cu12-12.1.105 torch-2.2.2 triton-2.2.0\nCollecting torch-cluster==1.6.3\n  Downloading torch_cluster-1.6.3.tar.gz (54 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-cluster==1.6.3) (1.15.2)\nRequirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy->torch-cluster==1.6.3) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy->torch-cluster==1.6.3) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy->torch-cluster==1.6.3) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy->torch-cluster==1.6.3) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy->torch-cluster==1.6.3) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy->torch-cluster==1.6.3) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy->torch-cluster==1.6.3) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.5,>=1.23.5->scipy->torch-cluster==1.6.3) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.5,>=1.23.5->scipy->torch-cluster==1.6.3) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2.5,>=1.23.5->scipy->torch-cluster==1.6.3) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2.5,>=1.23.5->scipy->torch-cluster==1.6.3) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2.5,>=1.23.5->scipy->torch-cluster==1.6.3) (2024.2.0)\nBuilding wheels for collected packages: torch-cluster\n  Building wheel for torch-cluster (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for torch-cluster: filename=torch_cluster-1.6.3-cp311-cp311-linux_x86_64.whl size=2037589 sha256=7d9d5fe0747d3696af0709bb00767cd2a3df5010d371701c97705a41266fbc1b\n  Stored in directory: /root/.cache/pip/wheels/ef/de/7d/a4211822af99147b93800e9e204f0be21294e3c0b95b3b861a\nSuccessfully built torch-cluster\nInstalling collected packages: torch-cluster\nSuccessfully installed torch-cluster-1.6.3\nCollecting torch-geometric==2.6.1\n  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.6.1) (3.11.18)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.6.1) (2025.3.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.6.1) (3.1.6)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.6.1) (1.26.4)\nRequirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.6.1) (7.0.0)\nRequirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.6.1) (3.0.9)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.6.1) (2.32.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.6.1) (4.67.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.6.1) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.6.1) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.6.1) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.6.1) (1.6.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.6.1) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.6.1) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.6.1) (1.20.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric==2.6.1) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric==2.6.1) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric==2.6.1) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric==2.6.1) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric==2.6.1) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric==2.6.1) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric==2.6.1) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric==2.6.1) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric==2.6.1) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric==2.6.1) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric==2.6.1) (2025.4.26)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torch-geometric==2.6.1) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torch-geometric==2.6.1) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torch-geometric==2.6.1) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torch-geometric==2.6.1) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torch-geometric==2.6.1) (2024.2.0)\nDownloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: torch-geometric\nSuccessfully installed torch-geometric-2.6.1\nCollecting torch-scatter==2.1.2\n  Downloading torch_scatter-2.1.2.tar.gz (108 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.0/108.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nBuilding wheels for collected packages: torch-scatter\n  Building wheel for torch-scatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for torch-scatter: filename=torch_scatter-2.1.2-cp311-cp311-linux_x86_64.whl size=3541199 sha256=c66c3006b2a50fe2b49d38cf609deb4fcc56b25cc59e0748e78cd0a286e732c4\n  Stored in directory: /root/.cache/pip/wheels/b8/d4/0e/a80af2465354ea7355a2c153b11af2da739cfcf08b6c0b28e2\nSuccessfully built torch-scatter\nInstalling collected packages: torch-scatter\nSuccessfully installed torch-scatter-2.1.2\nCollecting torch-sparse==0.6.18\n  Downloading torch_sparse-0.6.18.tar.gz (209 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.0/210.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-sparse==0.6.18) (1.15.2)\nRequirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy->torch-sparse==0.6.18) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy->torch-sparse==0.6.18) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy->torch-sparse==0.6.18) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy->torch-sparse==0.6.18) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy->torch-sparse==0.6.18) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy->torch-sparse==0.6.18) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy->torch-sparse==0.6.18) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.5,>=1.23.5->scipy->torch-sparse==0.6.18) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.5,>=1.23.5->scipy->torch-sparse==0.6.18) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2.5,>=1.23.5->scipy->torch-sparse==0.6.18) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2.5,>=1.23.5->scipy->torch-sparse==0.6.18) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2.5,>=1.23.5->scipy->torch-sparse==0.6.18) (2024.2.0)\nBuilding wheels for collected packages: torch-sparse\n  Building wheel for torch-sparse (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for torch-sparse: filename=torch_sparse-0.6.18-cp311-cp311-linux_x86_64.whl size=2773959 sha256=73b6b26508fe5bce763ffa747b84df22ed4759f72933350a433ae77bb28daa34\n  Stored in directory: /root/.cache/pip/wheels/75/e2/1e/299c596063839303657c211f587f05591891cc6cf126d94d21\nSuccessfully built torch-sparse\nInstalling collected packages: torch-sparse\nSuccessfully installed torch-sparse-0.6.18\nCollecting torch-spline-conv==1.2.2\n  Downloading torch_spline_conv-1.2.2.tar.gz (25 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nBuilding wheels for collected packages: torch-spline-conv\n  Building wheel for torch-spline-conv (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for torch-spline-conv: filename=torch_spline_conv-1.2.2-cp311-cp311-linux_x86_64.whl size=525839 sha256=3b2d230ab02c16ffa3a2dfe83b1a9c8a3092ec75ec5cfbd9843fc3a34ce0b018\n  Stored in directory: /root/.cache/pip/wheels/25/16/8a/a98b0173c4fbbc7aa1c4929b46d2eb08d1475c5c7b54e289b6\nSuccessfully built torch-spline-conv\nInstalling collected packages: torch-spline-conv\nSuccessfully installed torch-spline-conv-1.2.2\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# !pip install torch-geometric\n!pip install ogb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T05:30:06.054074Z","iopub.execute_input":"2025-05-28T05:30:06.054365Z","iopub.status.idle":"2025-05-28T05:30:09.873454Z","shell.execute_reply.started":"2025-05-28T05:30:06.054326Z","shell.execute_reply":"2025-05-28T05:30:09.872601Z"}},"outputs":[{"name":"stdout","text":"Collecting ogb\n  Downloading ogb-1.3.6-py3-none-any.whl.metadata (6.2 kB)\nRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from ogb) (2.2.2)\nRequirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from ogb) (1.26.4)\nRequirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.11/dist-packages (from ogb) (4.67.1)\nRequirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from ogb) (1.2.2)\nRequirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from ogb) (2.2.3)\nRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from ogb) (1.17.0)\nRequirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.11/dist-packages (from ogb) (2.4.0)\nCollecting outdated>=0.2.0 (from ogb)\n  Downloading outdated-0.2.2-py2.py3-none-any.whl.metadata (4.7 kB)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.0->ogb) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.0->ogb) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.0->ogb) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.0->ogb) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.0->ogb) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.0->ogb) (2.4.1)\nRequirement already satisfied: setuptools>=44 in /usr/local/lib/python3.11/dist-packages (from outdated>=0.2.0->ogb) (75.2.0)\nCollecting littleutils (from outdated>=0.2.0->ogb)\n  Downloading littleutils-0.2.4-py3-none-any.whl.metadata (679 bytes)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from outdated>=0.2.0->ogb) (2.32.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24.0->ogb) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24.0->ogb) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24.0->ogb) (2025.2)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.0->ogb) (1.15.2)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.0->ogb) (1.5.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.0->ogb) (3.6.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (4.13.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (1.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (12.1.105)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (12.1.105)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (12.1.105)\nRequirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (8.9.2.26)\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (12.1.3.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (11.0.2.54)\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (10.3.2.106)\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (11.4.5.107)\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (12.1.0.106)\nRequirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (2.19.3)\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (12.1.105)\nRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (2.2.0)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.6.0->ogb) (12.9.41)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.6.0->ogb) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.16.0->ogb) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.16.0->ogb) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.16.0->ogb) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.16.0->ogb) (2024.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->outdated>=0.2.0->ogb) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->outdated>=0.2.0->ogb) (3.10)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->outdated>=0.2.0->ogb) (2025.4.26)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch>=1.6.0->ogb) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.16.0->ogb) (2024.2.0)\nDownloading ogb-1.3.6-py3-none-any.whl (78 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\nDownloading littleutils-0.2.4-py3-none-any.whl (8.1 kB)\nInstalling collected packages: littleutils, outdated, ogb\nSuccessfully installed littleutils-0.2.4 ogb-1.3.6 outdated-0.2.2\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# !pip install torch-scatter \n# !pip install torch-sparse\n\n# !pip install torch-scatter -f https://data.pyg.org/whl/torch-1.10.0+cu111.html\n# !pip install torch-sparse -f https://data.pyg.org/whl/torch-1.10.0+cu111.html\n# !pip install torch-geometric\n# !pip install -q git+https://github.com/snap-stanford/deepsnap.git\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T05:30:09.876098Z","iopub.execute_input":"2025-05-28T05:30:09.876332Z","iopub.status.idle":"2025-05-28T05:30:09.879900Z","shell.execute_reply.started":"2025-05-28T05:30:09.876308Z","shell.execute_reply":"2025-05-28T05:30:09.879325Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# !pip install -q git+https://github.com/snap-stanford/deepsnap.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T05:30:09.880683Z","iopub.execute_input":"2025-05-28T05:30:09.880935Z","iopub.status.idle":"2025-05-28T05:30:09.895446Z","shell.execute_reply.started":"2025-05-28T05:30:09.880910Z","shell.execute_reply":"2025-05-28T05:30:09.894781Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# !pip install torch-cluster","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T05:30:09.896379Z","iopub.execute_input":"2025-05-28T05:30:09.896622Z","iopub.status.idle":"2025-05-28T05:30:09.909263Z","shell.execute_reply.started":"2025-05-28T05:30:09.896600Z","shell.execute_reply":"2025-05-28T05:30:09.908691Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# !pip install --verbose git+https://github.com/pyg-team/pyg-lib.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T05:30:09.909898Z","iopub.execute_input":"2025-05-28T05:30:09.910118Z","iopub.status.idle":"2025-05-28T05:30:09.925169Z","shell.execute_reply.started":"2025-05-28T05:30:09.910099Z","shell.execute_reply":"2025-05-28T05:30:09.924629Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Importing necessary dependencies in order to import our dataset, create our\n# GCN models, and evaluate the models\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn import Sequential, Linear, BatchNorm1d, ReLU\n\nfrom torch_geometric.utils import negative_sampling\nfrom torch_geometric.data import Data\n\nimport torch_geometric.transforms as T\nfrom torch_geometric.nn import GCNConv, SAGEConv, GINConv, GATConv\n\nfrom ogb.linkproppred import PygLinkPropPredDataset, Evaluator\nfrom torch_geometric.data import DataLoader\nfrom torch_geometric.data.data import DataEdgeAttr, GlobalStorage\n\nfrom torch_geometric.nn import Node2Vec\n\nimport pandas as pd\nimport shutil, os\nimport os.path as osp\nimport numpy as np\n\n#from logger import Logger\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nimport random\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T05:30:09.925965Z","iopub.execute_input":"2025-05-28T05:30:09.926246Z","iopub.status.idle":"2025-05-28T05:30:18.743756Z","shell.execute_reply.started":"2025-05-28T05:30:09.926227Z","shell.execute_reply":"2025-05-28T05:30:18.743159Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\n# If you use GPU, the device should be cuda\nprint('Device: {}'.format(device))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T05:30:18.744454Z","iopub.execute_input":"2025-05-28T05:30:18.744924Z","iopub.status.idle":"2025-05-28T05:30:18.749376Z","shell.execute_reply.started":"2025-05-28T05:30:18.744905Z","shell.execute_reply":"2025-05-28T05:30:18.748722Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# loaded with transform parameter set as such in order to obtain the adj_t matrix\n# required for the GNN layers\n# torch.serialization.add_safe_globals([DataEdgeAttr])\n# torch.serialization.add_safe_globals([GlobalStorage])\ndataset = PygLinkPropPredDataset(name='ogbl-ddi', root ='./', transform=T.ToSparseTensor()) # loading ogb-ddi\nprint('Task type: {}'.format(dataset.task_type))\ngraph = dataset[0]\nadj_t = graph.adj_t.to(device) # loads all edges in graph into sparse adj_t matrix","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T05:30:18.751642Z","iopub.execute_input":"2025-05-28T05:30:18.751859Z","iopub.status.idle":"2025-05-28T05:30:22.161515Z","shell.execute_reply.started":"2025-05-28T05:30:18.751843Z","shell.execute_reply":"2025-05-28T05:30:22.160960Z"}},"outputs":[{"name":"stdout","text":"Downloading http://snap.stanford.edu/ogb/data/linkproppred/ddi.zip\n","output_type":"stream"},{"name":"stderr","text":"Downloaded 0.04 GB: 100%|██████████| 46/46 [00:01<00:00, 23.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ./ddi.zip\n","output_type":"stream"},{"name":"stderr","text":"Processing...\n","output_type":"stream"},{"name":"stdout","text":"Loading necessary files...\nThis might take a while.\nProcessing graphs...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00, 58.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Converting graphs into PyG objects...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00, 3264.05it/s]","output_type":"stream"},{"name":"stdout","text":"Saving...\nTask type: link prediction\n","output_type":"stream"},{"name":"stderr","text":"\nDone!\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# getting the train, validation, and test edge splits\nsplit_edge = dataset.get_edge_split()\ntrain_edges = split_edge['train']['edge']\ntorch.manual_seed(70) # picking random samples to evaluate on\nidx = torch.randperm(split_edge['train']['edge'].size(0))\nidx = idx[:split_edge['valid']['edge'].size(0)]\nsplit_edge['eval_train'] = {'edge': split_edge['train']['edge'][idx]}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T05:30:22.162224Z","iopub.execute_input":"2025-05-28T05:30:22.162497Z","iopub.status.idle":"2025-05-28T05:30:22.270184Z","shell.execute_reply.started":"2025-05-28T05:30:22.162475Z","shell.execute_reply":"2025-05-28T05:30:22.269610Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"train_edges_node2vec = train_edges.T # transpose to get the right dimension\n# Initialize edge-induced subgraph with only train edges (edge-induced subgraph)\ndata_node2vec = Data(edge_index=train_edges_node2vec)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T05:30:22.270941Z","iopub.execute_input":"2025-05-28T05:30:22.271204Z","iopub.status.idle":"2025-05-28T05:30:22.275179Z","shell.execute_reply.started":"2025-05-28T05:30:22.271181Z","shell.execute_reply":"2025-05-28T05:30:22.274436Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def save_embedding(model, filepath): # function to save embedding to specified filepath\n    torch.save(model.embedding.weight.data.cpu(), filepath)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T05:30:22.275877Z","iopub.execute_input":"2025-05-28T05:30:22.276153Z","iopub.status.idle":"2025-05-28T05:30:22.288728Z","shell.execute_reply.started":"2025-05-28T05:30:22.276119Z","shell.execute_reply":"2025-05-28T05:30:22.288113Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def load_node2vec(args, data, filepath):\n    model = Node2Vec(data.edge_index, args['embedding_dim'], args['walk_length'],\n                    args['context_size'], args['walks_per_node'],\n                    sparse=True).to(device)\n    loader = model.loader(batch_size=args['batch_size'], shuffle=True,\n                        num_workers=4)\n    optimizer = torch.optim.SparseAdam(list(model.parameters()), lr=args['lr'])\n\n    model.load_state_dict(torch.load(filepath))\n\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T05:58:16.694943Z","iopub.execute_input":"2025-05-28T05:58:16.695474Z","iopub.status.idle":"2025-05-28T05:58:16.700172Z","shell.execute_reply.started":"2025-05-28T05:58:16.695451Z","shell.execute_reply":"2025-05-28T05:58:16.699402Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"# training function for node2vec using PyG's Node2Vec function\ndef train_Node2Vec(args, data, filepath):\n  model = Node2Vec(data.edge_index, args['embedding_dim'], args['walk_length'],\n                    args['context_size'], args['walks_per_node'],\n                    sparse=True).to(device)\n\n  loader = model.loader(batch_size=args['batch_size'], shuffle=True,\n                        num_workers=4)\n  optimizer = torch.optim.SparseAdam(list(model.parameters()), lr=args['lr'])\n\n  model.train()\n  for epoch in range(1, args['epochs'] + 1):\n      for i, (pos_rw, neg_rw) in enumerate(loader):\n          optimizer.zero_grad()\n          loss = model.loss(pos_rw.to(device), neg_rw.to(device))\n          loss.backward()\n          optimizer.step()\n\n          if (i + 1) % args['log_steps'] == 0:\n              print(f'Epoch: {epoch:02d}, Step: {i+1:03d}/{len(loader)}, '\n                    f'Loss: {loss:.4f}')\n\n          if (i + 1) % 100 == 0:  # Save model every 100 steps.\n              save_embedding(model, filepath)\n      save_embedding(model, filepath)\n  return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T05:47:38.086334Z","iopub.execute_input":"2025-05-28T05:47:38.086882Z","iopub.status.idle":"2025-05-28T05:47:38.093375Z","shell.execute_reply.started":"2025-05-28T05:47:38.086857Z","shell.execute_reply":"2025-05-28T05:47:38.092833Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"train_node2vec = True # change to true if you'd like to train/retrain embeddings","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T05:47:41.115990Z","iopub.execute_input":"2025-05-28T05:47:41.116631Z","iopub.status.idle":"2025-05-28T05:47:41.119841Z","shell.execute_reply.started":"2025-05-28T05:47:41.116600Z","shell.execute_reply":"2025-05-28T05:47:41.119015Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"node2vec_args = {'device':0, 'embedding_dim':256, 'walk_length':40, 'context_size':20, 'walks_per_node':10,\n      'batch_size':256, 'lr':0.01, 'epochs':100, 'log_steps':1}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T05:30:22.323973Z","iopub.execute_input":"2025-05-28T05:30:22.324202Z","iopub.status.idle":"2025-05-28T05:30:22.338896Z","shell.execute_reply.started":"2025-05-28T05:30:22.324176Z","shell.execute_reply":"2025-05-28T05:30:22.338130Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"import os\nif not os.path.exists('/kaggle/working/training_outputs/'):\n    os.makedirs('/kaggle/working/training_outputs/') # directory for saving visualizations and model checkpoints","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T05:30:22.339702Z","iopub.execute_input":"2025-05-28T05:30:22.339983Z","iopub.status.idle":"2025-05-28T05:30:22.352842Z","shell.execute_reply.started":"2025-05-28T05:30:22.339968Z","shell.execute_reply":"2025-05-28T05:30:22.352355Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"  # Train Node2Vec on the train edge-induced subgraph\n  filepath = '/kaggle/working/training_outputs/train-embedding-256.pt'\n  node2vec_args = {'device':0, 'embedding_dim':256, 'walk_length':40, 'context_size':20, 'walks_per_node':10,\n      'batch_size':256, 'lr':0.01, 'epochs':100, 'log_steps':1}\n  node2vec = train_Node2Vec(node2vec_args, data_node2vec, filepath) # embeddings will save to specified filepath","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T05:47:57.245180Z","iopub.execute_input":"2025-05-28T05:47:57.245764Z","iopub.status.idle":"2025-05-28T05:53:59.582482Z","shell.execute_reply.started":"2025-05-28T05:47:57.245742Z","shell.execute_reply":"2025-05-28T05:53:59.581639Z"}},"outputs":[{"name":"stdout","text":"Epoch: 01, Step: 001/17, Loss: 9.4905\nEpoch: 01, Step: 002/17, Loss: 9.2653\nEpoch: 01, Step: 003/17, Loss: 9.0823\nEpoch: 01, Step: 004/17, Loss: 8.9872\nEpoch: 01, Step: 005/17, Loss: 8.8345\nEpoch: 01, Step: 006/17, Loss: 8.7417\nEpoch: 01, Step: 007/17, Loss: 8.5942\nEpoch: 01, Step: 008/17, Loss: 8.5636\nEpoch: 01, Step: 009/17, Loss: 8.4319\nEpoch: 01, Step: 010/17, Loss: 8.4013\nEpoch: 01, Step: 011/17, Loss: 8.2981\nEpoch: 01, Step: 012/17, Loss: 8.1747\nEpoch: 01, Step: 013/17, Loss: 8.1233\nEpoch: 01, Step: 014/17, Loss: 8.0605\nEpoch: 01, Step: 015/17, Loss: 8.0044\nEpoch: 01, Step: 016/17, Loss: 7.9633\nEpoch: 01, Step: 017/17, Loss: 7.8605\nEpoch: 02, Step: 001/17, Loss: 7.8060\nEpoch: 02, Step: 002/17, Loss: 7.7441\nEpoch: 02, Step: 003/17, Loss: 7.6939\nEpoch: 02, Step: 004/17, Loss: 7.6498\nEpoch: 02, Step: 005/17, Loss: 7.5864\nEpoch: 02, Step: 006/17, Loss: 7.5487\nEpoch: 02, Step: 007/17, Loss: 7.4917\nEpoch: 02, Step: 008/17, Loss: 7.4348\nEpoch: 02, Step: 009/17, Loss: 7.3704\nEpoch: 02, Step: 010/17, Loss: 7.3289\nEpoch: 02, Step: 011/17, Loss: 7.3109\nEpoch: 02, Step: 012/17, Loss: 7.2234\nEpoch: 02, Step: 013/17, Loss: 7.2315\nEpoch: 02, Step: 014/17, Loss: 7.1411\nEpoch: 02, Step: 015/17, Loss: 7.0624\nEpoch: 02, Step: 016/17, Loss: 7.0269\nEpoch: 02, Step: 017/17, Loss: 6.9586\nEpoch: 03, Step: 001/17, Loss: 6.9373\nEpoch: 03, Step: 002/17, Loss: 6.8648\nEpoch: 03, Step: 003/17, Loss: 6.7913\nEpoch: 03, Step: 004/17, Loss: 6.8084\nEpoch: 03, Step: 005/17, Loss: 6.7469\nEpoch: 03, Step: 006/17, Loss: 6.6746\nEpoch: 03, Step: 007/17, Loss: 6.5909\nEpoch: 03, Step: 008/17, Loss: 6.5912\nEpoch: 03, Step: 009/17, Loss: 6.5146\nEpoch: 03, Step: 010/17, Loss: 6.4788\nEpoch: 03, Step: 011/17, Loss: 6.4413\nEpoch: 03, Step: 012/17, Loss: 6.3940\nEpoch: 03, Step: 013/17, Loss: 6.3189\nEpoch: 03, Step: 014/17, Loss: 6.3009\nEpoch: 03, Step: 015/17, Loss: 6.2716\nEpoch: 03, Step: 016/17, Loss: 6.2278\nEpoch: 03, Step: 017/17, Loss: 6.1479\nEpoch: 04, Step: 001/17, Loss: 6.1019\nEpoch: 04, Step: 002/17, Loss: 6.0331\nEpoch: 04, Step: 003/17, Loss: 6.0216\nEpoch: 04, Step: 004/17, Loss: 5.9449\nEpoch: 04, Step: 005/17, Loss: 5.9196\nEpoch: 04, Step: 006/17, Loss: 5.8577\nEpoch: 04, Step: 007/17, Loss: 5.8026\nEpoch: 04, Step: 008/17, Loss: 5.7422\nEpoch: 04, Step: 009/17, Loss: 5.7225\nEpoch: 04, Step: 010/17, Loss: 5.6464\nEpoch: 04, Step: 011/17, Loss: 5.6031\nEpoch: 04, Step: 012/17, Loss: 5.5859\nEpoch: 04, Step: 013/17, Loss: 5.5366\nEpoch: 04, Step: 014/17, Loss: 5.4951\nEpoch: 04, Step: 015/17, Loss: 5.3922\nEpoch: 04, Step: 016/17, Loss: 5.4209\nEpoch: 04, Step: 017/17, Loss: 5.3410\nEpoch: 05, Step: 001/17, Loss: 5.2753\nEpoch: 05, Step: 002/17, Loss: 5.2313\nEpoch: 05, Step: 003/17, Loss: 5.1944\nEpoch: 05, Step: 004/17, Loss: 5.1397\nEpoch: 05, Step: 005/17, Loss: 5.1044\nEpoch: 05, Step: 006/17, Loss: 5.0365\nEpoch: 05, Step: 007/17, Loss: 4.9879\nEpoch: 05, Step: 008/17, Loss: 4.9627\nEpoch: 05, Step: 009/17, Loss: 4.8962\nEpoch: 05, Step: 010/17, Loss: 4.8532\nEpoch: 05, Step: 011/17, Loss: 4.8170\nEpoch: 05, Step: 012/17, Loss: 4.7416\nEpoch: 05, Step: 013/17, Loss: 4.7170\nEpoch: 05, Step: 014/17, Loss: 4.6832\nEpoch: 05, Step: 015/17, Loss: 4.6088\nEpoch: 05, Step: 016/17, Loss: 4.5578\nEpoch: 05, Step: 017/17, Loss: 4.5172\nEpoch: 06, Step: 001/17, Loss: 4.4944\nEpoch: 06, Step: 002/17, Loss: 4.4141\nEpoch: 06, Step: 003/17, Loss: 4.3871\nEpoch: 06, Step: 004/17, Loss: 4.3155\nEpoch: 06, Step: 005/17, Loss: 4.2599\nEpoch: 06, Step: 006/17, Loss: 4.2408\nEpoch: 06, Step: 007/17, Loss: 4.1806\nEpoch: 06, Step: 008/17, Loss: 4.1358\nEpoch: 06, Step: 009/17, Loss: 4.0887\nEpoch: 06, Step: 010/17, Loss: 4.0716\nEpoch: 06, Step: 011/17, Loss: 4.0101\nEpoch: 06, Step: 012/17, Loss: 3.9593\nEpoch: 06, Step: 013/17, Loss: 3.9310\nEpoch: 06, Step: 014/17, Loss: 3.8832\nEpoch: 06, Step: 015/17, Loss: 3.8459\nEpoch: 06, Step: 016/17, Loss: 3.7872\nEpoch: 06, Step: 017/17, Loss: 3.7450\nEpoch: 07, Step: 001/17, Loss: 3.6913\nEpoch: 07, Step: 002/17, Loss: 3.6716\nEpoch: 07, Step: 003/17, Loss: 3.6090\nEpoch: 07, Step: 004/17, Loss: 3.5621\nEpoch: 07, Step: 005/17, Loss: 3.5116\nEpoch: 07, Step: 006/17, Loss: 3.4817\nEpoch: 07, Step: 007/17, Loss: 3.4474\nEpoch: 07, Step: 008/17, Loss: 3.3988\nEpoch: 07, Step: 009/17, Loss: 3.3631\nEpoch: 07, Step: 010/17, Loss: 3.3418\nEpoch: 07, Step: 011/17, Loss: 3.2840\nEpoch: 07, Step: 012/17, Loss: 3.2603\nEpoch: 07, Step: 013/17, Loss: 3.2133\nEpoch: 07, Step: 014/17, Loss: 3.1630\nEpoch: 07, Step: 015/17, Loss: 3.1281\nEpoch: 07, Step: 016/17, Loss: 3.1148\nEpoch: 07, Step: 017/17, Loss: 3.0762\nEpoch: 08, Step: 001/17, Loss: 3.0158\nEpoch: 08, Step: 002/17, Loss: 2.9864\nEpoch: 08, Step: 003/17, Loss: 2.9538\nEpoch: 08, Step: 004/17, Loss: 2.9097\nEpoch: 08, Step: 005/17, Loss: 2.8753\nEpoch: 08, Step: 006/17, Loss: 2.8368\nEpoch: 08, Step: 007/17, Loss: 2.8128\nEpoch: 08, Step: 008/17, Loss: 2.7827\nEpoch: 08, Step: 009/17, Loss: 2.7506\nEpoch: 08, Step: 010/17, Loss: 2.7257\nEpoch: 08, Step: 011/17, Loss: 2.6809\nEpoch: 08, Step: 012/17, Loss: 2.6554\nEpoch: 08, Step: 013/17, Loss: 2.6300\nEpoch: 08, Step: 014/17, Loss: 2.6003\nEpoch: 08, Step: 015/17, Loss: 2.5664\nEpoch: 08, Step: 016/17, Loss: 2.5530\nEpoch: 08, Step: 017/17, Loss: 2.5093\nEpoch: 09, Step: 001/17, Loss: 2.4782\nEpoch: 09, Step: 002/17, Loss: 2.4548\nEpoch: 09, Step: 003/17, Loss: 2.4171\nEpoch: 09, Step: 004/17, Loss: 2.3900\nEpoch: 09, Step: 005/17, Loss: 2.3712\nEpoch: 09, Step: 006/17, Loss: 2.3280\nEpoch: 09, Step: 007/17, Loss: 2.3159\nEpoch: 09, Step: 008/17, Loss: 2.2853\nEpoch: 09, Step: 009/17, Loss: 2.2582\nEpoch: 09, Step: 010/17, Loss: 2.2447\nEpoch: 09, Step: 011/17, Loss: 2.2138\nEpoch: 09, Step: 012/17, Loss: 2.1937\nEpoch: 09, Step: 013/17, Loss: 2.1661\nEpoch: 09, Step: 014/17, Loss: 2.1506\nEpoch: 09, Step: 015/17, Loss: 2.1332\nEpoch: 09, Step: 016/17, Loss: 2.0956\nEpoch: 09, Step: 017/17, Loss: 2.0983\nEpoch: 10, Step: 001/17, Loss: 2.0542\nEpoch: 10, Step: 002/17, Loss: 2.0395\nEpoch: 10, Step: 003/17, Loss: 2.0045\nEpoch: 10, Step: 004/17, Loss: 1.9956\nEpoch: 10, Step: 005/17, Loss: 1.9600\nEpoch: 10, Step: 006/17, Loss: 1.9563\nEpoch: 10, Step: 007/17, Loss: 1.9234\nEpoch: 10, Step: 008/17, Loss: 1.9039\nEpoch: 10, Step: 009/17, Loss: 1.8861\nEpoch: 10, Step: 010/17, Loss: 1.8768\nEpoch: 10, Step: 011/17, Loss: 1.8474\nEpoch: 10, Step: 012/17, Loss: 1.8493\nEpoch: 10, Step: 013/17, Loss: 1.8296\nEpoch: 10, Step: 014/17, Loss: 1.8043\nEpoch: 10, Step: 015/17, Loss: 1.7899\nEpoch: 10, Step: 016/17, Loss: 1.7750\nEpoch: 10, Step: 017/17, Loss: 1.7606\nEpoch: 11, Step: 001/17, Loss: 1.7416\nEpoch: 11, Step: 002/17, Loss: 1.7136\nEpoch: 11, Step: 003/17, Loss: 1.7056\nEpoch: 11, Step: 004/17, Loss: 1.6841\nEpoch: 11, Step: 005/17, Loss: 1.6712\nEpoch: 11, Step: 006/17, Loss: 1.6594\nEpoch: 11, Step: 007/17, Loss: 1.6406\nEpoch: 11, Step: 008/17, Loss: 1.6246\nEpoch: 11, Step: 009/17, Loss: 1.6171\nEpoch: 11, Step: 010/17, Loss: 1.5992\nEpoch: 11, Step: 011/17, Loss: 1.5757\nEpoch: 11, Step: 012/17, Loss: 1.5740\nEpoch: 11, Step: 013/17, Loss: 1.5610\nEpoch: 11, Step: 014/17, Loss: 1.5491\nEpoch: 11, Step: 015/17, Loss: 1.5315\nEpoch: 11, Step: 016/17, Loss: 1.5227\nEpoch: 11, Step: 017/17, Loss: 1.5030\nEpoch: 12, Step: 001/17, Loss: 1.4946\nEpoch: 12, Step: 002/17, Loss: 1.4833\nEpoch: 12, Step: 003/17, Loss: 1.4650\nEpoch: 12, Step: 004/17, Loss: 1.4530\nEpoch: 12, Step: 005/17, Loss: 1.4380\nEpoch: 12, Step: 006/17, Loss: 1.4286\nEpoch: 12, Step: 007/17, Loss: 1.4173\nEpoch: 12, Step: 008/17, Loss: 1.4198\nEpoch: 12, Step: 009/17, Loss: 1.4007\nEpoch: 12, Step: 010/17, Loss: 1.3913\nEpoch: 12, Step: 011/17, Loss: 1.3766\nEpoch: 12, Step: 012/17, Loss: 1.3732\nEpoch: 12, Step: 013/17, Loss: 1.3708\nEpoch: 12, Step: 014/17, Loss: 1.3440\nEpoch: 12, Step: 015/17, Loss: 1.3462\nEpoch: 12, Step: 016/17, Loss: 1.3363\nEpoch: 12, Step: 017/17, Loss: 1.3138\nEpoch: 13, Step: 001/17, Loss: 1.3128\nEpoch: 13, Step: 002/17, Loss: 1.2988\nEpoch: 13, Step: 003/17, Loss: 1.2857\nEpoch: 13, Step: 004/17, Loss: 1.2851\nEpoch: 13, Step: 005/17, Loss: 1.2720\nEpoch: 13, Step: 006/17, Loss: 1.2657\nEpoch: 13, Step: 007/17, Loss: 1.2574\nEpoch: 13, Step: 008/17, Loss: 1.2504\nEpoch: 13, Step: 009/17, Loss: 1.2453\nEpoch: 13, Step: 010/17, Loss: 1.2328\nEpoch: 13, Step: 011/17, Loss: 1.2302\nEpoch: 13, Step: 012/17, Loss: 1.2181\nEpoch: 13, Step: 013/17, Loss: 1.2171\nEpoch: 13, Step: 014/17, Loss: 1.1984\nEpoch: 13, Step: 015/17, Loss: 1.1973\nEpoch: 13, Step: 016/17, Loss: 1.1942\nEpoch: 13, Step: 017/17, Loss: 1.1876\nEpoch: 14, Step: 001/17, Loss: 1.1701\nEpoch: 14, Step: 002/17, Loss: 1.1687\nEpoch: 14, Step: 003/17, Loss: 1.1524\nEpoch: 14, Step: 004/17, Loss: 1.1512\nEpoch: 14, Step: 005/17, Loss: 1.1434\nEpoch: 14, Step: 006/17, Loss: 1.1412\nEpoch: 14, Step: 007/17, Loss: 1.1333\nEpoch: 14, Step: 008/17, Loss: 1.1225\nEpoch: 14, Step: 009/17, Loss: 1.1268\nEpoch: 14, Step: 010/17, Loss: 1.1131\nEpoch: 14, Step: 011/17, Loss: 1.1123\nEpoch: 14, Step: 012/17, Loss: 1.1058\nEpoch: 14, Step: 013/17, Loss: 1.1049\nEpoch: 14, Step: 014/17, Loss: 1.1045\nEpoch: 14, Step: 015/17, Loss: 1.0893\nEpoch: 14, Step: 016/17, Loss: 1.0899\nEpoch: 14, Step: 017/17, Loss: 1.0805\nEpoch: 15, Step: 001/17, Loss: 1.0663\nEpoch: 15, Step: 002/17, Loss: 1.0614\nEpoch: 15, Step: 003/17, Loss: 1.0626\nEpoch: 15, Step: 004/17, Loss: 1.0530\nEpoch: 15, Step: 005/17, Loss: 1.0484\nEpoch: 15, Step: 006/17, Loss: 1.0513\nEpoch: 15, Step: 007/17, Loss: 1.0468\nEpoch: 15, Step: 008/17, Loss: 1.0324\nEpoch: 15, Step: 009/17, Loss: 1.0318\nEpoch: 15, Step: 010/17, Loss: 1.0293\nEpoch: 15, Step: 011/17, Loss: 1.0290\nEpoch: 15, Step: 012/17, Loss: 1.0215\nEpoch: 15, Step: 013/17, Loss: 1.0262\nEpoch: 15, Step: 014/17, Loss: 1.0195\nEpoch: 15, Step: 015/17, Loss: 1.0065\nEpoch: 15, Step: 016/17, Loss: 1.0072\nEpoch: 15, Step: 017/17, Loss: 1.0077\nEpoch: 16, Step: 001/17, Loss: 0.9917\nEpoch: 16, Step: 002/17, Loss: 0.9922\nEpoch: 16, Step: 003/17, Loss: 0.9855\nEpoch: 16, Step: 004/17, Loss: 0.9828\nEpoch: 16, Step: 005/17, Loss: 0.9767\nEpoch: 16, Step: 006/17, Loss: 0.9761\nEpoch: 16, Step: 007/17, Loss: 0.9784\nEpoch: 16, Step: 008/17, Loss: 0.9721\nEpoch: 16, Step: 009/17, Loss: 0.9640\nEpoch: 16, Step: 010/17, Loss: 0.9662\nEpoch: 16, Step: 011/17, Loss: 0.9653\nEpoch: 16, Step: 012/17, Loss: 0.9562\nEpoch: 16, Step: 013/17, Loss: 0.9594\nEpoch: 16, Step: 014/17, Loss: 0.9553\nEpoch: 16, Step: 015/17, Loss: 0.9570\nEpoch: 16, Step: 016/17, Loss: 0.9523\nEpoch: 16, Step: 017/17, Loss: 0.9554\nEpoch: 17, Step: 001/17, Loss: 0.9344\nEpoch: 17, Step: 002/17, Loss: 0.9356\nEpoch: 17, Step: 003/17, Loss: 0.9329\nEpoch: 17, Step: 004/17, Loss: 0.9326\nEpoch: 17, Step: 005/17, Loss: 0.9281\nEpoch: 17, Step: 006/17, Loss: 0.9272\nEpoch: 17, Step: 007/17, Loss: 0.9260\nEpoch: 17, Step: 008/17, Loss: 0.9182\nEpoch: 17, Step: 009/17, Loss: 0.9181\nEpoch: 17, Step: 010/17, Loss: 0.9247\nEpoch: 17, Step: 011/17, Loss: 0.9160\nEpoch: 17, Step: 012/17, Loss: 0.9115\nEpoch: 17, Step: 013/17, Loss: 0.9151\nEpoch: 17, Step: 014/17, Loss: 0.9107\nEpoch: 17, Step: 015/17, Loss: 0.9074\nEpoch: 17, Step: 016/17, Loss: 0.9070\nEpoch: 17, Step: 017/17, Loss: 0.9082\nEpoch: 18, Step: 001/17, Loss: 0.8938\nEpoch: 18, Step: 002/17, Loss: 0.8950\nEpoch: 18, Step: 003/17, Loss: 0.8949\nEpoch: 18, Step: 004/17, Loss: 0.8865\nEpoch: 18, Step: 005/17, Loss: 0.8918\nEpoch: 18, Step: 006/17, Loss: 0.8901\nEpoch: 18, Step: 007/17, Loss: 0.8810\nEpoch: 18, Step: 008/17, Loss: 0.8828\nEpoch: 18, Step: 009/17, Loss: 0.8840\nEpoch: 18, Step: 010/17, Loss: 0.8820\nEpoch: 18, Step: 011/17, Loss: 0.8833\nEpoch: 18, Step: 012/17, Loss: 0.8765\nEpoch: 18, Step: 013/17, Loss: 0.8778\nEpoch: 18, Step: 014/17, Loss: 0.8784\nEpoch: 18, Step: 015/17, Loss: 0.8790\nEpoch: 18, Step: 016/17, Loss: 0.8779\nEpoch: 18, Step: 017/17, Loss: 0.8752\nEpoch: 19, Step: 001/17, Loss: 0.8689\nEpoch: 19, Step: 002/17, Loss: 0.8657\nEpoch: 19, Step: 003/17, Loss: 0.8637\nEpoch: 19, Step: 004/17, Loss: 0.8602\nEpoch: 19, Step: 005/17, Loss: 0.8629\nEpoch: 19, Step: 006/17, Loss: 0.8591\nEpoch: 19, Step: 007/17, Loss: 0.8575\nEpoch: 19, Step: 008/17, Loss: 0.8559\nEpoch: 19, Step: 009/17, Loss: 0.8564\nEpoch: 19, Step: 010/17, Loss: 0.8545\nEpoch: 19, Step: 011/17, Loss: 0.8570\nEpoch: 19, Step: 012/17, Loss: 0.8517\nEpoch: 19, Step: 013/17, Loss: 0.8534\nEpoch: 19, Step: 014/17, Loss: 0.8534\nEpoch: 19, Step: 015/17, Loss: 0.8550\nEpoch: 19, Step: 016/17, Loss: 0.8507\nEpoch: 19, Step: 017/17, Loss: 0.8482\nEpoch: 20, Step: 001/17, Loss: 0.8403\nEpoch: 20, Step: 002/17, Loss: 0.8403\nEpoch: 20, Step: 003/17, Loss: 0.8359\nEpoch: 20, Step: 004/17, Loss: 0.8355\nEpoch: 20, Step: 005/17, Loss: 0.8346\nEpoch: 20, Step: 006/17, Loss: 0.8410\nEpoch: 20, Step: 007/17, Loss: 0.8356\nEpoch: 20, Step: 008/17, Loss: 0.8367\nEpoch: 20, Step: 009/17, Loss: 0.8358\nEpoch: 20, Step: 010/17, Loss: 0.8379\nEpoch: 20, Step: 011/17, Loss: 0.8345\nEpoch: 20, Step: 012/17, Loss: 0.8327\nEpoch: 20, Step: 013/17, Loss: 0.8342\nEpoch: 20, Step: 014/17, Loss: 0.8371\nEpoch: 20, Step: 015/17, Loss: 0.8341\nEpoch: 20, Step: 016/17, Loss: 0.8316\nEpoch: 20, Step: 017/17, Loss: 0.8298\nEpoch: 21, Step: 001/17, Loss: 0.8224\nEpoch: 21, Step: 002/17, Loss: 0.8254\nEpoch: 21, Step: 003/17, Loss: 0.8192\nEpoch: 21, Step: 004/17, Loss: 0.8228\nEpoch: 21, Step: 005/17, Loss: 0.8194\nEpoch: 21, Step: 006/17, Loss: 0.8225\nEpoch: 21, Step: 007/17, Loss: 0.8198\nEpoch: 21, Step: 008/17, Loss: 0.8229\nEpoch: 21, Step: 009/17, Loss: 0.8165\nEpoch: 21, Step: 010/17, Loss: 0.8185\nEpoch: 21, Step: 011/17, Loss: 0.8157\nEpoch: 21, Step: 012/17, Loss: 0.8196\nEpoch: 21, Step: 013/17, Loss: 0.8217\nEpoch: 21, Step: 014/17, Loss: 0.8197\nEpoch: 21, Step: 015/17, Loss: 0.8232\nEpoch: 21, Step: 016/17, Loss: 0.8148\nEpoch: 21, Step: 017/17, Loss: 0.8189\nEpoch: 22, Step: 001/17, Loss: 0.8050\nEpoch: 22, Step: 002/17, Loss: 0.8107\nEpoch: 22, Step: 003/17, Loss: 0.8076\nEpoch: 22, Step: 004/17, Loss: 0.8053\nEpoch: 22, Step: 005/17, Loss: 0.8111\nEpoch: 22, Step: 006/17, Loss: 0.8065\nEpoch: 22, Step: 007/17, Loss: 0.8068\nEpoch: 22, Step: 008/17, Loss: 0.8082\nEpoch: 22, Step: 009/17, Loss: 0.8074\nEpoch: 22, Step: 010/17, Loss: 0.8077\nEpoch: 22, Step: 011/17, Loss: 0.8059\nEpoch: 22, Step: 012/17, Loss: 0.8067\nEpoch: 22, Step: 013/17, Loss: 0.8061\nEpoch: 22, Step: 014/17, Loss: 0.8069\nEpoch: 22, Step: 015/17, Loss: 0.8097\nEpoch: 22, Step: 016/17, Loss: 0.8068\nEpoch: 22, Step: 017/17, Loss: 0.8078\nEpoch: 23, Step: 001/17, Loss: 0.7998\nEpoch: 23, Step: 002/17, Loss: 0.7954\nEpoch: 23, Step: 003/17, Loss: 0.7977\nEpoch: 23, Step: 004/17, Loss: 0.7988\nEpoch: 23, Step: 005/17, Loss: 0.7961\nEpoch: 23, Step: 006/17, Loss: 0.7965\nEpoch: 23, Step: 007/17, Loss: 0.7945\nEpoch: 23, Step: 008/17, Loss: 0.7965\nEpoch: 23, Step: 009/17, Loss: 0.7960\nEpoch: 23, Step: 010/17, Loss: 0.7978\nEpoch: 23, Step: 011/17, Loss: 0.7965\nEpoch: 23, Step: 012/17, Loss: 0.7956\nEpoch: 23, Step: 013/17, Loss: 0.7959\nEpoch: 23, Step: 014/17, Loss: 0.8004\nEpoch: 23, Step: 015/17, Loss: 0.7969\nEpoch: 23, Step: 016/17, Loss: 0.7989\nEpoch: 23, Step: 017/17, Loss: 0.7991\nEpoch: 24, Step: 001/17, Loss: 0.7901\nEpoch: 24, Step: 002/17, Loss: 0.7902\nEpoch: 24, Step: 003/17, Loss: 0.7876\nEpoch: 24, Step: 004/17, Loss: 0.7888\nEpoch: 24, Step: 005/17, Loss: 0.7866\nEpoch: 24, Step: 006/17, Loss: 0.7900\nEpoch: 24, Step: 007/17, Loss: 0.7886\nEpoch: 24, Step: 008/17, Loss: 0.7879\nEpoch: 24, Step: 009/17, Loss: 0.7916\nEpoch: 24, Step: 010/17, Loss: 0.7875\nEpoch: 24, Step: 011/17, Loss: 0.7863\nEpoch: 24, Step: 012/17, Loss: 0.7867\nEpoch: 24, Step: 013/17, Loss: 0.7898\nEpoch: 24, Step: 014/17, Loss: 0.7925\nEpoch: 24, Step: 015/17, Loss: 0.7946\nEpoch: 24, Step: 016/17, Loss: 0.7903\nEpoch: 24, Step: 017/17, Loss: 0.7902\nEpoch: 25, Step: 001/17, Loss: 0.7805\nEpoch: 25, Step: 002/17, Loss: 0.7814\nEpoch: 25, Step: 003/17, Loss: 0.7797\nEpoch: 25, Step: 004/17, Loss: 0.7805\nEpoch: 25, Step: 005/17, Loss: 0.7814\nEpoch: 25, Step: 006/17, Loss: 0.7808\nEpoch: 25, Step: 007/17, Loss: 0.7814\nEpoch: 25, Step: 008/17, Loss: 0.7798\nEpoch: 25, Step: 009/17, Loss: 0.7860\nEpoch: 25, Step: 010/17, Loss: 0.7836\nEpoch: 25, Step: 011/17, Loss: 0.7820\nEpoch: 25, Step: 012/17, Loss: 0.7816\nEpoch: 25, Step: 013/17, Loss: 0.7865\nEpoch: 25, Step: 014/17, Loss: 0.7838\nEpoch: 25, Step: 015/17, Loss: 0.7838\nEpoch: 25, Step: 016/17, Loss: 0.7863\nEpoch: 25, Step: 017/17, Loss: 0.7893\nEpoch: 26, Step: 001/17, Loss: 0.7762\nEpoch: 26, Step: 002/17, Loss: 0.7748\nEpoch: 26, Step: 003/17, Loss: 0.7769\nEpoch: 26, Step: 004/17, Loss: 0.7758\nEpoch: 26, Step: 005/17, Loss: 0.7727\nEpoch: 26, Step: 006/17, Loss: 0.7758\nEpoch: 26, Step: 007/17, Loss: 0.7783\nEpoch: 26, Step: 008/17, Loss: 0.7744\nEpoch: 26, Step: 009/17, Loss: 0.7763\nEpoch: 26, Step: 010/17, Loss: 0.7773\nEpoch: 26, Step: 011/17, Loss: 0.7752\nEpoch: 26, Step: 012/17, Loss: 0.7795\nEpoch: 26, Step: 013/17, Loss: 0.7816\nEpoch: 26, Step: 014/17, Loss: 0.7791\nEpoch: 26, Step: 015/17, Loss: 0.7786\nEpoch: 26, Step: 016/17, Loss: 0.7820\nEpoch: 26, Step: 017/17, Loss: 0.7799\nEpoch: 27, Step: 001/17, Loss: 0.7699\nEpoch: 27, Step: 002/17, Loss: 0.7707\nEpoch: 27, Step: 003/17, Loss: 0.7714\nEpoch: 27, Step: 004/17, Loss: 0.7716\nEpoch: 27, Step: 005/17, Loss: 0.7737\nEpoch: 27, Step: 006/17, Loss: 0.7725\nEpoch: 27, Step: 007/17, Loss: 0.7725\nEpoch: 27, Step: 008/17, Loss: 0.7739\nEpoch: 27, Step: 009/17, Loss: 0.7726\nEpoch: 27, Step: 010/17, Loss: 0.7721\nEpoch: 27, Step: 011/17, Loss: 0.7738\nEpoch: 27, Step: 012/17, Loss: 0.7734\nEpoch: 27, Step: 013/17, Loss: 0.7729\nEpoch: 27, Step: 014/17, Loss: 0.7743\nEpoch: 27, Step: 015/17, Loss: 0.7762\nEpoch: 27, Step: 016/17, Loss: 0.7786\nEpoch: 27, Step: 017/17, Loss: 0.7790\nEpoch: 28, Step: 001/17, Loss: 0.7654\nEpoch: 28, Step: 002/17, Loss: 0.7675\nEpoch: 28, Step: 003/17, Loss: 0.7680\nEpoch: 28, Step: 004/17, Loss: 0.7674\nEpoch: 28, Step: 005/17, Loss: 0.7675\nEpoch: 28, Step: 006/17, Loss: 0.7677\nEpoch: 28, Step: 007/17, Loss: 0.7675\nEpoch: 28, Step: 008/17, Loss: 0.7672\nEpoch: 28, Step: 009/17, Loss: 0.7669\nEpoch: 28, Step: 010/17, Loss: 0.7697\nEpoch: 28, Step: 011/17, Loss: 0.7727\nEpoch: 28, Step: 012/17, Loss: 0.7706\nEpoch: 28, Step: 013/17, Loss: 0.7751\nEpoch: 28, Step: 014/17, Loss: 0.7729\nEpoch: 28, Step: 015/17, Loss: 0.7706\nEpoch: 28, Step: 016/17, Loss: 0.7781\nEpoch: 28, Step: 017/17, Loss: 0.7723\nEpoch: 29, Step: 001/17, Loss: 0.7644\nEpoch: 29, Step: 002/17, Loss: 0.7656\nEpoch: 29, Step: 003/17, Loss: 0.7620\nEpoch: 29, Step: 004/17, Loss: 0.7643\nEpoch: 29, Step: 005/17, Loss: 0.7636\nEpoch: 29, Step: 006/17, Loss: 0.7646\nEpoch: 29, Step: 007/17, Loss: 0.7615\nEpoch: 29, Step: 008/17, Loss: 0.7660\nEpoch: 29, Step: 009/17, Loss: 0.7665\nEpoch: 29, Step: 010/17, Loss: 0.7674\nEpoch: 29, Step: 011/17, Loss: 0.7666\nEpoch: 29, Step: 012/17, Loss: 0.7683\nEpoch: 29, Step: 013/17, Loss: 0.7699\nEpoch: 29, Step: 014/17, Loss: 0.7697\nEpoch: 29, Step: 015/17, Loss: 0.7719\nEpoch: 29, Step: 016/17, Loss: 0.7728\nEpoch: 29, Step: 017/17, Loss: 0.7705\nEpoch: 30, Step: 001/17, Loss: 0.7605\nEpoch: 30, Step: 002/17, Loss: 0.7618\nEpoch: 30, Step: 003/17, Loss: 0.7582\nEpoch: 30, Step: 004/17, Loss: 0.7592\nEpoch: 30, Step: 005/17, Loss: 0.7621\nEpoch: 30, Step: 006/17, Loss: 0.7621\nEpoch: 30, Step: 007/17, Loss: 0.7616\nEpoch: 30, Step: 008/17, Loss: 0.7631\nEpoch: 30, Step: 009/17, Loss: 0.7642\nEpoch: 30, Step: 010/17, Loss: 0.7639\nEpoch: 30, Step: 011/17, Loss: 0.7646\nEpoch: 30, Step: 012/17, Loss: 0.7640\nEpoch: 30, Step: 013/17, Loss: 0.7671\nEpoch: 30, Step: 014/17, Loss: 0.7659\nEpoch: 30, Step: 015/17, Loss: 0.7695\nEpoch: 30, Step: 016/17, Loss: 0.7711\nEpoch: 30, Step: 017/17, Loss: 0.7727\nEpoch: 31, Step: 001/17, Loss: 0.7592\nEpoch: 31, Step: 002/17, Loss: 0.7565\nEpoch: 31, Step: 003/17, Loss: 0.7595\nEpoch: 31, Step: 004/17, Loss: 0.7608\nEpoch: 31, Step: 005/17, Loss: 0.7628\nEpoch: 31, Step: 006/17, Loss: 0.7588\nEpoch: 31, Step: 007/17, Loss: 0.7619\nEpoch: 31, Step: 008/17, Loss: 0.7613\nEpoch: 31, Step: 009/17, Loss: 0.7620\nEpoch: 31, Step: 010/17, Loss: 0.7617\nEpoch: 31, Step: 011/17, Loss: 0.7616\nEpoch: 31, Step: 012/17, Loss: 0.7674\nEpoch: 31, Step: 013/17, Loss: 0.7649\nEpoch: 31, Step: 014/17, Loss: 0.7602\nEpoch: 31, Step: 015/17, Loss: 0.7673\nEpoch: 31, Step: 016/17, Loss: 0.7659\nEpoch: 31, Step: 017/17, Loss: 0.7718\nEpoch: 32, Step: 001/17, Loss: 0.7599\nEpoch: 32, Step: 002/17, Loss: 0.7570\nEpoch: 32, Step: 003/17, Loss: 0.7586\nEpoch: 32, Step: 004/17, Loss: 0.7565\nEpoch: 32, Step: 005/17, Loss: 0.7579\nEpoch: 32, Step: 006/17, Loss: 0.7582\nEpoch: 32, Step: 007/17, Loss: 0.7611\nEpoch: 32, Step: 008/17, Loss: 0.7587\nEpoch: 32, Step: 009/17, Loss: 0.7582\nEpoch: 32, Step: 010/17, Loss: 0.7616\nEpoch: 32, Step: 011/17, Loss: 0.7586\nEpoch: 32, Step: 012/17, Loss: 0.7599\nEpoch: 32, Step: 013/17, Loss: 0.7654\nEpoch: 32, Step: 014/17, Loss: 0.7640\nEpoch: 32, Step: 015/17, Loss: 0.7622\nEpoch: 32, Step: 016/17, Loss: 0.7680\nEpoch: 32, Step: 017/17, Loss: 0.7685\nEpoch: 33, Step: 001/17, Loss: 0.7539\nEpoch: 33, Step: 002/17, Loss: 0.7546\nEpoch: 33, Step: 003/17, Loss: 0.7553\nEpoch: 33, Step: 004/17, Loss: 0.7554\nEpoch: 33, Step: 005/17, Loss: 0.7575\nEpoch: 33, Step: 006/17, Loss: 0.7551\nEpoch: 33, Step: 007/17, Loss: 0.7575\nEpoch: 33, Step: 008/17, Loss: 0.7556\nEpoch: 33, Step: 009/17, Loss: 0.7597\nEpoch: 33, Step: 010/17, Loss: 0.7572\nEpoch: 33, Step: 011/17, Loss: 0.7638\nEpoch: 33, Step: 012/17, Loss: 0.7619\nEpoch: 33, Step: 013/17, Loss: 0.7639\nEpoch: 33, Step: 014/17, Loss: 0.7643\nEpoch: 33, Step: 015/17, Loss: 0.7653\nEpoch: 33, Step: 016/17, Loss: 0.7653\nEpoch: 33, Step: 017/17, Loss: 0.7649\nEpoch: 34, Step: 001/17, Loss: 0.7543\nEpoch: 34, Step: 002/17, Loss: 0.7532\nEpoch: 34, Step: 003/17, Loss: 0.7543\nEpoch: 34, Step: 004/17, Loss: 0.7546\nEpoch: 34, Step: 005/17, Loss: 0.7528\nEpoch: 34, Step: 006/17, Loss: 0.7570\nEpoch: 34, Step: 007/17, Loss: 0.7569\nEpoch: 34, Step: 008/17, Loss: 0.7582\nEpoch: 34, Step: 009/17, Loss: 0.7572\nEpoch: 34, Step: 010/17, Loss: 0.7560\nEpoch: 34, Step: 011/17, Loss: 0.7592\nEpoch: 34, Step: 012/17, Loss: 0.7619\nEpoch: 34, Step: 013/17, Loss: 0.7602\nEpoch: 34, Step: 014/17, Loss: 0.7606\nEpoch: 34, Step: 015/17, Loss: 0.7602\nEpoch: 34, Step: 016/17, Loss: 0.7675\nEpoch: 34, Step: 017/17, Loss: 0.7642\nEpoch: 35, Step: 001/17, Loss: 0.7528\nEpoch: 35, Step: 002/17, Loss: 0.7528\nEpoch: 35, Step: 003/17, Loss: 0.7539\nEpoch: 35, Step: 004/17, Loss: 0.7520\nEpoch: 35, Step: 005/17, Loss: 0.7540\nEpoch: 35, Step: 006/17, Loss: 0.7541\nEpoch: 35, Step: 007/17, Loss: 0.7540\nEpoch: 35, Step: 008/17, Loss: 0.7555\nEpoch: 35, Step: 009/17, Loss: 0.7568\nEpoch: 35, Step: 010/17, Loss: 0.7579\nEpoch: 35, Step: 011/17, Loss: 0.7562\nEpoch: 35, Step: 012/17, Loss: 0.7580\nEpoch: 35, Step: 013/17, Loss: 0.7624\nEpoch: 35, Step: 014/17, Loss: 0.7602\nEpoch: 35, Step: 015/17, Loss: 0.7589\nEpoch: 35, Step: 016/17, Loss: 0.7619\nEpoch: 35, Step: 017/17, Loss: 0.7629\nEpoch: 36, Step: 001/17, Loss: 0.7511\nEpoch: 36, Step: 002/17, Loss: 0.7513\nEpoch: 36, Step: 003/17, Loss: 0.7512\nEpoch: 36, Step: 004/17, Loss: 0.7501\nEpoch: 36, Step: 005/17, Loss: 0.7537\nEpoch: 36, Step: 006/17, Loss: 0.7550\nEpoch: 36, Step: 007/17, Loss: 0.7529\nEpoch: 36, Step: 008/17, Loss: 0.7543\nEpoch: 36, Step: 009/17, Loss: 0.7547\nEpoch: 36, Step: 010/17, Loss: 0.7567\nEpoch: 36, Step: 011/17, Loss: 0.7578\nEpoch: 36, Step: 012/17, Loss: 0.7587\nEpoch: 36, Step: 013/17, Loss: 0.7598\nEpoch: 36, Step: 014/17, Loss: 0.7616\nEpoch: 36, Step: 015/17, Loss: 0.7620\nEpoch: 36, Step: 016/17, Loss: 0.7627\nEpoch: 36, Step: 017/17, Loss: 0.7630\nEpoch: 37, Step: 001/17, Loss: 0.7492\nEpoch: 37, Step: 002/17, Loss: 0.7524\nEpoch: 37, Step: 003/17, Loss: 0.7513\nEpoch: 37, Step: 004/17, Loss: 0.7530\nEpoch: 37, Step: 005/17, Loss: 0.7525\nEpoch: 37, Step: 006/17, Loss: 0.7519\nEpoch: 37, Step: 007/17, Loss: 0.7551\nEpoch: 37, Step: 008/17, Loss: 0.7533\nEpoch: 37, Step: 009/17, Loss: 0.7523\nEpoch: 37, Step: 010/17, Loss: 0.7550\nEpoch: 37, Step: 011/17, Loss: 0.7570\nEpoch: 37, Step: 012/17, Loss: 0.7607\nEpoch: 37, Step: 013/17, Loss: 0.7580\nEpoch: 37, Step: 014/17, Loss: 0.7593\nEpoch: 37, Step: 015/17, Loss: 0.7588\nEpoch: 37, Step: 016/17, Loss: 0.7605\nEpoch: 37, Step: 017/17, Loss: 0.7606\nEpoch: 38, Step: 001/17, Loss: 0.7516\nEpoch: 38, Step: 002/17, Loss: 0.7502\nEpoch: 38, Step: 003/17, Loss: 0.7506\nEpoch: 38, Step: 004/17, Loss: 0.7521\nEpoch: 38, Step: 005/17, Loss: 0.7519\nEpoch: 38, Step: 006/17, Loss: 0.7521\nEpoch: 38, Step: 007/17, Loss: 0.7534\nEpoch: 38, Step: 008/17, Loss: 0.7535\nEpoch: 38, Step: 009/17, Loss: 0.7563\nEpoch: 38, Step: 010/17, Loss: 0.7542\nEpoch: 38, Step: 011/17, Loss: 0.7533\nEpoch: 38, Step: 012/17, Loss: 0.7531\nEpoch: 38, Step: 013/17, Loss: 0.7570\nEpoch: 38, Step: 014/17, Loss: 0.7585\nEpoch: 38, Step: 015/17, Loss: 0.7571\nEpoch: 38, Step: 016/17, Loss: 0.7622\nEpoch: 38, Step: 017/17, Loss: 0.7624\nEpoch: 39, Step: 001/17, Loss: 0.7506\nEpoch: 39, Step: 002/17, Loss: 0.7508\nEpoch: 39, Step: 003/17, Loss: 0.7505\nEpoch: 39, Step: 004/17, Loss: 0.7496\nEpoch: 39, Step: 005/17, Loss: 0.7510\nEpoch: 39, Step: 006/17, Loss: 0.7501\nEpoch: 39, Step: 007/17, Loss: 0.7536\nEpoch: 39, Step: 008/17, Loss: 0.7520\nEpoch: 39, Step: 009/17, Loss: 0.7508\nEpoch: 39, Step: 010/17, Loss: 0.7530\nEpoch: 39, Step: 011/17, Loss: 0.7532\nEpoch: 39, Step: 012/17, Loss: 0.7553\nEpoch: 39, Step: 013/17, Loss: 0.7564\nEpoch: 39, Step: 014/17, Loss: 0.7593\nEpoch: 39, Step: 015/17, Loss: 0.7583\nEpoch: 39, Step: 016/17, Loss: 0.7610\nEpoch: 39, Step: 017/17, Loss: 0.7574\nEpoch: 40, Step: 001/17, Loss: 0.7478\nEpoch: 40, Step: 002/17, Loss: 0.7516\nEpoch: 40, Step: 003/17, Loss: 0.7497\nEpoch: 40, Step: 004/17, Loss: 0.7501\nEpoch: 40, Step: 005/17, Loss: 0.7495\nEpoch: 40, Step: 006/17, Loss: 0.7516\nEpoch: 40, Step: 007/17, Loss: 0.7514\nEpoch: 40, Step: 008/17, Loss: 0.7538\nEpoch: 40, Step: 009/17, Loss: 0.7531\nEpoch: 40, Step: 010/17, Loss: 0.7533\nEpoch: 40, Step: 011/17, Loss: 0.7502\nEpoch: 40, Step: 012/17, Loss: 0.7588\nEpoch: 40, Step: 013/17, Loss: 0.7555\nEpoch: 40, Step: 014/17, Loss: 0.7563\nEpoch: 40, Step: 015/17, Loss: 0.7574\nEpoch: 40, Step: 016/17, Loss: 0.7569\nEpoch: 40, Step: 017/17, Loss: 0.7599\nEpoch: 41, Step: 001/17, Loss: 0.7481\nEpoch: 41, Step: 002/17, Loss: 0.7526\nEpoch: 41, Step: 003/17, Loss: 0.7481\nEpoch: 41, Step: 004/17, Loss: 0.7497\nEpoch: 41, Step: 005/17, Loss: 0.7488\nEpoch: 41, Step: 006/17, Loss: 0.7481\nEpoch: 41, Step: 007/17, Loss: 0.7494\nEpoch: 41, Step: 008/17, Loss: 0.7533\nEpoch: 41, Step: 009/17, Loss: 0.7544\nEpoch: 41, Step: 010/17, Loss: 0.7551\nEpoch: 41, Step: 011/17, Loss: 0.7524\nEpoch: 41, Step: 012/17, Loss: 0.7528\nEpoch: 41, Step: 013/17, Loss: 0.7549\nEpoch: 41, Step: 014/17, Loss: 0.7535\nEpoch: 41, Step: 015/17, Loss: 0.7555\nEpoch: 41, Step: 016/17, Loss: 0.7585\nEpoch: 41, Step: 017/17, Loss: 0.7610\nEpoch: 42, Step: 001/17, Loss: 0.7500\nEpoch: 42, Step: 002/17, Loss: 0.7485\nEpoch: 42, Step: 003/17, Loss: 0.7471\nEpoch: 42, Step: 004/17, Loss: 0.7491\nEpoch: 42, Step: 005/17, Loss: 0.7457\nEpoch: 42, Step: 006/17, Loss: 0.7488\nEpoch: 42, Step: 007/17, Loss: 0.7510\nEpoch: 42, Step: 008/17, Loss: 0.7521\nEpoch: 42, Step: 009/17, Loss: 0.7530\nEpoch: 42, Step: 010/17, Loss: 0.7514\nEpoch: 42, Step: 011/17, Loss: 0.7543\nEpoch: 42, Step: 012/17, Loss: 0.7567\nEpoch: 42, Step: 013/17, Loss: 0.7535\nEpoch: 42, Step: 014/17, Loss: 0.7568\nEpoch: 42, Step: 015/17, Loss: 0.7552\nEpoch: 42, Step: 016/17, Loss: 0.7582\nEpoch: 42, Step: 017/17, Loss: 0.7572\nEpoch: 43, Step: 001/17, Loss: 0.7449\nEpoch: 43, Step: 002/17, Loss: 0.7492\nEpoch: 43, Step: 003/17, Loss: 0.7477\nEpoch: 43, Step: 004/17, Loss: 0.7480\nEpoch: 43, Step: 005/17, Loss: 0.7478\nEpoch: 43, Step: 006/17, Loss: 0.7480\nEpoch: 43, Step: 007/17, Loss: 0.7485\nEpoch: 43, Step: 008/17, Loss: 0.7530\nEpoch: 43, Step: 009/17, Loss: 0.7527\nEpoch: 43, Step: 010/17, Loss: 0.7514\nEpoch: 43, Step: 011/17, Loss: 0.7534\nEpoch: 43, Step: 012/17, Loss: 0.7521\nEpoch: 43, Step: 013/17, Loss: 0.7557\nEpoch: 43, Step: 014/17, Loss: 0.7548\nEpoch: 43, Step: 015/17, Loss: 0.7572\nEpoch: 43, Step: 016/17, Loss: 0.7561\nEpoch: 43, Step: 017/17, Loss: 0.7599\nEpoch: 44, Step: 001/17, Loss: 0.7455\nEpoch: 44, Step: 002/17, Loss: 0.7451\nEpoch: 44, Step: 003/17, Loss: 0.7471\nEpoch: 44, Step: 004/17, Loss: 0.7463\nEpoch: 44, Step: 005/17, Loss: 0.7491\nEpoch: 44, Step: 006/17, Loss: 0.7481\nEpoch: 44, Step: 007/17, Loss: 0.7473\nEpoch: 44, Step: 008/17, Loss: 0.7508\nEpoch: 44, Step: 009/17, Loss: 0.7499\nEpoch: 44, Step: 010/17, Loss: 0.7550\nEpoch: 44, Step: 011/17, Loss: 0.7529\nEpoch: 44, Step: 012/17, Loss: 0.7543\nEpoch: 44, Step: 013/17, Loss: 0.7545\nEpoch: 44, Step: 014/17, Loss: 0.7530\nEpoch: 44, Step: 015/17, Loss: 0.7570\nEpoch: 44, Step: 016/17, Loss: 0.7551\nEpoch: 44, Step: 017/17, Loss: 0.7550\nEpoch: 45, Step: 001/17, Loss: 0.7465\nEpoch: 45, Step: 002/17, Loss: 0.7422\nEpoch: 45, Step: 003/17, Loss: 0.7451\nEpoch: 45, Step: 004/17, Loss: 0.7485\nEpoch: 45, Step: 005/17, Loss: 0.7477\nEpoch: 45, Step: 006/17, Loss: 0.7463\nEpoch: 45, Step: 007/17, Loss: 0.7471\nEpoch: 45, Step: 008/17, Loss: 0.7475\nEpoch: 45, Step: 009/17, Loss: 0.7527\nEpoch: 45, Step: 010/17, Loss: 0.7505\nEpoch: 45, Step: 011/17, Loss: 0.7523\nEpoch: 45, Step: 012/17, Loss: 0.7528\nEpoch: 45, Step: 013/17, Loss: 0.7539\nEpoch: 45, Step: 014/17, Loss: 0.7540\nEpoch: 45, Step: 015/17, Loss: 0.7592\nEpoch: 45, Step: 016/17, Loss: 0.7602\nEpoch: 45, Step: 017/17, Loss: 0.7599\nEpoch: 46, Step: 001/17, Loss: 0.7470\nEpoch: 46, Step: 002/17, Loss: 0.7449\nEpoch: 46, Step: 003/17, Loss: 0.7468\nEpoch: 46, Step: 004/17, Loss: 0.7442\nEpoch: 46, Step: 005/17, Loss: 0.7449\nEpoch: 46, Step: 006/17, Loss: 0.7485\nEpoch: 46, Step: 007/17, Loss: 0.7490\nEpoch: 46, Step: 008/17, Loss: 0.7492\nEpoch: 46, Step: 009/17, Loss: 0.7494\nEpoch: 46, Step: 010/17, Loss: 0.7516\nEpoch: 46, Step: 011/17, Loss: 0.7504\nEpoch: 46, Step: 012/17, Loss: 0.7528\nEpoch: 46, Step: 013/17, Loss: 0.7524\nEpoch: 46, Step: 014/17, Loss: 0.7530\nEpoch: 46, Step: 015/17, Loss: 0.7560\nEpoch: 46, Step: 016/17, Loss: 0.7559\nEpoch: 46, Step: 017/17, Loss: 0.7584\nEpoch: 47, Step: 001/17, Loss: 0.7473\nEpoch: 47, Step: 002/17, Loss: 0.7450\nEpoch: 47, Step: 003/17, Loss: 0.7456\nEpoch: 47, Step: 004/17, Loss: 0.7473\nEpoch: 47, Step: 005/17, Loss: 0.7496\nEpoch: 47, Step: 006/17, Loss: 0.7480\nEpoch: 47, Step: 007/17, Loss: 0.7486\nEpoch: 47, Step: 008/17, Loss: 0.7487\nEpoch: 47, Step: 009/17, Loss: 0.7487\nEpoch: 47, Step: 010/17, Loss: 0.7490\nEpoch: 47, Step: 011/17, Loss: 0.7520\nEpoch: 47, Step: 012/17, Loss: 0.7523\nEpoch: 47, Step: 013/17, Loss: 0.7543\nEpoch: 47, Step: 014/17, Loss: 0.7575\nEpoch: 47, Step: 015/17, Loss: 0.7564\nEpoch: 47, Step: 016/17, Loss: 0.7560\nEpoch: 47, Step: 017/17, Loss: 0.7636\nEpoch: 48, Step: 001/17, Loss: 0.7450\nEpoch: 48, Step: 002/17, Loss: 0.7439\nEpoch: 48, Step: 003/17, Loss: 0.7450\nEpoch: 48, Step: 004/17, Loss: 0.7461\nEpoch: 48, Step: 005/17, Loss: 0.7486\nEpoch: 48, Step: 006/17, Loss: 0.7466\nEpoch: 48, Step: 007/17, Loss: 0.7482\nEpoch: 48, Step: 008/17, Loss: 0.7473\nEpoch: 48, Step: 009/17, Loss: 0.7513\nEpoch: 48, Step: 010/17, Loss: 0.7515\nEpoch: 48, Step: 011/17, Loss: 0.7493\nEpoch: 48, Step: 012/17, Loss: 0.7514\nEpoch: 48, Step: 013/17, Loss: 0.7524\nEpoch: 48, Step: 014/17, Loss: 0.7542\nEpoch: 48, Step: 015/17, Loss: 0.7529\nEpoch: 48, Step: 016/17, Loss: 0.7575\nEpoch: 48, Step: 017/17, Loss: 0.7570\nEpoch: 49, Step: 001/17, Loss: 0.7441\nEpoch: 49, Step: 002/17, Loss: 0.7443\nEpoch: 49, Step: 003/17, Loss: 0.7448\nEpoch: 49, Step: 004/17, Loss: 0.7447\nEpoch: 49, Step: 005/17, Loss: 0.7457\nEpoch: 49, Step: 006/17, Loss: 0.7482\nEpoch: 49, Step: 007/17, Loss: 0.7469\nEpoch: 49, Step: 008/17, Loss: 0.7502\nEpoch: 49, Step: 009/17, Loss: 0.7477\nEpoch: 49, Step: 010/17, Loss: 0.7516\nEpoch: 49, Step: 011/17, Loss: 0.7511\nEpoch: 49, Step: 012/17, Loss: 0.7512\nEpoch: 49, Step: 013/17, Loss: 0.7542\nEpoch: 49, Step: 014/17, Loss: 0.7566\nEpoch: 49, Step: 015/17, Loss: 0.7568\nEpoch: 49, Step: 016/17, Loss: 0.7559\nEpoch: 49, Step: 017/17, Loss: 0.7598\nEpoch: 50, Step: 001/17, Loss: 0.7461\nEpoch: 50, Step: 002/17, Loss: 0.7471\nEpoch: 50, Step: 003/17, Loss: 0.7442\nEpoch: 50, Step: 004/17, Loss: 0.7454\nEpoch: 50, Step: 005/17, Loss: 0.7470\nEpoch: 50, Step: 006/17, Loss: 0.7459\nEpoch: 50, Step: 007/17, Loss: 0.7483\nEpoch: 50, Step: 008/17, Loss: 0.7464\nEpoch: 50, Step: 009/17, Loss: 0.7500\nEpoch: 50, Step: 010/17, Loss: 0.7499\nEpoch: 50, Step: 011/17, Loss: 0.7508\nEpoch: 50, Step: 012/17, Loss: 0.7521\nEpoch: 50, Step: 013/17, Loss: 0.7513\nEpoch: 50, Step: 014/17, Loss: 0.7538\nEpoch: 50, Step: 015/17, Loss: 0.7532\nEpoch: 50, Step: 016/17, Loss: 0.7568\nEpoch: 50, Step: 017/17, Loss: 0.7600\nEpoch: 51, Step: 001/17, Loss: 0.7422\nEpoch: 51, Step: 002/17, Loss: 0.7429\nEpoch: 51, Step: 003/17, Loss: 0.7424\nEpoch: 51, Step: 004/17, Loss: 0.7443\nEpoch: 51, Step: 005/17, Loss: 0.7465\nEpoch: 51, Step: 006/17, Loss: 0.7480\nEpoch: 51, Step: 007/17, Loss: 0.7444\nEpoch: 51, Step: 008/17, Loss: 0.7503\nEpoch: 51, Step: 009/17, Loss: 0.7467\nEpoch: 51, Step: 010/17, Loss: 0.7497\nEpoch: 51, Step: 011/17, Loss: 0.7542\nEpoch: 51, Step: 012/17, Loss: 0.7502\nEpoch: 51, Step: 013/17, Loss: 0.7548\nEpoch: 51, Step: 014/17, Loss: 0.7584\nEpoch: 51, Step: 015/17, Loss: 0.7545\nEpoch: 51, Step: 016/17, Loss: 0.7585\nEpoch: 51, Step: 017/17, Loss: 0.7601\nEpoch: 52, Step: 001/17, Loss: 0.7445\nEpoch: 52, Step: 002/17, Loss: 0.7435\nEpoch: 52, Step: 003/17, Loss: 0.7465\nEpoch: 52, Step: 004/17, Loss: 0.7468\nEpoch: 52, Step: 005/17, Loss: 0.7475\nEpoch: 52, Step: 006/17, Loss: 0.7471\nEpoch: 52, Step: 007/17, Loss: 0.7467\nEpoch: 52, Step: 008/17, Loss: 0.7480\nEpoch: 52, Step: 009/17, Loss: 0.7489\nEpoch: 52, Step: 010/17, Loss: 0.7504\nEpoch: 52, Step: 011/17, Loss: 0.7514\nEpoch: 52, Step: 012/17, Loss: 0.7510\nEpoch: 52, Step: 013/17, Loss: 0.7525\nEpoch: 52, Step: 014/17, Loss: 0.7547\nEpoch: 52, Step: 015/17, Loss: 0.7536\nEpoch: 52, Step: 016/17, Loss: 0.7545\nEpoch: 52, Step: 017/17, Loss: 0.7593\nEpoch: 53, Step: 001/17, Loss: 0.7436\nEpoch: 53, Step: 002/17, Loss: 0.7454\nEpoch: 53, Step: 003/17, Loss: 0.7461\nEpoch: 53, Step: 004/17, Loss: 0.7436\nEpoch: 53, Step: 005/17, Loss: 0.7483\nEpoch: 53, Step: 006/17, Loss: 0.7468\nEpoch: 53, Step: 007/17, Loss: 0.7468\nEpoch: 53, Step: 008/17, Loss: 0.7485\nEpoch: 53, Step: 009/17, Loss: 0.7470\nEpoch: 53, Step: 010/17, Loss: 0.7504\nEpoch: 53, Step: 011/17, Loss: 0.7504\nEpoch: 53, Step: 012/17, Loss: 0.7536\nEpoch: 53, Step: 013/17, Loss: 0.7547\nEpoch: 53, Step: 014/17, Loss: 0.7555\nEpoch: 53, Step: 015/17, Loss: 0.7517\nEpoch: 53, Step: 016/17, Loss: 0.7544\nEpoch: 53, Step: 017/17, Loss: 0.7567\nEpoch: 54, Step: 001/17, Loss: 0.7434\nEpoch: 54, Step: 002/17, Loss: 0.7448\nEpoch: 54, Step: 003/17, Loss: 0.7435\nEpoch: 54, Step: 004/17, Loss: 0.7447\nEpoch: 54, Step: 005/17, Loss: 0.7453\nEpoch: 54, Step: 006/17, Loss: 0.7456\nEpoch: 54, Step: 007/17, Loss: 0.7474\nEpoch: 54, Step: 008/17, Loss: 0.7501\nEpoch: 54, Step: 009/17, Loss: 0.7522\nEpoch: 54, Step: 010/17, Loss: 0.7477\nEpoch: 54, Step: 011/17, Loss: 0.7445\nEpoch: 54, Step: 012/17, Loss: 0.7505\nEpoch: 54, Step: 013/17, Loss: 0.7532\nEpoch: 54, Step: 014/17, Loss: 0.7535\nEpoch: 54, Step: 015/17, Loss: 0.7539\nEpoch: 54, Step: 016/17, Loss: 0.7561\nEpoch: 54, Step: 017/17, Loss: 0.7560\nEpoch: 55, Step: 001/17, Loss: 0.7435\nEpoch: 55, Step: 002/17, Loss: 0.7420\nEpoch: 55, Step: 003/17, Loss: 0.7439\nEpoch: 55, Step: 004/17, Loss: 0.7447\nEpoch: 55, Step: 005/17, Loss: 0.7485\nEpoch: 55, Step: 006/17, Loss: 0.7445\nEpoch: 55, Step: 007/17, Loss: 0.7458\nEpoch: 55, Step: 008/17, Loss: 0.7474\nEpoch: 55, Step: 009/17, Loss: 0.7496\nEpoch: 55, Step: 010/17, Loss: 0.7480\nEpoch: 55, Step: 011/17, Loss: 0.7476\nEpoch: 55, Step: 012/17, Loss: 0.7538\nEpoch: 55, Step: 013/17, Loss: 0.7524\nEpoch: 55, Step: 014/17, Loss: 0.7536\nEpoch: 55, Step: 015/17, Loss: 0.7537\nEpoch: 55, Step: 016/17, Loss: 0.7550\nEpoch: 55, Step: 017/17, Loss: 0.7514\nEpoch: 56, Step: 001/17, Loss: 0.7435\nEpoch: 56, Step: 002/17, Loss: 0.7468\nEpoch: 56, Step: 003/17, Loss: 0.7448\nEpoch: 56, Step: 004/17, Loss: 0.7419\nEpoch: 56, Step: 005/17, Loss: 0.7433\nEpoch: 56, Step: 006/17, Loss: 0.7460\nEpoch: 56, Step: 007/17, Loss: 0.7448\nEpoch: 56, Step: 008/17, Loss: 0.7481\nEpoch: 56, Step: 009/17, Loss: 0.7485\nEpoch: 56, Step: 010/17, Loss: 0.7495\nEpoch: 56, Step: 011/17, Loss: 0.7498\nEpoch: 56, Step: 012/17, Loss: 0.7533\nEpoch: 56, Step: 013/17, Loss: 0.7480\nEpoch: 56, Step: 014/17, Loss: 0.7545\nEpoch: 56, Step: 015/17, Loss: 0.7516\nEpoch: 56, Step: 016/17, Loss: 0.7567\nEpoch: 56, Step: 017/17, Loss: 0.7558\nEpoch: 57, Step: 001/17, Loss: 0.7427\nEpoch: 57, Step: 002/17, Loss: 0.7419\nEpoch: 57, Step: 003/17, Loss: 0.7444\nEpoch: 57, Step: 004/17, Loss: 0.7467\nEpoch: 57, Step: 005/17, Loss: 0.7449\nEpoch: 57, Step: 006/17, Loss: 0.7453\nEpoch: 57, Step: 007/17, Loss: 0.7468\nEpoch: 57, Step: 008/17, Loss: 0.7475\nEpoch: 57, Step: 009/17, Loss: 0.7483\nEpoch: 57, Step: 010/17, Loss: 0.7492\nEpoch: 57, Step: 011/17, Loss: 0.7509\nEpoch: 57, Step: 012/17, Loss: 0.7493\nEpoch: 57, Step: 013/17, Loss: 0.7514\nEpoch: 57, Step: 014/17, Loss: 0.7539\nEpoch: 57, Step: 015/17, Loss: 0.7529\nEpoch: 57, Step: 016/17, Loss: 0.7538\nEpoch: 57, Step: 017/17, Loss: 0.7546\nEpoch: 58, Step: 001/17, Loss: 0.7420\nEpoch: 58, Step: 002/17, Loss: 0.7430\nEpoch: 58, Step: 003/17, Loss: 0.7425\nEpoch: 58, Step: 004/17, Loss: 0.7450\nEpoch: 58, Step: 005/17, Loss: 0.7450\nEpoch: 58, Step: 006/17, Loss: 0.7478\nEpoch: 58, Step: 007/17, Loss: 0.7481\nEpoch: 58, Step: 008/17, Loss: 0.7448\nEpoch: 58, Step: 009/17, Loss: 0.7486\nEpoch: 58, Step: 010/17, Loss: 0.7485\nEpoch: 58, Step: 011/17, Loss: 0.7488\nEpoch: 58, Step: 012/17, Loss: 0.7475\nEpoch: 58, Step: 013/17, Loss: 0.7509\nEpoch: 58, Step: 014/17, Loss: 0.7509\nEpoch: 58, Step: 015/17, Loss: 0.7548\nEpoch: 58, Step: 016/17, Loss: 0.7579\nEpoch: 58, Step: 017/17, Loss: 0.7559\nEpoch: 59, Step: 001/17, Loss: 0.7437\nEpoch: 59, Step: 002/17, Loss: 0.7431\nEpoch: 59, Step: 003/17, Loss: 0.7447\nEpoch: 59, Step: 004/17, Loss: 0.7420\nEpoch: 59, Step: 005/17, Loss: 0.7470\nEpoch: 59, Step: 006/17, Loss: 0.7447\nEpoch: 59, Step: 007/17, Loss: 0.7439\nEpoch: 59, Step: 008/17, Loss: 0.7467\nEpoch: 59, Step: 009/17, Loss: 0.7462\nEpoch: 59, Step: 010/17, Loss: 0.7456\nEpoch: 59, Step: 011/17, Loss: 0.7471\nEpoch: 59, Step: 012/17, Loss: 0.7521\nEpoch: 59, Step: 013/17, Loss: 0.7500\nEpoch: 59, Step: 014/17, Loss: 0.7513\nEpoch: 59, Step: 015/17, Loss: 0.7537\nEpoch: 59, Step: 016/17, Loss: 0.7571\nEpoch: 59, Step: 017/17, Loss: 0.7530\nEpoch: 60, Step: 001/17, Loss: 0.7413\nEpoch: 60, Step: 002/17, Loss: 0.7407\nEpoch: 60, Step: 003/17, Loss: 0.7429\nEpoch: 60, Step: 004/17, Loss: 0.7430\nEpoch: 60, Step: 005/17, Loss: 0.7442\nEpoch: 60, Step: 006/17, Loss: 0.7431\nEpoch: 60, Step: 007/17, Loss: 0.7477\nEpoch: 60, Step: 008/17, Loss: 0.7466\nEpoch: 60, Step: 009/17, Loss: 0.7501\nEpoch: 60, Step: 010/17, Loss: 0.7502\nEpoch: 60, Step: 011/17, Loss: 0.7506\nEpoch: 60, Step: 012/17, Loss: 0.7509\nEpoch: 60, Step: 013/17, Loss: 0.7541\nEpoch: 60, Step: 014/17, Loss: 0.7487\nEpoch: 60, Step: 015/17, Loss: 0.7552\nEpoch: 60, Step: 016/17, Loss: 0.7550\nEpoch: 60, Step: 017/17, Loss: 0.7578\nEpoch: 61, Step: 001/17, Loss: 0.7392\nEpoch: 61, Step: 002/17, Loss: 0.7439\nEpoch: 61, Step: 003/17, Loss: 0.7420\nEpoch: 61, Step: 004/17, Loss: 0.7454\nEpoch: 61, Step: 005/17, Loss: 0.7437\nEpoch: 61, Step: 006/17, Loss: 0.7457\nEpoch: 61, Step: 007/17, Loss: 0.7449\nEpoch: 61, Step: 008/17, Loss: 0.7470\nEpoch: 61, Step: 009/17, Loss: 0.7499\nEpoch: 61, Step: 010/17, Loss: 0.7476\nEpoch: 61, Step: 011/17, Loss: 0.7477\nEpoch: 61, Step: 012/17, Loss: 0.7529\nEpoch: 61, Step: 013/17, Loss: 0.7515\nEpoch: 61, Step: 014/17, Loss: 0.7496\nEpoch: 61, Step: 015/17, Loss: 0.7555\nEpoch: 61, Step: 016/17, Loss: 0.7499\nEpoch: 61, Step: 017/17, Loss: 0.7518\nEpoch: 62, Step: 001/17, Loss: 0.7427\nEpoch: 62, Step: 002/17, Loss: 0.7435\nEpoch: 62, Step: 003/17, Loss: 0.7422\nEpoch: 62, Step: 004/17, Loss: 0.7428\nEpoch: 62, Step: 005/17, Loss: 0.7461\nEpoch: 62, Step: 006/17, Loss: 0.7456\nEpoch: 62, Step: 007/17, Loss: 0.7467\nEpoch: 62, Step: 008/17, Loss: 0.7474\nEpoch: 62, Step: 009/17, Loss: 0.7495\nEpoch: 62, Step: 010/17, Loss: 0.7492\nEpoch: 62, Step: 011/17, Loss: 0.7452\nEpoch: 62, Step: 012/17, Loss: 0.7476\nEpoch: 62, Step: 013/17, Loss: 0.7478\nEpoch: 62, Step: 014/17, Loss: 0.7522\nEpoch: 62, Step: 015/17, Loss: 0.7521\nEpoch: 62, Step: 016/17, Loss: 0.7534\nEpoch: 62, Step: 017/17, Loss: 0.7586\nEpoch: 63, Step: 001/17, Loss: 0.7423\nEpoch: 63, Step: 002/17, Loss: 0.7446\nEpoch: 63, Step: 003/17, Loss: 0.7435\nEpoch: 63, Step: 004/17, Loss: 0.7444\nEpoch: 63, Step: 005/17, Loss: 0.7449\nEpoch: 63, Step: 006/17, Loss: 0.7431\nEpoch: 63, Step: 007/17, Loss: 0.7436\nEpoch: 63, Step: 008/17, Loss: 0.7460\nEpoch: 63, Step: 009/17, Loss: 0.7457\nEpoch: 63, Step: 010/17, Loss: 0.7463\nEpoch: 63, Step: 011/17, Loss: 0.7491\nEpoch: 63, Step: 012/17, Loss: 0.7492\nEpoch: 63, Step: 013/17, Loss: 0.7529\nEpoch: 63, Step: 014/17, Loss: 0.7468\nEpoch: 63, Step: 015/17, Loss: 0.7542\nEpoch: 63, Step: 016/17, Loss: 0.7558\nEpoch: 63, Step: 017/17, Loss: 0.7536\nEpoch: 64, Step: 001/17, Loss: 0.7424\nEpoch: 64, Step: 002/17, Loss: 0.7420\nEpoch: 64, Step: 003/17, Loss: 0.7454\nEpoch: 64, Step: 004/17, Loss: 0.7427\nEpoch: 64, Step: 005/17, Loss: 0.7436\nEpoch: 64, Step: 006/17, Loss: 0.7437\nEpoch: 64, Step: 007/17, Loss: 0.7447\nEpoch: 64, Step: 008/17, Loss: 0.7449\nEpoch: 64, Step: 009/17, Loss: 0.7465\nEpoch: 64, Step: 010/17, Loss: 0.7481\nEpoch: 64, Step: 011/17, Loss: 0.7465\nEpoch: 64, Step: 012/17, Loss: 0.7479\nEpoch: 64, Step: 013/17, Loss: 0.7515\nEpoch: 64, Step: 014/17, Loss: 0.7530\nEpoch: 64, Step: 015/17, Loss: 0.7520\nEpoch: 64, Step: 016/17, Loss: 0.7528\nEpoch: 64, Step: 017/17, Loss: 0.7590\nEpoch: 65, Step: 001/17, Loss: 0.7432\nEpoch: 65, Step: 002/17, Loss: 0.7434\nEpoch: 65, Step: 003/17, Loss: 0.7415\nEpoch: 65, Step: 004/17, Loss: 0.7437\nEpoch: 65, Step: 005/17, Loss: 0.7447\nEpoch: 65, Step: 006/17, Loss: 0.7450\nEpoch: 65, Step: 007/17, Loss: 0.7438\nEpoch: 65, Step: 008/17, Loss: 0.7456\nEpoch: 65, Step: 009/17, Loss: 0.7443\nEpoch: 65, Step: 010/17, Loss: 0.7509\nEpoch: 65, Step: 011/17, Loss: 0.7471\nEpoch: 65, Step: 012/17, Loss: 0.7485\nEpoch: 65, Step: 013/17, Loss: 0.7493\nEpoch: 65, Step: 014/17, Loss: 0.7526\nEpoch: 65, Step: 015/17, Loss: 0.7492\nEpoch: 65, Step: 016/17, Loss: 0.7543\nEpoch: 65, Step: 017/17, Loss: 0.7514\nEpoch: 66, Step: 001/17, Loss: 0.7435\nEpoch: 66, Step: 002/17, Loss: 0.7420\nEpoch: 66, Step: 003/17, Loss: 0.7447\nEpoch: 66, Step: 004/17, Loss: 0.7422\nEpoch: 66, Step: 005/17, Loss: 0.7454\nEpoch: 66, Step: 006/17, Loss: 0.7436\nEpoch: 66, Step: 007/17, Loss: 0.7465\nEpoch: 66, Step: 008/17, Loss: 0.7455\nEpoch: 66, Step: 009/17, Loss: 0.7472\nEpoch: 66, Step: 010/17, Loss: 0.7463\nEpoch: 66, Step: 011/17, Loss: 0.7481\nEpoch: 66, Step: 012/17, Loss: 0.7470\nEpoch: 66, Step: 013/17, Loss: 0.7481\nEpoch: 66, Step: 014/17, Loss: 0.7499\nEpoch: 66, Step: 015/17, Loss: 0.7542\nEpoch: 66, Step: 016/17, Loss: 0.7511\nEpoch: 66, Step: 017/17, Loss: 0.7562\nEpoch: 67, Step: 001/17, Loss: 0.7442\nEpoch: 67, Step: 002/17, Loss: 0.7409\nEpoch: 67, Step: 003/17, Loss: 0.7430\nEpoch: 67, Step: 004/17, Loss: 0.7440\nEpoch: 67, Step: 005/17, Loss: 0.7436\nEpoch: 67, Step: 006/17, Loss: 0.7432\nEpoch: 67, Step: 007/17, Loss: 0.7431\nEpoch: 67, Step: 008/17, Loss: 0.7468\nEpoch: 67, Step: 009/17, Loss: 0.7445\nEpoch: 67, Step: 010/17, Loss: 0.7473\nEpoch: 67, Step: 011/17, Loss: 0.7457\nEpoch: 67, Step: 012/17, Loss: 0.7478\nEpoch: 67, Step: 013/17, Loss: 0.7487\nEpoch: 67, Step: 014/17, Loss: 0.7524\nEpoch: 67, Step: 015/17, Loss: 0.7519\nEpoch: 67, Step: 016/17, Loss: 0.7559\nEpoch: 67, Step: 017/17, Loss: 0.7512\nEpoch: 68, Step: 001/17, Loss: 0.7413\nEpoch: 68, Step: 002/17, Loss: 0.7415\nEpoch: 68, Step: 003/17, Loss: 0.7407\nEpoch: 68, Step: 004/17, Loss: 0.7407\nEpoch: 68, Step: 005/17, Loss: 0.7439\nEpoch: 68, Step: 006/17, Loss: 0.7466\nEpoch: 68, Step: 007/17, Loss: 0.7438\nEpoch: 68, Step: 008/17, Loss: 0.7474\nEpoch: 68, Step: 009/17, Loss: 0.7444\nEpoch: 68, Step: 010/17, Loss: 0.7459\nEpoch: 68, Step: 011/17, Loss: 0.7472\nEpoch: 68, Step: 012/17, Loss: 0.7476\nEpoch: 68, Step: 013/17, Loss: 0.7510\nEpoch: 68, Step: 014/17, Loss: 0.7525\nEpoch: 68, Step: 015/17, Loss: 0.7509\nEpoch: 68, Step: 016/17, Loss: 0.7504\nEpoch: 68, Step: 017/17, Loss: 0.7579\nEpoch: 69, Step: 001/17, Loss: 0.7406\nEpoch: 69, Step: 002/17, Loss: 0.7418\nEpoch: 69, Step: 003/17, Loss: 0.7403\nEpoch: 69, Step: 004/17, Loss: 0.7425\nEpoch: 69, Step: 005/17, Loss: 0.7435\nEpoch: 69, Step: 006/17, Loss: 0.7442\nEpoch: 69, Step: 007/17, Loss: 0.7427\nEpoch: 69, Step: 008/17, Loss: 0.7443\nEpoch: 69, Step: 009/17, Loss: 0.7488\nEpoch: 69, Step: 010/17, Loss: 0.7486\nEpoch: 69, Step: 011/17, Loss: 0.7491\nEpoch: 69, Step: 012/17, Loss: 0.7491\nEpoch: 69, Step: 013/17, Loss: 0.7485\nEpoch: 69, Step: 014/17, Loss: 0.7503\nEpoch: 69, Step: 015/17, Loss: 0.7500\nEpoch: 69, Step: 016/17, Loss: 0.7563\nEpoch: 69, Step: 017/17, Loss: 0.7560\nEpoch: 70, Step: 001/17, Loss: 0.7393\nEpoch: 70, Step: 002/17, Loss: 0.7429\nEpoch: 70, Step: 003/17, Loss: 0.7410\nEpoch: 70, Step: 004/17, Loss: 0.7457\nEpoch: 70, Step: 005/17, Loss: 0.7451\nEpoch: 70, Step: 006/17, Loss: 0.7442\nEpoch: 70, Step: 007/17, Loss: 0.7452\nEpoch: 70, Step: 008/17, Loss: 0.7477\nEpoch: 70, Step: 009/17, Loss: 0.7473\nEpoch: 70, Step: 010/17, Loss: 0.7473\nEpoch: 70, Step: 011/17, Loss: 0.7462\nEpoch: 70, Step: 012/17, Loss: 0.7464\nEpoch: 70, Step: 013/17, Loss: 0.7480\nEpoch: 70, Step: 014/17, Loss: 0.7483\nEpoch: 70, Step: 015/17, Loss: 0.7539\nEpoch: 70, Step: 016/17, Loss: 0.7537\nEpoch: 70, Step: 017/17, Loss: 0.7567\nEpoch: 71, Step: 001/17, Loss: 0.7422\nEpoch: 71, Step: 002/17, Loss: 0.7400\nEpoch: 71, Step: 003/17, Loss: 0.7443\nEpoch: 71, Step: 004/17, Loss: 0.7413\nEpoch: 71, Step: 005/17, Loss: 0.7456\nEpoch: 71, Step: 006/17, Loss: 0.7436\nEpoch: 71, Step: 007/17, Loss: 0.7436\nEpoch: 71, Step: 008/17, Loss: 0.7451\nEpoch: 71, Step: 009/17, Loss: 0.7453\nEpoch: 71, Step: 010/17, Loss: 0.7486\nEpoch: 71, Step: 011/17, Loss: 0.7489\nEpoch: 71, Step: 012/17, Loss: 0.7492\nEpoch: 71, Step: 013/17, Loss: 0.7516\nEpoch: 71, Step: 014/17, Loss: 0.7507\nEpoch: 71, Step: 015/17, Loss: 0.7518\nEpoch: 71, Step: 016/17, Loss: 0.7495\nEpoch: 71, Step: 017/17, Loss: 0.7484\nEpoch: 72, Step: 001/17, Loss: 0.7417\nEpoch: 72, Step: 002/17, Loss: 0.7431\nEpoch: 72, Step: 003/17, Loss: 0.7404\nEpoch: 72, Step: 004/17, Loss: 0.7434\nEpoch: 72, Step: 005/17, Loss: 0.7445\nEpoch: 72, Step: 006/17, Loss: 0.7448\nEpoch: 72, Step: 007/17, Loss: 0.7413\nEpoch: 72, Step: 008/17, Loss: 0.7431\nEpoch: 72, Step: 009/17, Loss: 0.7451\nEpoch: 72, Step: 010/17, Loss: 0.7467\nEpoch: 72, Step: 011/17, Loss: 0.7471\nEpoch: 72, Step: 012/17, Loss: 0.7464\nEpoch: 72, Step: 013/17, Loss: 0.7490\nEpoch: 72, Step: 014/17, Loss: 0.7501\nEpoch: 72, Step: 015/17, Loss: 0.7522\nEpoch: 72, Step: 016/17, Loss: 0.7532\nEpoch: 72, Step: 017/17, Loss: 0.7531\nEpoch: 73, Step: 001/17, Loss: 0.7404\nEpoch: 73, Step: 002/17, Loss: 0.7404\nEpoch: 73, Step: 003/17, Loss: 0.7404\nEpoch: 73, Step: 004/17, Loss: 0.7391\nEpoch: 73, Step: 005/17, Loss: 0.7426\nEpoch: 73, Step: 006/17, Loss: 0.7445\nEpoch: 73, Step: 007/17, Loss: 0.7433\nEpoch: 73, Step: 008/17, Loss: 0.7449\nEpoch: 73, Step: 009/17, Loss: 0.7466\nEpoch: 73, Step: 010/17, Loss: 0.7439\nEpoch: 73, Step: 011/17, Loss: 0.7476\nEpoch: 73, Step: 012/17, Loss: 0.7478\nEpoch: 73, Step: 013/17, Loss: 0.7479\nEpoch: 73, Step: 014/17, Loss: 0.7471\nEpoch: 73, Step: 015/17, Loss: 0.7499\nEpoch: 73, Step: 016/17, Loss: 0.7535\nEpoch: 73, Step: 017/17, Loss: 0.7603\nEpoch: 74, Step: 001/17, Loss: 0.7408\nEpoch: 74, Step: 002/17, Loss: 0.7389\nEpoch: 74, Step: 003/17, Loss: 0.7403\nEpoch: 74, Step: 004/17, Loss: 0.7429\nEpoch: 74, Step: 005/17, Loss: 0.7437\nEpoch: 74, Step: 006/17, Loss: 0.7424\nEpoch: 74, Step: 007/17, Loss: 0.7424\nEpoch: 74, Step: 008/17, Loss: 0.7419\nEpoch: 74, Step: 009/17, Loss: 0.7456\nEpoch: 74, Step: 010/17, Loss: 0.7454\nEpoch: 74, Step: 011/17, Loss: 0.7472\nEpoch: 74, Step: 012/17, Loss: 0.7459\nEpoch: 74, Step: 013/17, Loss: 0.7499\nEpoch: 74, Step: 014/17, Loss: 0.7470\nEpoch: 74, Step: 015/17, Loss: 0.7494\nEpoch: 74, Step: 016/17, Loss: 0.7550\nEpoch: 74, Step: 017/17, Loss: 0.7571\nEpoch: 75, Step: 001/17, Loss: 0.7393\nEpoch: 75, Step: 002/17, Loss: 0.7397\nEpoch: 75, Step: 003/17, Loss: 0.7389\nEpoch: 75, Step: 004/17, Loss: 0.7424\nEpoch: 75, Step: 005/17, Loss: 0.7407\nEpoch: 75, Step: 006/17, Loss: 0.7437\nEpoch: 75, Step: 007/17, Loss: 0.7467\nEpoch: 75, Step: 008/17, Loss: 0.7424\nEpoch: 75, Step: 009/17, Loss: 0.7468\nEpoch: 75, Step: 010/17, Loss: 0.7464\nEpoch: 75, Step: 011/17, Loss: 0.7453\nEpoch: 75, Step: 012/17, Loss: 0.7491\nEpoch: 75, Step: 013/17, Loss: 0.7476\nEpoch: 75, Step: 014/17, Loss: 0.7515\nEpoch: 75, Step: 015/17, Loss: 0.7515\nEpoch: 75, Step: 016/17, Loss: 0.7496\nEpoch: 75, Step: 017/17, Loss: 0.7514\nEpoch: 76, Step: 001/17, Loss: 0.7389\nEpoch: 76, Step: 002/17, Loss: 0.7419\nEpoch: 76, Step: 003/17, Loss: 0.7393\nEpoch: 76, Step: 004/17, Loss: 0.7415\nEpoch: 76, Step: 005/17, Loss: 0.7426\nEpoch: 76, Step: 006/17, Loss: 0.7432\nEpoch: 76, Step: 007/17, Loss: 0.7432\nEpoch: 76, Step: 008/17, Loss: 0.7471\nEpoch: 76, Step: 009/17, Loss: 0.7448\nEpoch: 76, Step: 010/17, Loss: 0.7446\nEpoch: 76, Step: 011/17, Loss: 0.7491\nEpoch: 76, Step: 012/17, Loss: 0.7443\nEpoch: 76, Step: 013/17, Loss: 0.7475\nEpoch: 76, Step: 014/17, Loss: 0.7484\nEpoch: 76, Step: 015/17, Loss: 0.7509\nEpoch: 76, Step: 016/17, Loss: 0.7534\nEpoch: 76, Step: 017/17, Loss: 0.7506\nEpoch: 77, Step: 001/17, Loss: 0.7423\nEpoch: 77, Step: 002/17, Loss: 0.7389\nEpoch: 77, Step: 003/17, Loss: 0.7408\nEpoch: 77, Step: 004/17, Loss: 0.7410\nEpoch: 77, Step: 005/17, Loss: 0.7417\nEpoch: 77, Step: 006/17, Loss: 0.7425\nEpoch: 77, Step: 007/17, Loss: 0.7440\nEpoch: 77, Step: 008/17, Loss: 0.7449\nEpoch: 77, Step: 009/17, Loss: 0.7447\nEpoch: 77, Step: 010/17, Loss: 0.7451\nEpoch: 77, Step: 011/17, Loss: 0.7451\nEpoch: 77, Step: 012/17, Loss: 0.7454\nEpoch: 77, Step: 013/17, Loss: 0.7469\nEpoch: 77, Step: 014/17, Loss: 0.7489\nEpoch: 77, Step: 015/17, Loss: 0.7491\nEpoch: 77, Step: 016/17, Loss: 0.7515\nEpoch: 77, Step: 017/17, Loss: 0.7501\nEpoch: 78, Step: 001/17, Loss: 0.7416\nEpoch: 78, Step: 002/17, Loss: 0.7405\nEpoch: 78, Step: 003/17, Loss: 0.7409\nEpoch: 78, Step: 004/17, Loss: 0.7414\nEpoch: 78, Step: 005/17, Loss: 0.7413\nEpoch: 78, Step: 006/17, Loss: 0.7431\nEpoch: 78, Step: 007/17, Loss: 0.7435\nEpoch: 78, Step: 008/17, Loss: 0.7426\nEpoch: 78, Step: 009/17, Loss: 0.7421\nEpoch: 78, Step: 010/17, Loss: 0.7453\nEpoch: 78, Step: 011/17, Loss: 0.7454\nEpoch: 78, Step: 012/17, Loss: 0.7452\nEpoch: 78, Step: 013/17, Loss: 0.7490\nEpoch: 78, Step: 014/17, Loss: 0.7469\nEpoch: 78, Step: 015/17, Loss: 0.7502\nEpoch: 78, Step: 016/17, Loss: 0.7502\nEpoch: 78, Step: 017/17, Loss: 0.7541\nEpoch: 79, Step: 001/17, Loss: 0.7423\nEpoch: 79, Step: 002/17, Loss: 0.7424\nEpoch: 79, Step: 003/17, Loss: 0.7414\nEpoch: 79, Step: 004/17, Loss: 0.7420\nEpoch: 79, Step: 005/17, Loss: 0.7403\nEpoch: 79, Step: 006/17, Loss: 0.7413\nEpoch: 79, Step: 007/17, Loss: 0.7427\nEpoch: 79, Step: 008/17, Loss: 0.7432\nEpoch: 79, Step: 009/17, Loss: 0.7460\nEpoch: 79, Step: 010/17, Loss: 0.7440\nEpoch: 79, Step: 011/17, Loss: 0.7433\nEpoch: 79, Step: 012/17, Loss: 0.7455\nEpoch: 79, Step: 013/17, Loss: 0.7457\nEpoch: 79, Step: 014/17, Loss: 0.7477\nEpoch: 79, Step: 015/17, Loss: 0.7492\nEpoch: 79, Step: 016/17, Loss: 0.7538\nEpoch: 79, Step: 017/17, Loss: 0.7528\nEpoch: 80, Step: 001/17, Loss: 0.7410\nEpoch: 80, Step: 002/17, Loss: 0.7405\nEpoch: 80, Step: 003/17, Loss: 0.7418\nEpoch: 80, Step: 004/17, Loss: 0.7410\nEpoch: 80, Step: 005/17, Loss: 0.7431\nEpoch: 80, Step: 006/17, Loss: 0.7410\nEpoch: 80, Step: 007/17, Loss: 0.7421\nEpoch: 80, Step: 008/17, Loss: 0.7454\nEpoch: 80, Step: 009/17, Loss: 0.7437\nEpoch: 80, Step: 010/17, Loss: 0.7467\nEpoch: 80, Step: 011/17, Loss: 0.7448\nEpoch: 80, Step: 012/17, Loss: 0.7448\nEpoch: 80, Step: 013/17, Loss: 0.7481\nEpoch: 80, Step: 014/17, Loss: 0.7492\nEpoch: 80, Step: 015/17, Loss: 0.7505\nEpoch: 80, Step: 016/17, Loss: 0.7479\nEpoch: 80, Step: 017/17, Loss: 0.7484\nEpoch: 81, Step: 001/17, Loss: 0.7419\nEpoch: 81, Step: 002/17, Loss: 0.7421\nEpoch: 81, Step: 003/17, Loss: 0.7401\nEpoch: 81, Step: 004/17, Loss: 0.7410\nEpoch: 81, Step: 005/17, Loss: 0.7395\nEpoch: 81, Step: 006/17, Loss: 0.7405\nEpoch: 81, Step: 007/17, Loss: 0.7418\nEpoch: 81, Step: 008/17, Loss: 0.7426\nEpoch: 81, Step: 009/17, Loss: 0.7427\nEpoch: 81, Step: 010/17, Loss: 0.7463\nEpoch: 81, Step: 011/17, Loss: 0.7475\nEpoch: 81, Step: 012/17, Loss: 0.7441\nEpoch: 81, Step: 013/17, Loss: 0.7478\nEpoch: 81, Step: 014/17, Loss: 0.7487\nEpoch: 81, Step: 015/17, Loss: 0.7508\nEpoch: 81, Step: 016/17, Loss: 0.7516\nEpoch: 81, Step: 017/17, Loss: 0.7551\nEpoch: 82, Step: 001/17, Loss: 0.7394\nEpoch: 82, Step: 002/17, Loss: 0.7396\nEpoch: 82, Step: 003/17, Loss: 0.7401\nEpoch: 82, Step: 004/17, Loss: 0.7419\nEpoch: 82, Step: 005/17, Loss: 0.7396\nEpoch: 82, Step: 006/17, Loss: 0.7410\nEpoch: 82, Step: 007/17, Loss: 0.7426\nEpoch: 82, Step: 008/17, Loss: 0.7429\nEpoch: 82, Step: 009/17, Loss: 0.7441\nEpoch: 82, Step: 010/17, Loss: 0.7467\nEpoch: 82, Step: 011/17, Loss: 0.7443\nEpoch: 82, Step: 012/17, Loss: 0.7456\nEpoch: 82, Step: 013/17, Loss: 0.7472\nEpoch: 82, Step: 014/17, Loss: 0.7473\nEpoch: 82, Step: 015/17, Loss: 0.7494\nEpoch: 82, Step: 016/17, Loss: 0.7486\nEpoch: 82, Step: 017/17, Loss: 0.7495\nEpoch: 83, Step: 001/17, Loss: 0.7400\nEpoch: 83, Step: 002/17, Loss: 0.7413\nEpoch: 83, Step: 003/17, Loss: 0.7389\nEpoch: 83, Step: 004/17, Loss: 0.7407\nEpoch: 83, Step: 005/17, Loss: 0.7388\nEpoch: 83, Step: 006/17, Loss: 0.7417\nEpoch: 83, Step: 007/17, Loss: 0.7435\nEpoch: 83, Step: 008/17, Loss: 0.7436\nEpoch: 83, Step: 009/17, Loss: 0.7435\nEpoch: 83, Step: 010/17, Loss: 0.7430\nEpoch: 83, Step: 011/17, Loss: 0.7454\nEpoch: 83, Step: 012/17, Loss: 0.7449\nEpoch: 83, Step: 013/17, Loss: 0.7441\nEpoch: 83, Step: 014/17, Loss: 0.7476\nEpoch: 83, Step: 015/17, Loss: 0.7462\nEpoch: 83, Step: 016/17, Loss: 0.7507\nEpoch: 83, Step: 017/17, Loss: 0.7535\nEpoch: 84, Step: 001/17, Loss: 0.7408\nEpoch: 84, Step: 002/17, Loss: 0.7411\nEpoch: 84, Step: 003/17, Loss: 0.7396\nEpoch: 84, Step: 004/17, Loss: 0.7415\nEpoch: 84, Step: 005/17, Loss: 0.7398\nEpoch: 84, Step: 006/17, Loss: 0.7400\nEpoch: 84, Step: 007/17, Loss: 0.7404\nEpoch: 84, Step: 008/17, Loss: 0.7435\nEpoch: 84, Step: 009/17, Loss: 0.7423\nEpoch: 84, Step: 010/17, Loss: 0.7441\nEpoch: 84, Step: 011/17, Loss: 0.7447\nEpoch: 84, Step: 012/17, Loss: 0.7445\nEpoch: 84, Step: 013/17, Loss: 0.7448\nEpoch: 84, Step: 014/17, Loss: 0.7465\nEpoch: 84, Step: 015/17, Loss: 0.7480\nEpoch: 84, Step: 016/17, Loss: 0.7488\nEpoch: 84, Step: 017/17, Loss: 0.7507\nEpoch: 85, Step: 001/17, Loss: 0.7408\nEpoch: 85, Step: 002/17, Loss: 0.7384\nEpoch: 85, Step: 003/17, Loss: 0.7404\nEpoch: 85, Step: 004/17, Loss: 0.7387\nEpoch: 85, Step: 005/17, Loss: 0.7422\nEpoch: 85, Step: 006/17, Loss: 0.7420\nEpoch: 85, Step: 007/17, Loss: 0.7400\nEpoch: 85, Step: 008/17, Loss: 0.7408\nEpoch: 85, Step: 009/17, Loss: 0.7452\nEpoch: 85, Step: 010/17, Loss: 0.7452\nEpoch: 85, Step: 011/17, Loss: 0.7441\nEpoch: 85, Step: 012/17, Loss: 0.7467\nEpoch: 85, Step: 013/17, Loss: 0.7459\nEpoch: 85, Step: 014/17, Loss: 0.7470\nEpoch: 85, Step: 015/17, Loss: 0.7461\nEpoch: 85, Step: 016/17, Loss: 0.7513\nEpoch: 85, Step: 017/17, Loss: 0.7497\nEpoch: 86, Step: 001/17, Loss: 0.7403\nEpoch: 86, Step: 002/17, Loss: 0.7378\nEpoch: 86, Step: 003/17, Loss: 0.7421\nEpoch: 86, Step: 004/17, Loss: 0.7432\nEpoch: 86, Step: 005/17, Loss: 0.7437\nEpoch: 86, Step: 006/17, Loss: 0.7385\nEpoch: 86, Step: 007/17, Loss: 0.7417\nEpoch: 86, Step: 008/17, Loss: 0.7398\nEpoch: 86, Step: 009/17, Loss: 0.7415\nEpoch: 86, Step: 010/17, Loss: 0.7422\nEpoch: 86, Step: 011/17, Loss: 0.7429\nEpoch: 86, Step: 012/17, Loss: 0.7445\nEpoch: 86, Step: 013/17, Loss: 0.7462\nEpoch: 86, Step: 014/17, Loss: 0.7473\nEpoch: 86, Step: 015/17, Loss: 0.7471\nEpoch: 86, Step: 016/17, Loss: 0.7486\nEpoch: 86, Step: 017/17, Loss: 0.7485\nEpoch: 87, Step: 001/17, Loss: 0.7390\nEpoch: 87, Step: 002/17, Loss: 0.7384\nEpoch: 87, Step: 003/17, Loss: 0.7401\nEpoch: 87, Step: 004/17, Loss: 0.7410\nEpoch: 87, Step: 005/17, Loss: 0.7394\nEpoch: 87, Step: 006/17, Loss: 0.7417\nEpoch: 87, Step: 007/17, Loss: 0.7412\nEpoch: 87, Step: 008/17, Loss: 0.7436\nEpoch: 87, Step: 009/17, Loss: 0.7437\nEpoch: 87, Step: 010/17, Loss: 0.7447\nEpoch: 87, Step: 011/17, Loss: 0.7440\nEpoch: 87, Step: 012/17, Loss: 0.7461\nEpoch: 87, Step: 013/17, Loss: 0.7456\nEpoch: 87, Step: 014/17, Loss: 0.7457\nEpoch: 87, Step: 015/17, Loss: 0.7462\nEpoch: 87, Step: 016/17, Loss: 0.7513\nEpoch: 87, Step: 017/17, Loss: 0.7456\nEpoch: 88, Step: 001/17, Loss: 0.7419\nEpoch: 88, Step: 002/17, Loss: 0.7397\nEpoch: 88, Step: 003/17, Loss: 0.7415\nEpoch: 88, Step: 004/17, Loss: 0.7378\nEpoch: 88, Step: 005/17, Loss: 0.7424\nEpoch: 88, Step: 006/17, Loss: 0.7397\nEpoch: 88, Step: 007/17, Loss: 0.7411\nEpoch: 88, Step: 008/17, Loss: 0.7428\nEpoch: 88, Step: 009/17, Loss: 0.7402\nEpoch: 88, Step: 010/17, Loss: 0.7425\nEpoch: 88, Step: 011/17, Loss: 0.7420\nEpoch: 88, Step: 012/17, Loss: 0.7470\nEpoch: 88, Step: 013/17, Loss: 0.7479\nEpoch: 88, Step: 014/17, Loss: 0.7454\nEpoch: 88, Step: 015/17, Loss: 0.7501\nEpoch: 88, Step: 016/17, Loss: 0.7469\nEpoch: 88, Step: 017/17, Loss: 0.7478\nEpoch: 89, Step: 001/17, Loss: 0.7375\nEpoch: 89, Step: 002/17, Loss: 0.7386\nEpoch: 89, Step: 003/17, Loss: 0.7400\nEpoch: 89, Step: 004/17, Loss: 0.7391\nEpoch: 89, Step: 005/17, Loss: 0.7408\nEpoch: 89, Step: 006/17, Loss: 0.7409\nEpoch: 89, Step: 007/17, Loss: 0.7412\nEpoch: 89, Step: 008/17, Loss: 0.7421\nEpoch: 89, Step: 009/17, Loss: 0.7418\nEpoch: 89, Step: 010/17, Loss: 0.7415\nEpoch: 89, Step: 011/17, Loss: 0.7448\nEpoch: 89, Step: 012/17, Loss: 0.7462\nEpoch: 89, Step: 013/17, Loss: 0.7474\nEpoch: 89, Step: 014/17, Loss: 0.7453\nEpoch: 89, Step: 015/17, Loss: 0.7481\nEpoch: 89, Step: 016/17, Loss: 0.7498\nEpoch: 89, Step: 017/17, Loss: 0.7516\nEpoch: 90, Step: 001/17, Loss: 0.7393\nEpoch: 90, Step: 002/17, Loss: 0.7397\nEpoch: 90, Step: 003/17, Loss: 0.7394\nEpoch: 90, Step: 004/17, Loss: 0.7415\nEpoch: 90, Step: 005/17, Loss: 0.7402\nEpoch: 90, Step: 006/17, Loss: 0.7428\nEpoch: 90, Step: 007/17, Loss: 0.7390\nEpoch: 90, Step: 008/17, Loss: 0.7429\nEpoch: 90, Step: 009/17, Loss: 0.7421\nEpoch: 90, Step: 010/17, Loss: 0.7436\nEpoch: 90, Step: 011/17, Loss: 0.7426\nEpoch: 90, Step: 012/17, Loss: 0.7447\nEpoch: 90, Step: 013/17, Loss: 0.7444\nEpoch: 90, Step: 014/17, Loss: 0.7492\nEpoch: 90, Step: 015/17, Loss: 0.7476\nEpoch: 90, Step: 016/17, Loss: 0.7467\nEpoch: 90, Step: 017/17, Loss: 0.7473\nEpoch: 91, Step: 001/17, Loss: 0.7393\nEpoch: 91, Step: 002/17, Loss: 0.7397\nEpoch: 91, Step: 003/17, Loss: 0.7398\nEpoch: 91, Step: 004/17, Loss: 0.7398\nEpoch: 91, Step: 005/17, Loss: 0.7414\nEpoch: 91, Step: 006/17, Loss: 0.7396\nEpoch: 91, Step: 007/17, Loss: 0.7418\nEpoch: 91, Step: 008/17, Loss: 0.7416\nEpoch: 91, Step: 009/17, Loss: 0.7420\nEpoch: 91, Step: 010/17, Loss: 0.7428\nEpoch: 91, Step: 011/17, Loss: 0.7442\nEpoch: 91, Step: 012/17, Loss: 0.7451\nEpoch: 91, Step: 013/17, Loss: 0.7425\nEpoch: 91, Step: 014/17, Loss: 0.7447\nEpoch: 91, Step: 015/17, Loss: 0.7493\nEpoch: 91, Step: 016/17, Loss: 0.7496\nEpoch: 91, Step: 017/17, Loss: 0.7515\nEpoch: 92, Step: 001/17, Loss: 0.7391\nEpoch: 92, Step: 002/17, Loss: 0.7376\nEpoch: 92, Step: 003/17, Loss: 0.7395\nEpoch: 92, Step: 004/17, Loss: 0.7403\nEpoch: 92, Step: 005/17, Loss: 0.7390\nEpoch: 92, Step: 006/17, Loss: 0.7410\nEpoch: 92, Step: 007/17, Loss: 0.7433\nEpoch: 92, Step: 008/17, Loss: 0.7417\nEpoch: 92, Step: 009/17, Loss: 0.7420\nEpoch: 92, Step: 010/17, Loss: 0.7455\nEpoch: 92, Step: 011/17, Loss: 0.7440\nEpoch: 92, Step: 012/17, Loss: 0.7435\nEpoch: 92, Step: 013/17, Loss: 0.7467\nEpoch: 92, Step: 014/17, Loss: 0.7454\nEpoch: 92, Step: 015/17, Loss: 0.7476\nEpoch: 92, Step: 016/17, Loss: 0.7492\nEpoch: 92, Step: 017/17, Loss: 0.7460\nEpoch: 93, Step: 001/17, Loss: 0.7391\nEpoch: 93, Step: 002/17, Loss: 0.7383\nEpoch: 93, Step: 003/17, Loss: 0.7410\nEpoch: 93, Step: 004/17, Loss: 0.7396\nEpoch: 93, Step: 005/17, Loss: 0.7421\nEpoch: 93, Step: 006/17, Loss: 0.7394\nEpoch: 93, Step: 007/17, Loss: 0.7416\nEpoch: 93, Step: 008/17, Loss: 0.7408\nEpoch: 93, Step: 009/17, Loss: 0.7423\nEpoch: 93, Step: 010/17, Loss: 0.7426\nEpoch: 93, Step: 011/17, Loss: 0.7432\nEpoch: 93, Step: 012/17, Loss: 0.7420\nEpoch: 93, Step: 013/17, Loss: 0.7460\nEpoch: 93, Step: 014/17, Loss: 0.7460\nEpoch: 93, Step: 015/17, Loss: 0.7445\nEpoch: 93, Step: 016/17, Loss: 0.7487\nEpoch: 93, Step: 017/17, Loss: 0.7467\nEpoch: 94, Step: 001/17, Loss: 0.7380\nEpoch: 94, Step: 002/17, Loss: 0.7381\nEpoch: 94, Step: 003/17, Loss: 0.7404\nEpoch: 94, Step: 004/17, Loss: 0.7402\nEpoch: 94, Step: 005/17, Loss: 0.7425\nEpoch: 94, Step: 006/17, Loss: 0.7422\nEpoch: 94, Step: 007/17, Loss: 0.7390\nEpoch: 94, Step: 008/17, Loss: 0.7415\nEpoch: 94, Step: 009/17, Loss: 0.7405\nEpoch: 94, Step: 010/17, Loss: 0.7441\nEpoch: 94, Step: 011/17, Loss: 0.7409\nEpoch: 94, Step: 012/17, Loss: 0.7464\nEpoch: 94, Step: 013/17, Loss: 0.7421\nEpoch: 94, Step: 014/17, Loss: 0.7462\nEpoch: 94, Step: 015/17, Loss: 0.7447\nEpoch: 94, Step: 016/17, Loss: 0.7444\nEpoch: 94, Step: 017/17, Loss: 0.7486\nEpoch: 95, Step: 001/17, Loss: 0.7390\nEpoch: 95, Step: 002/17, Loss: 0.7367\nEpoch: 95, Step: 003/17, Loss: 0.7386\nEpoch: 95, Step: 004/17, Loss: 0.7398\nEpoch: 95, Step: 005/17, Loss: 0.7395\nEpoch: 95, Step: 006/17, Loss: 0.7408\nEpoch: 95, Step: 007/17, Loss: 0.7401\nEpoch: 95, Step: 008/17, Loss: 0.7413\nEpoch: 95, Step: 009/17, Loss: 0.7400\nEpoch: 95, Step: 010/17, Loss: 0.7417\nEpoch: 95, Step: 011/17, Loss: 0.7439\nEpoch: 95, Step: 012/17, Loss: 0.7446\nEpoch: 95, Step: 013/17, Loss: 0.7437\nEpoch: 95, Step: 014/17, Loss: 0.7430\nEpoch: 95, Step: 015/17, Loss: 0.7456\nEpoch: 95, Step: 016/17, Loss: 0.7494\nEpoch: 95, Step: 017/17, Loss: 0.7460\nEpoch: 96, Step: 001/17, Loss: 0.7375\nEpoch: 96, Step: 002/17, Loss: 0.7379\nEpoch: 96, Step: 003/17, Loss: 0.7396\nEpoch: 96, Step: 004/17, Loss: 0.7369\nEpoch: 96, Step: 005/17, Loss: 0.7388\nEpoch: 96, Step: 006/17, Loss: 0.7399\nEpoch: 96, Step: 007/17, Loss: 0.7395\nEpoch: 96, Step: 008/17, Loss: 0.7396\nEpoch: 96, Step: 009/17, Loss: 0.7419\nEpoch: 96, Step: 010/17, Loss: 0.7434\nEpoch: 96, Step: 011/17, Loss: 0.7428\nEpoch: 96, Step: 012/17, Loss: 0.7428\nEpoch: 96, Step: 013/17, Loss: 0.7438\nEpoch: 96, Step: 014/17, Loss: 0.7476\nEpoch: 96, Step: 015/17, Loss: 0.7450\nEpoch: 96, Step: 016/17, Loss: 0.7467\nEpoch: 96, Step: 017/17, Loss: 0.7445\nEpoch: 97, Step: 001/17, Loss: 0.7384\nEpoch: 97, Step: 002/17, Loss: 0.7387\nEpoch: 97, Step: 003/17, Loss: 0.7370\nEpoch: 97, Step: 004/17, Loss: 0.7386\nEpoch: 97, Step: 005/17, Loss: 0.7405\nEpoch: 97, Step: 006/17, Loss: 0.7406\nEpoch: 97, Step: 007/17, Loss: 0.7402\nEpoch: 97, Step: 008/17, Loss: 0.7407\nEpoch: 97, Step: 009/17, Loss: 0.7422\nEpoch: 97, Step: 010/17, Loss: 0.7452\nEpoch: 97, Step: 011/17, Loss: 0.7439\nEpoch: 97, Step: 012/17, Loss: 0.7430\nEpoch: 97, Step: 013/17, Loss: 0.7445\nEpoch: 97, Step: 014/17, Loss: 0.7429\nEpoch: 97, Step: 015/17, Loss: 0.7458\nEpoch: 97, Step: 016/17, Loss: 0.7452\nEpoch: 97, Step: 017/17, Loss: 0.7482\nEpoch: 98, Step: 001/17, Loss: 0.7372\nEpoch: 98, Step: 002/17, Loss: 0.7381\nEpoch: 98, Step: 003/17, Loss: 0.7387\nEpoch: 98, Step: 004/17, Loss: 0.7394\nEpoch: 98, Step: 005/17, Loss: 0.7397\nEpoch: 98, Step: 006/17, Loss: 0.7393\nEpoch: 98, Step: 007/17, Loss: 0.7411\nEpoch: 98, Step: 008/17, Loss: 0.7397\nEpoch: 98, Step: 009/17, Loss: 0.7415\nEpoch: 98, Step: 010/17, Loss: 0.7396\nEpoch: 98, Step: 011/17, Loss: 0.7408\nEpoch: 98, Step: 012/17, Loss: 0.7437\nEpoch: 98, Step: 013/17, Loss: 0.7417\nEpoch: 98, Step: 014/17, Loss: 0.7464\nEpoch: 98, Step: 015/17, Loss: 0.7454\nEpoch: 98, Step: 016/17, Loss: 0.7461\nEpoch: 98, Step: 017/17, Loss: 0.7483\nEpoch: 99, Step: 001/17, Loss: 0.7383\nEpoch: 99, Step: 002/17, Loss: 0.7385\nEpoch: 99, Step: 003/17, Loss: 0.7373\nEpoch: 99, Step: 004/17, Loss: 0.7392\nEpoch: 99, Step: 005/17, Loss: 0.7388\nEpoch: 99, Step: 006/17, Loss: 0.7405\nEpoch: 99, Step: 007/17, Loss: 0.7378\nEpoch: 99, Step: 008/17, Loss: 0.7394\nEpoch: 99, Step: 009/17, Loss: 0.7407\nEpoch: 99, Step: 010/17, Loss: 0.7418\nEpoch: 99, Step: 011/17, Loss: 0.7414\nEpoch: 99, Step: 012/17, Loss: 0.7445\nEpoch: 99, Step: 013/17, Loss: 0.7466\nEpoch: 99, Step: 014/17, Loss: 0.7433\nEpoch: 99, Step: 015/17, Loss: 0.7435\nEpoch: 99, Step: 016/17, Loss: 0.7442\nEpoch: 99, Step: 017/17, Loss: 0.7475\nEpoch: 100, Step: 001/17, Loss: 0.7361\nEpoch: 100, Step: 002/17, Loss: 0.7389\nEpoch: 100, Step: 003/17, Loss: 0.7383\nEpoch: 100, Step: 004/17, Loss: 0.7378\nEpoch: 100, Step: 005/17, Loss: 0.7396\nEpoch: 100, Step: 006/17, Loss: 0.7403\nEpoch: 100, Step: 007/17, Loss: 0.7392\nEpoch: 100, Step: 008/17, Loss: 0.7404\nEpoch: 100, Step: 009/17, Loss: 0.7400\nEpoch: 100, Step: 010/17, Loss: 0.7415\nEpoch: 100, Step: 011/17, Loss: 0.7404\nEpoch: 100, Step: 012/17, Loss: 0.7406\nEpoch: 100, Step: 013/17, Loss: 0.7437\nEpoch: 100, Step: 014/17, Loss: 0.7424\nEpoch: 100, Step: 015/17, Loss: 0.7450\nEpoch: 100, Step: 016/17, Loss: 0.7495\nEpoch: 100, Step: 017/17, Loss: 0.7462\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"## Upload node2vec to huggingface","metadata":{}},{"cell_type":"code","source":"node2vec","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T05:53:59.584160Z","iopub.execute_input":"2025-05-28T05:53:59.584408Z","iopub.status.idle":"2025-05-28T05:53:59.591335Z","shell.execute_reply.started":"2025-05-28T05:53:59.584384Z","shell.execute_reply":"2025-05-28T05:53:59.590644Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"Node2Vec(4267, 256)"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"# OR save just the state_dict (recommended)\ntorch.save(node2vec.state_dict(), \"/kaggle/working/training_outputs/node2vec_state_dict.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T05:56:19.368430Z","iopub.execute_input":"2025-05-28T05:56:19.369196Z","iopub.status.idle":"2025-05-28T05:56:19.381130Z","shell.execute_reply.started":"2025-05-28T05:56:19.369172Z","shell.execute_reply":"2025-05-28T05:56:19.380390Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"filepath = \"/kaggle/working/training_outputs/node2vec_state_dict.pth\"\n\nnew_node2vec = load_node2vec(node2vec_args, data_node2vec, filepath)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T06:01:00.288088Z","iopub.execute_input":"2025-05-28T06:01:00.288699Z","iopub.status.idle":"2025-05-28T06:01:00.394719Z","shell.execute_reply.started":"2025-05-28T06:01:00.288677Z","shell.execute_reply":"2025-05-28T06:01:00.393929Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"def compare_models(model_1, model_2):\n    for p1, p2 in zip(model_1.parameters(), model_2.parameters()):\n        if not torch.equal(p1.data, p2.data):\n            return False\n    return True\n\nif compare_models(node2vec, new_node2vec):\n    print(\"The saved and loaded models are identical\")\nelse:\n    print(\"The saved and loaded models are different\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T06:01:56.595033Z","iopub.execute_input":"2025-05-28T06:01:56.595373Z","iopub.status.idle":"2025-05-28T06:01:56.611879Z","shell.execute_reply.started":"2025-05-28T06:01:56.595348Z","shell.execute_reply":"2025-05-28T06:01:56.611264Z"}},"outputs":[{"name":"stdout","text":"The saved and loaded models are identical\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import HfApi, HfFolder, upload_file, create_repo, hf_hub_download","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T06:14:45.551169Z","iopub.execute_input":"2025-05-28T06:14:45.551813Z","iopub.status.idle":"2025-05-28T06:14:45.555312Z","shell.execute_reply.started":"2025-05-28T06:14:45.551790Z","shell.execute_reply":"2025-05-28T06:14:45.554631Z"}},"outputs":[],"execution_count":53},{"cell_type":"code","source":"from huggingface_hub import login","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T06:10:20.047717Z","iopub.execute_input":"2025-05-28T06:10:20.047985Z","iopub.status.idle":"2025-05-28T06:10:20.051853Z","shell.execute_reply.started":"2025-05-28T06:10:20.047966Z","shell.execute_reply":"2025-05-28T06:10:20.051157Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"hf_token = os.environ.get(\"HF_TOKEN\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T06:12:28.341949Z","iopub.execute_input":"2025-05-28T06:12:28.342500Z","iopub.status.idle":"2025-05-28T06:12:28.346048Z","shell.execute_reply.started":"2025-05-28T06:12:28.342479Z","shell.execute_reply":"2025-05-28T06:12:28.345140Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"login()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T06:12:30.554815Z","iopub.execute_input":"2025-05-28T06:12:30.555369Z","iopub.status.idle":"2025-05-28T06:12:30.569311Z","shell.execute_reply.started":"2025-05-28T06:12:30.555346Z","shell.execute_reply":"2025-05-28T06:12:30.568504Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a54b52d444474cdb815ac9c2aee86ad1"}},"metadata":{}}],"execution_count":49},{"cell_type":"code","source":"# # Create a new repo (if it doesn't already exist)\n# create_repo(\"RZoro/ogb_ddi-node2vec-model\", repo_type=\"model\", exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T06:13:32.879395Z","iopub.execute_input":"2025-05-28T06:13:32.880007Z","iopub.status.idle":"2025-05-28T06:13:32.882913Z","shell.execute_reply.started":"2025-05-28T06:13:32.879982Z","shell.execute_reply":"2025-05-28T06:13:32.882299Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"upload_file(\n    path_or_fileobj=\"/kaggle/working/training_outputs/node2vec_state_dict.pth\",\n    path_in_repo=\"node2vec_state_dict.pth\",\n    repo_id=\"Rzoro/ogb_ddi\",\n    repo_type=\"model\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T06:13:38.141322Z","iopub.execute_input":"2025-05-28T06:13:38.141629Z","iopub.status.idle":"2025-05-28T06:13:41.638942Z","shell.execute_reply.started":"2025-05-28T06:13:38.141609Z","shell.execute_reply":"2025-05-28T06:13:41.638329Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"node2vec_state_dict.pth:   0%|          | 0.00/4.37M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db17acac37e0496f9f679ec70fc2a120"}},"metadata":{}},{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/Rzoro/ogb_ddi/commit/9c9fc3fb347ba8834403ccf9015a25acb6e96527', commit_message='Upload node2vec_state_dict.pth with huggingface_hub', commit_description='', oid='9c9fc3fb347ba8834403ccf9015a25acb6e96527', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Rzoro/ogb_ddi', endpoint='https://huggingface.co', repo_type='model', repo_id='Rzoro/ogb_ddi'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":52},{"cell_type":"code","source":"file_path = hf_hub_download(\n    repo_id=\"Rzoro/ogb_ddi\",  # Replace with your model repo\n    filename=\"node2vec_state_dict.pth\",          # Replace with your model file\n    repo_type=\"model\"                         # Could be 'dataset' or 'space' too\n)\n\nprint(\"Downloaded to:\", file_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T06:15:55.568850Z","iopub.execute_input":"2025-05-28T06:15:55.569393Z","iopub.status.idle":"2025-05-28T06:15:57.061320Z","shell.execute_reply.started":"2025-05-28T06:15:55.569364Z","shell.execute_reply":"2025-05-28T06:15:57.060761Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"node2vec_state_dict.pth:   0%|          | 0.00/4.37M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e42f0730e3544cc693016772c0dd7aed"}},"metadata":{}},{"name":"stdout","text":"Downloaded to: /root/.cache/huggingface/hub/models--Rzoro--ogb_ddi/snapshots/9c9fc3fb347ba8834403ccf9015a25acb6e96527/node2vec_state_dict.pth\n","output_type":"stream"}],"execution_count":54},{"cell_type":"code","source":"new_node2vec2 = load_node2vec(node2vec_args, data_node2vec, filepath)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T06:17:54.040304Z","iopub.execute_input":"2025-05-28T06:17:54.041027Z","iopub.status.idle":"2025-05-28T06:17:54.138220Z","shell.execute_reply.started":"2025-05-28T06:17:54.041001Z","shell.execute_reply":"2025-05-28T06:17:54.137374Z"}},"outputs":[],"execution_count":55},{"cell_type":"code","source":"new_node2vec2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T06:18:06.384606Z","iopub.execute_input":"2025-05-28T06:18:06.385268Z","iopub.status.idle":"2025-05-28T06:18:06.389376Z","shell.execute_reply.started":"2025-05-28T06:18:06.385246Z","shell.execute_reply":"2025-05-28T06:18:06.388804Z"}},"outputs":[{"execution_count":57,"output_type":"execute_result","data":{"text/plain":"Node2Vec(4267, 256)"},"metadata":{}}],"execution_count":57},{"cell_type":"code","source":"if compare_models(node2vec, new_node2vec2):\n    print(\"The saved and loaded models are identical\")\nelse:\n    print(\"The saved and loaded models are different\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T06:18:01.593927Z","iopub.execute_input":"2025-05-28T06:18:01.594641Z","iopub.status.idle":"2025-05-28T06:18:01.598834Z","shell.execute_reply.started":"2025-05-28T06:18:01.594617Z","shell.execute_reply":"2025-05-28T06:18:01.598146Z"}},"outputs":[{"name":"stdout","text":"The saved and loaded models are identical\n","output_type":"stream"}],"execution_count":56},{"cell_type":"markdown","source":"## Train GCN model","metadata":{}},{"cell_type":"code","source":"# class using PyG's GCNConv layer\nclass GCN(torch.nn.Module):\n    ''' Define GCN network. '''\n    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n                 dropout):\n        super(GCN, self).__init__()\n\n        self.convs = torch.nn.ModuleList()\n        self.convs.append(GCNConv(in_channels, hidden_channels, cached=True))\n        for _ in range(num_layers - 2):\n            self.convs.append(\n                GCNConv(hidden_channels, hidden_channels, cached=True))\n        self.convs.append(GCNConv(hidden_channels, out_channels, cached=True))\n\n        self.dropout = dropout\n\n    def reset_parameters(self):\n        for conv in self.convs:\n            conv.reset_parameters()\n\n    def forward(self, x, adj_t):\n        # Execute conv -> relu -> dropout sequence\n        for conv in self.convs[:-1]:\n            x = conv(x, adj_t)\n            x = F.relu(x)\n            x = F.dropout(x, p=self.dropout, training=self.training)\n        x = self.convs[-1](x, adj_t)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T06:19:57.054970Z","iopub.execute_input":"2025-05-28T06:19:57.055755Z","iopub.status.idle":"2025-05-28T06:19:57.064425Z","shell.execute_reply.started":"2025-05-28T06:19:57.055732Z","shell.execute_reply":"2025-05-28T06:19:57.063761Z"}},"outputs":[],"execution_count":58},{"cell_type":"code","source":"# class in order to predict whether a link exists between two nodes using\n# their embeddings, x_i and x_j\nclass LinkPredictor(torch.nn.Module):\n    ''' Neural network which predicts whether a link (interaction) exists between 2 nodes i,j\n    given their embeddings x_i, x_j.\n    '''\n    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n                 dropout):\n        super(LinkPredictor, self).__init__()\n\n        self.lins = torch.nn.ModuleList()\n        self.lins.append(torch.nn.Linear(in_channels, hidden_channels))\n        for _ in range(num_layers - 2):\n            self.lins.append(torch.nn.Linear(hidden_channels, hidden_channels))\n        self.lins.append(torch.nn.Linear(hidden_channels, out_channels))\n\n        self.dropout = dropout\n\n    def reset_parameters(self):\n        for lin in self.lins:\n            lin.reset_parameters()\n\n    def forward(self, x_i, x_j):\n        x = x_i * x_j # hadamard product\n        for lin in self.lins[:-1]: # linear layer -> relu -> dropout\n            x = lin(x)\n            x = F.relu(x)\n            x = F.dropout(x, p=self.dropout, training=self.training)\n        x = self.lins[-1](x)\n        return torch.sigmoid(x) # sigmoid activation outputs probability that a given edge exists for all node pairs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T06:20:16.731768Z","iopub.execute_input":"2025-05-28T06:20:16.732501Z","iopub.status.idle":"2025-05-28T06:20:16.738444Z","shell.execute_reply.started":"2025-05-28T06:20:16.732474Z","shell.execute_reply":"2025-05-28T06:20:16.737632Z"}},"outputs":[],"execution_count":59},{"cell_type":"code","source":"def train(model, predictor, x, adj_t, split_edge, optimizer, batch_size):\n\n    row, col, _ = adj_t.coo()\n    edge_index = torch.stack([col, row], dim=0)\n\n    model.train()\n    predictor.train()\n\n    pos_train_edge = split_edge['train']['edge'].to(x.device)\n\n    total_loss = total_examples = 0\n    for perm in DataLoader(range(pos_train_edge.size(0)), batch_size,\n                           shuffle=True):\n        optimizer.zero_grad()\n\n        h = model(x, adj_t)\n\n        edge = pos_train_edge[perm].t()\n\n        # computes the loss for positive edges\n        pos_out = predictor(h[edge[0]], h[edge[1]])\n        pos_loss = -torch.log(pos_out + 1e-15).mean()\n\n        # samples negative edges from the graph\n        edge = negative_sampling(edge_index, num_nodes=x.size(0),\n                                 num_neg_samples=perm.size(0), method='dense')\n\n        # computes the loss for negative edges\n        neg_out = predictor(h[edge[0]], h[edge[1]])\n        neg_loss = -torch.log(1 - neg_out + 1e-15).mean()\n\n        loss = pos_loss + neg_loss\n        loss.backward()\n\n        torch.nn.utils.clip_grad_norm_(x, 1.0)\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        torch.nn.utils.clip_grad_norm_(predictor.parameters(), 1.0)\n\n        optimizer.step()\n\n        num_examples = pos_out.size(0)\n        total_loss += loss.item() * num_examples\n        total_examples += num_examples\n\n    return total_loss / total_examples\n\n\n@torch.no_grad()\ndef test(model, predictor, x, adj_t, split_edge, evaluator, batch_size):\n    model.eval()\n    predictor.eval()\n\n    h = model(x, adj_t)\n\n    pos_train_edge = split_edge['eval_train']['edge'].to(x.device)\n    pos_valid_edge = split_edge['valid']['edge'].to(x.device)\n    neg_valid_edge = split_edge['valid']['edge_neg'].to(x.device)\n    pos_test_edge = split_edge['test']['edge'].to(x.device)\n    neg_test_edge = split_edge['test']['edge_neg'].to(x.device)\n\n    # store what the link predictor outputs for each positive and negative\n    # edge in order to compute the hits@K\n    pos_train_preds = []\n    for perm in DataLoader(range(pos_train_edge.size(0)), batch_size):\n        edge = pos_train_edge[perm].t()\n        pos_train_preds += [predictor(h[edge[0]], h[edge[1]]).squeeze().cpu()]\n    pos_train_pred = torch.cat(pos_train_preds, dim=0)\n\n    pos_valid_preds = []\n    for perm in DataLoader(range(pos_valid_edge.size(0)), batch_size):\n        edge = pos_valid_edge[perm].t()\n        pos_valid_preds += [predictor(h[edge[0]], h[edge[1]]).squeeze().cpu()]\n    pos_valid_pred = torch.cat(pos_valid_preds, dim=0)\n\n    neg_valid_preds = []\n    for perm in DataLoader(range(neg_valid_edge.size(0)), batch_size):\n        edge = neg_valid_edge[perm].t()\n        neg_valid_preds += [predictor(h[edge[0]], h[edge[1]]).squeeze().cpu()]\n    neg_valid_pred = torch.cat(neg_valid_preds, dim=0)\n\n    pos_test_preds = []\n    for perm in DataLoader(range(pos_test_edge.size(0)), batch_size):\n        edge = pos_test_edge[perm].t()\n        pos_test_preds += [predictor(h[edge[0]], h[edge[1]]).squeeze().cpu()]\n    pos_test_pred = torch.cat(pos_test_preds, dim=0)\n\n    neg_test_preds = []\n    for perm in DataLoader(range(neg_test_edge.size(0)), batch_size):\n        edge = neg_test_edge[perm].t()\n        neg_test_preds += [predictor(h[edge[0]], h[edge[1]]).squeeze().cpu()]\n    neg_test_pred = torch.cat(neg_test_preds, dim=0)\n\n    # compute the hits@K for training, validation, and test\n    results = {}\n    for K in [10, 20]:\n        evaluator.K = K\n        train_hits = evaluator.eval({\n            'y_pred_pos': pos_train_pred,\n            'y_pred_neg': neg_valid_pred,\n        })[f'hits@{K}']\n        valid_hits = evaluator.eval({\n            'y_pred_pos': pos_valid_pred,\n            'y_pred_neg': neg_valid_pred,\n        })[f'hits@{K}']\n        test_hits = evaluator.eval({\n            'y_pred_pos': pos_test_pred,\n            'y_pred_neg': neg_test_pred,\n        })[f'hits@{K}']\n\n        results[f'Hits@{K}'] = (train_hits, valid_hits, test_hits)\n\n    return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T06:20:30.995692Z","iopub.execute_input":"2025-05-28T06:20:30.996283Z","iopub.status.idle":"2025-05-28T06:20:31.018569Z","shell.execute_reply.started":"2025-05-28T06:20:30.996259Z","shell.execute_reply":"2025-05-28T06:20:31.018007Z"}},"outputs":[],"execution_count":60},{"cell_type":"code","source":"gnn_args = { # define GNN hyperparams\n    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n    'hidden_size': 256,\n    'dropout': 0.5,\n    'epochs': 100,\n    'weight_decay': 1e-5,\n    'lr': 0.005,\n    'attn_size': 32,\n    'num_layers':2,\n    'log_steps':1,\n    'eval_steps':5,\n    'runs':10,\n    'batch_size': 1024,\n    'attn_heads': 1,\n\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T07:54:41.583451Z","iopub.execute_input":"2025-05-28T07:54:41.584167Z","iopub.status.idle":"2025-05-28T07:54:41.587928Z","shell.execute_reply.started":"2025-05-28T07:54:41.584141Z","shell.execute_reply":"2025-05-28T07:54:41.587264Z"}},"outputs":[],"execution_count":127},{"cell_type":"code","source":"pretrained_weight = new_node2vec2.embedding.weight.data.cpu()\n# node2vec_emb.weight.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T06:26:31.323553Z","iopub.execute_input":"2025-05-28T06:26:31.324145Z","iopub.status.idle":"2025-05-28T06:26:31.328936Z","shell.execute_reply.started":"2025-05-28T06:26:31.324107Z","shell.execute_reply":"2025-05-28T06:26:31.328048Z"}},"outputs":[],"execution_count":70},{"cell_type":"code","source":"dataset.data.num_nodes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T06:25:47.536907Z","iopub.execute_input":"2025-05-28T06:25:47.537461Z","iopub.status.idle":"2025-05-28T06:25:47.541835Z","shell.execute_reply.started":"2025-05-28T06:25:47.537439Z","shell.execute_reply":"2025-05-28T06:25:47.541265Z"}},"outputs":[{"execution_count":67,"output_type":"execute_result","data":{"text/plain":"4267"},"metadata":{}}],"execution_count":67},{"cell_type":"code","source":"# Initialize embedding matrix\nnode2vec_emb = torch.nn.Embedding(dataset.data.num_nodes, gnn_args['hidden_size']).to(device)\nnode2vec_emb.weight.data.cpu()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T06:28:52.702413Z","iopub.execute_input":"2025-05-28T06:28:52.702680Z","iopub.status.idle":"2025-05-28T06:28:52.722395Z","shell.execute_reply.started":"2025-05-28T06:28:52.702661Z","shell.execute_reply":"2025-05-28T06:28:52.721760Z"}},"outputs":[{"execution_count":80,"output_type":"execute_result","data":{"text/plain":"tensor([[-2.5577, -0.5522,  0.7761,  ...,  0.7560,  0.6740, -0.7607],\n        [-0.6029,  0.9182,  2.1672,  ..., -0.4306, -1.9059,  1.2720],\n        [-0.4639,  0.8636,  2.0954,  ..., -0.6557, -0.2586,  0.6770],\n        ...,\n        [-0.3463,  0.2970, -0.2306,  ..., -0.4084,  0.1816, -1.1687],\n        [-0.5405,  1.5223,  0.3612,  ...,  1.7321,  0.5031, -0.5614],\n        [ 0.7525,  2.0423, -0.0928,  ..., -0.1488,  0.3232, -0.3791]])"},"metadata":{}}],"execution_count":80},{"cell_type":"code","source":"# get embedding weights from node2vec model\nnode2vec_emb.weight.data.copy_(pretrained_weight)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T06:29:19.213806Z","iopub.execute_input":"2025-05-28T06:29:19.214072Z","iopub.status.idle":"2025-05-28T06:29:19.221666Z","shell.execute_reply.started":"2025-05-28T06:29:19.214053Z","shell.execute_reply":"2025-05-28T06:29:19.220849Z"}},"outputs":[{"execution_count":81,"output_type":"execute_result","data":{"text/plain":"tensor([[-8.9925e-01,  3.5224e-01, -1.6614e-01,  ...,  1.6531e-01,\n         -5.0935e-01,  2.8803e-01],\n        [ 9.0283e-02, -1.7072e-01, -8.8191e-02,  ...,  1.9789e-02,\n         -8.3763e-02,  2.4516e-02],\n        [-1.1388e-01, -3.9438e-02,  2.2235e-02,  ...,  1.4269e-01,\n         -2.7632e-01, -1.1623e-02],\n        ...,\n        [ 2.6147e-02,  1.0147e-01,  8.9643e-03,  ..., -2.5608e-04,\n          1.6373e-02, -7.6566e-02],\n        [-1.0164e-02,  5.5890e-02, -1.3590e-02,  ...,  8.7863e-03,\n         -8.5676e-02, -4.0881e-02],\n        [ 1.0617e-01, -4.3757e-02,  3.1394e-02,  ...,  2.7990e-02,\n          7.6857e-02, -3.9015e-02]], device='cuda:0')"},"metadata":{}}],"execution_count":81},{"cell_type":"markdown","source":"Basically we converted a node2vec object to embedding ","metadata":{}},{"cell_type":"code","source":"# comparing if two embeddings are same\ntorch.equal(node2vec_emb.weight.data.cpu(), pretrained_weight)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T06:30:51.833708Z","iopub.execute_input":"2025-05-28T06:30:51.834271Z","iopub.status.idle":"2025-05-28T06:30:51.840143Z","shell.execute_reply.started":"2025-05-28T06:30:51.834250Z","shell.execute_reply":"2025-05-28T06:30:51.839519Z"}},"outputs":[{"execution_count":85,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":85},{"cell_type":"code","source":"node2vec_emb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T06:31:04.005281Z","iopub.execute_input":"2025-05-28T06:31:04.005564Z","iopub.status.idle":"2025-05-28T06:31:04.010260Z","shell.execute_reply.started":"2025-05-28T06:31:04.005544Z","shell.execute_reply":"2025-05-28T06:31:04.009733Z"}},"outputs":[{"execution_count":86,"output_type":"execute_result","data":{"text/plain":"Embedding(4267, 256)"},"metadata":{}}],"execution_count":86},{"cell_type":"code","source":"new_node2vec2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T06:31:08.500318Z","iopub.execute_input":"2025-05-28T06:31:08.500551Z","iopub.status.idle":"2025-05-28T06:31:08.505183Z","shell.execute_reply.started":"2025-05-28T06:31:08.500535Z","shell.execute_reply":"2025-05-28T06:31:08.504634Z"}},"outputs":[{"execution_count":87,"output_type":"execute_result","data":{"text/plain":"Node2Vec(4267, 256)"},"metadata":{}}],"execution_count":87},{"cell_type":"code","source":"def train_model(model, emb, gnn_args, predictor, model_name):\n  '''\n  Train specified GNN model. Model and embeddings should be initialized.\n  Save model after every run.\n  '''\n  train_hits_arr, val_hits_arr, test_hits_arr = [], [], []\n\n  evaluator = Evaluator(name='ogbl-ddi')\n  for run in range(2):\n    max_valhits, train_hits_run, test_hits_run = float('-inf'), 0, 0\n\n    torch.nn.init.xavier_uniform_(emb.weight)\n    model.reset_parameters()\n    predictor.reset_parameters()\n    optimizer = torch.optim.Adam(\n        list(model.parameters()) + list(emb.parameters()) +\n        list(predictor.parameters()), lr=gnn_args['lr'])\n\n    for epoch in range(1, 1 + gnn_args['epochs']):\n        loss = train(model, predictor, emb.weight, adj_t, split_edge,\n                      optimizer, gnn_args['batch_size'])\n\n        if epoch % gnn_args['eval_steps'] == 0:\n            results = test(model, predictor, emb.weight, adj_t, split_edge,\n                            evaluator, gnn_args['batch_size'])\n\n\n            if epoch % gnn_args['log_steps'] == 0:\n                for key, result in results.items():\n                    train_hits, valid_hits, test_hits = result\n                    print(key)\n                    print(f'Run: {run + 1:02d}, '\n                          f'Epoch: {epoch:02d}, '\n                          f'Loss: {loss:.4f}, '\n                          f'Train: {100 * train_hits:.2f}%, '\n                          f'Valid: {100 * valid_hits:.2f}%, '\n                          f'Test: {100 * test_hits:.2f}%')\n                print('---')\n\n            # check val-hits@20\n            train_hits, valid_hits, test_hits = results['Hits@20']\n            if valid_hits >= max_valhits: # if validhits20 is higher than max, save ckpt\n              max_valhits = valid_hits\n              train_hits_run = train_hits\n              test_hits_run = test_hits\n              # Save model checkpoint for current run.\n              model_path = f\"training_outputs/{model_name}.pt\"\n              emb_path = f'training_outputs/{model_name}_init_emb.pt'\n              save_model_ckpt(model, emb, optimizer, predictor, loss, emb_path, model_path)\n    train_hits_arr.append(train_hits_run)\n    test_hits_arr.append(test_hits_run)\n    val_hits_arr.append(max_valhits)\n\n\n  # Print overall stats arrays for best model based on val hits@20\n  print(\"Val_hits@20: \", val_hits_arr)\n  print(\"Test_hits@20: \", test_hits_arr)\n  print(\"Train_hits@20: \", train_hits_arr)\n\n  # Print best model stats (based on val hits@20)\n  val_max = max(val_hits_arr)\n  print(\"Best model val hits@20: \", max(val_hits_arr))\n  max_idx = val_hits_arr.index(val_max)\n  print('Best model test hits@20: ', test_hits_arr[max_idx])\n  print('Best model train hits@20: ', val_hits_arr[max_idx])\n\n  # convert to numpy array\n  val_hits_arr = np.array(val_hits_arr)\n  test_hits_arr = np.array(test_hits_arr)\n  train_hits_arr = np.array(train_hits_arr)\n\n  # Print average stats + variance\n  print(f\"Average best train hits@20: {np.mean(train_hits_arr)}; var: {np.var(train_hits_arr)}\")\n  print(f\"Average best val hits@20: {np.mean(val_hits_arr)}; var: {np.var(val_hits_arr)}\")\n  print(f\"Average best test hits@20: {np.mean(test_hits_arr)}; var: {np.var(test_hits_arr)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T07:54:46.310635Z","iopub.execute_input":"2025-05-28T07:54:46.311481Z","iopub.status.idle":"2025-05-28T07:54:46.320333Z","shell.execute_reply.started":"2025-05-28T07:54:46.311442Z","shell.execute_reply":"2025-05-28T07:54:46.319661Z"}},"outputs":[],"execution_count":128},{"cell_type":"code","source":"def save_model_ckpt(model, emb, optimizer, predictor, loss, emb_path, model_path):\n  ''' Save model and embedding checkpoints. '''\n  EPOCH = 100\n  # Save model params\n  torch.save({\n            'epoch': EPOCH,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'predictor_state_dict': predictor.state_dict(),\n            'loss': loss,\n            }, model_path)\n  # Also save initial embedding (just in case)\n  torch.save(emb.weight.data.cpu(), emb_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T06:32:30.057071Z","iopub.execute_input":"2025-05-28T06:32:30.057770Z","iopub.status.idle":"2025-05-28T06:32:30.061990Z","shell.execute_reply.started":"2025-05-28T06:32:30.057746Z","shell.execute_reply":"2025-05-28T06:32:30.061286Z"}},"outputs":[],"execution_count":89},{"cell_type":"code","source":"def load_model_ckpt(curr_model, model_name, run):\n  ''' Load model checkpoint. '''\n  evaluator = Evaluator(name='ogbl-ddi')\n  model_path = f\"training_outputs/{model_name}.pt\"\n  emb_path = f'training_outputs/{model_name}_init_emb.pt'\n\n  # Load emb (init feature representations)\n  pretrained_weight = torch.load(emb_path, map_location='cpu').to(device)\n  pretrained_weight = pretrained_weight.cpu().data.numpy()\n  emb_after = torch.nn.Embedding(dataset.data.num_nodes, gnn_args['hidden_size']).to(device)\n  # Pretrained_weight is a numpy matrix of shape (num_embeddings, embedding_dim)\n  emb_after.weight.data.copy_(torch.from_numpy(pretrained_weight))\n\n  # Init optimizer and predictor objects\n  predictor = LinkPredictor(gnn_args['hidden_size'], gnn_args['hidden_size'], 1,\n                          gnn_args['num_layers'], gnn_args['dropout']).to(device)\n  optimizer = torch.optim.Adam(\n        list(curr_model.parameters()) + list(emb_after.parameters()) +\n        list(predictor.parameters()), lr=gnn_args['lr'])\n\n\n  # Load model, predictor, and optimizer params\n  checkpoint = torch.load(model_path)\n  curr_model.load_state_dict(checkpoint['model_state_dict'])\n  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n  predictor.load_state_dict(checkpoint['predictor_state_dict'])\n  epoch = checkpoint['epoch']\n  loss = checkpoint['loss']\n\n\n  # Save final embedding representation of all nodes\n  h = curr_model(emb_after.weight, adj_t)\n  final_emb_path = f'training_outputs/{model_name}_final_emb_{run}.pt'\n  torch.save(h, final_emb_path)\n\n  # Evaluate pretrained model\n  results = test(curr_model, predictor, emb_after.weight, adj_t, split_edge,\n                               evaluator, gnn_args['batch_size'])\n\n  # Print hits stats\n  for key, result in results.items():\n    print(key)\n    train_hits, valid_hits, test_hits = result\n    print(f'Train: {100 * train_hits:.2f}%, '\n                              f'Valid: {100 * valid_hits:.2f}%, '\n                              f'Test: {100 * test_hits:.2f}%')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T06:32:39.821266Z","iopub.execute_input":"2025-05-28T06:32:39.821820Z","iopub.status.idle":"2025-05-28T06:32:39.829045Z","shell.execute_reply.started":"2025-05-28T06:32:39.821795Z","shell.execute_reply":"2025-05-28T06:32:39.828381Z"}},"outputs":[],"execution_count":90},{"cell_type":"code","source":"# TRAIN gcn with random features\nmodel_name = 'gcn_rand_feat'\ngcn_model = GCN(gnn_args['hidden_size'], gnn_args['hidden_size'], gnn_args['hidden_size'],\n              gnn_args['num_layers'], gnn_args['dropout']).to(device)\npredictor = LinkPredictor(gnn_args['hidden_size'], gnn_args['hidden_size'], 1,\n                          gnn_args['num_layers'], gnn_args['dropout']).to(device)\ngcn_emb_rand = torch.nn.Embedding(dataset.data.num_nodes, gnn_args['hidden_size']).to(device)\ntrain_model(gcn_model, gcn_emb_rand, gnn_args, predictor, model_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T06:34:26.835976Z","iopub.execute_input":"2025-05-28T06:34:26.836549Z","iopub.status.idle":"2025-05-28T06:41:40.921921Z","shell.execute_reply.started":"2025-05-28T06:34:26.836529Z","shell.execute_reply":"2025-05-28T06:41:40.921159Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n  warnings.warn(out)\n","output_type":"stream"},{"name":"stdout","text":"Hits@10\nRun: 01, Epoch: 05, Loss: 0.6586, Train: 17.27%, Valid: 15.94%, Test: 12.54%\nHits@20\nRun: 01, Epoch: 05, Loss: 0.6586, Train: 18.90%, Valid: 17.70%, Test: 16.43%\n---\nHits@10\nRun: 01, Epoch: 10, Loss: 0.5010, Train: 3.37%, Valid: 2.72%, Test: 6.64%\nHits@20\nRun: 01, Epoch: 10, Loss: 0.5010, Train: 16.45%, Valid: 14.52%, Test: 12.31%\n---\nHits@10\nRun: 01, Epoch: 15, Loss: 0.4248, Train: 19.47%, Valid: 17.16%, Test: 6.63%\nHits@20\nRun: 01, Epoch: 15, Loss: 0.4248, Train: 23.85%, Valid: 21.25%, Test: 11.66%\n---\nHits@10\nRun: 01, Epoch: 20, Loss: 0.3845, Train: 16.97%, Valid: 14.67%, Test: 12.45%\nHits@20\nRun: 01, Epoch: 20, Loss: 0.3845, Train: 23.00%, Valid: 20.31%, Test: 17.31%\n---\nHits@10\nRun: 01, Epoch: 25, Loss: 0.3585, Train: 15.79%, Valid: 13.52%, Test: 8.45%\nHits@20\nRun: 01, Epoch: 25, Loss: 0.3585, Train: 24.29%, Valid: 21.24%, Test: 20.74%\n---\nHits@10\nRun: 01, Epoch: 30, Loss: 0.3416, Train: 19.82%, Valid: 17.13%, Test: 6.20%\nHits@20\nRun: 01, Epoch: 30, Loss: 0.3416, Train: 30.20%, Valid: 26.55%, Test: 15.88%\n---\nHits@10\nRun: 01, Epoch: 35, Loss: 0.3255, Train: 23.44%, Valid: 20.27%, Test: 5.72%\nHits@20\nRun: 01, Epoch: 35, Loss: 0.3255, Train: 29.68%, Valid: 25.99%, Test: 10.35%\n---\nHits@10\nRun: 01, Epoch: 40, Loss: 0.3115, Train: 32.97%, Valid: 28.96%, Test: 6.18%\nHits@20\nRun: 01, Epoch: 40, Loss: 0.3115, Train: 39.04%, Valid: 34.61%, Test: 15.00%\n---\nHits@10\nRun: 01, Epoch: 45, Loss: 0.3015, Train: 31.52%, Valid: 27.72%, Test: 8.23%\nHits@20\nRun: 01, Epoch: 45, Loss: 0.3015, Train: 37.08%, Valid: 32.65%, Test: 13.81%\n---\nHits@10\nRun: 01, Epoch: 50, Loss: 0.2880, Train: 15.67%, Valid: 13.28%, Test: 4.87%\nHits@20\nRun: 01, Epoch: 50, Loss: 0.2880, Train: 24.97%, Valid: 21.58%, Test: 9.13%\n---\nHits@10\nRun: 01, Epoch: 55, Loss: 0.2800, Train: 22.52%, Valid: 19.33%, Test: 7.57%\nHits@20\nRun: 01, Epoch: 55, Loss: 0.2800, Train: 30.00%, Valid: 26.06%, Test: 15.96%\n---\nHits@10\nRun: 01, Epoch: 60, Loss: 0.2727, Train: 25.43%, Valid: 22.12%, Test: 6.80%\nHits@20\nRun: 01, Epoch: 60, Loss: 0.2727, Train: 33.88%, Valid: 29.66%, Test: 10.48%\n---\nHits@10\nRun: 01, Epoch: 65, Loss: 0.2630, Train: 23.92%, Valid: 20.36%, Test: 6.11%\nHits@20\nRun: 01, Epoch: 65, Loss: 0.2630, Train: 29.58%, Valid: 25.58%, Test: 15.45%\n---\nHits@10\nRun: 01, Epoch: 70, Loss: 0.2586, Train: 27.47%, Valid: 23.60%, Test: 12.84%\nHits@20\nRun: 01, Epoch: 70, Loss: 0.2586, Train: 41.01%, Valid: 35.91%, Test: 29.36%\n---\nHits@10\nRun: 01, Epoch: 75, Loss: 0.2542, Train: 16.72%, Valid: 14.02%, Test: 7.59%\nHits@20\nRun: 01, Epoch: 75, Loss: 0.2542, Train: 26.31%, Valid: 22.57%, Test: 15.85%\n---\nHits@10\nRun: 01, Epoch: 80, Loss: 0.2488, Train: 19.52%, Valid: 16.51%, Test: 3.61%\nHits@20\nRun: 01, Epoch: 80, Loss: 0.2488, Train: 31.81%, Valid: 27.36%, Test: 18.16%\n---\nHits@10\nRun: 01, Epoch: 85, Loss: 0.2438, Train: 27.29%, Valid: 23.35%, Test: 5.65%\nHits@20\nRun: 01, Epoch: 85, Loss: 0.2438, Train: 38.76%, Valid: 33.72%, Test: 16.27%\n---\nHits@10\nRun: 01, Epoch: 90, Loss: 0.2407, Train: 23.96%, Valid: 20.32%, Test: 6.89%\nHits@20\nRun: 01, Epoch: 90, Loss: 0.2407, Train: 30.29%, Valid: 25.97%, Test: 19.15%\n---\nHits@10\nRun: 01, Epoch: 95, Loss: 0.2363, Train: 24.34%, Valid: 20.65%, Test: 12.58%\nHits@20\nRun: 01, Epoch: 95, Loss: 0.2363, Train: 36.56%, Valid: 31.48%, Test: 18.46%\n---\nHits@10\nRun: 01, Epoch: 100, Loss: 0.2316, Train: 33.06%, Valid: 28.06%, Test: 12.94%\nHits@20\nRun: 01, Epoch: 100, Loss: 0.2316, Train: 44.95%, Valid: 38.91%, Test: 30.21%\n---\nHits@10\nRun: 02, Epoch: 05, Loss: 0.6677, Train: 11.48%, Valid: 10.12%, Test: 4.20%\nHits@20\nRun: 02, Epoch: 05, Loss: 0.6677, Train: 15.72%, Valid: 14.21%, Test: 8.27%\n---\nHits@10\nRun: 02, Epoch: 10, Loss: 0.5090, Train: 11.49%, Valid: 10.01%, Test: 10.05%\nHits@20\nRun: 02, Epoch: 10, Loss: 0.5090, Train: 18.67%, Valid: 16.48%, Test: 13.28%\n---\nHits@10\nRun: 02, Epoch: 15, Loss: 0.4304, Train: 14.58%, Valid: 12.63%, Test: 8.04%\nHits@20\nRun: 02, Epoch: 15, Loss: 0.4304, Train: 24.30%, Valid: 21.80%, Test: 11.80%\n---\nHits@10\nRun: 02, Epoch: 20, Loss: 0.3949, Train: 23.92%, Valid: 21.19%, Test: 15.91%\nHits@20\nRun: 02, Epoch: 20, Loss: 0.3949, Train: 27.65%, Valid: 24.70%, Test: 22.13%\n---\nHits@10\nRun: 02, Epoch: 25, Loss: 0.3578, Train: 19.24%, Valid: 16.79%, Test: 9.91%\nHits@20\nRun: 02, Epoch: 25, Loss: 0.3578, Train: 26.55%, Valid: 23.37%, Test: 17.78%\n---\nHits@10\nRun: 02, Epoch: 30, Loss: 0.3345, Train: 24.54%, Valid: 21.67%, Test: 4.57%\nHits@20\nRun: 02, Epoch: 30, Loss: 0.3345, Train: 28.35%, Valid: 25.07%, Test: 14.18%\n---\nHits@10\nRun: 02, Epoch: 35, Loss: 0.3211, Train: 19.91%, Valid: 17.16%, Test: 7.18%\nHits@20\nRun: 02, Epoch: 35, Loss: 0.3211, Train: 27.94%, Valid: 24.28%, Test: 14.05%\n---\nHits@10\nRun: 02, Epoch: 40, Loss: 0.3078, Train: 13.33%, Valid: 11.34%, Test: 3.81%\nHits@20\nRun: 02, Epoch: 40, Loss: 0.3078, Train: 28.93%, Valid: 25.31%, Test: 8.33%\n---\nHits@10\nRun: 02, Epoch: 45, Loss: 0.2911, Train: 20.36%, Valid: 17.46%, Test: 3.98%\nHits@20\nRun: 02, Epoch: 45, Loss: 0.2911, Train: 27.97%, Valid: 24.44%, Test: 9.16%\n---\nHits@10\nRun: 02, Epoch: 50, Loss: 0.2785, Train: 21.98%, Valid: 18.86%, Test: 7.20%\nHits@20\nRun: 02, Epoch: 50, Loss: 0.2785, Train: 28.74%, Valid: 25.01%, Test: 11.32%\n---\nHits@10\nRun: 02, Epoch: 55, Loss: 0.2745, Train: 18.13%, Valid: 15.39%, Test: 3.53%\nHits@20\nRun: 02, Epoch: 55, Loss: 0.2745, Train: 32.59%, Valid: 28.27%, Test: 9.72%\n---\nHits@10\nRun: 02, Epoch: 60, Loss: 0.2672, Train: 21.09%, Valid: 18.19%, Test: 4.07%\nHits@20\nRun: 02, Epoch: 60, Loss: 0.2672, Train: 29.50%, Valid: 25.70%, Test: 9.66%\n---\nHits@10\nRun: 02, Epoch: 65, Loss: 0.2594, Train: 16.06%, Valid: 13.54%, Test: 5.29%\nHits@20\nRun: 02, Epoch: 65, Loss: 0.2594, Train: 28.08%, Valid: 24.10%, Test: 11.39%\n---\nHits@10\nRun: 02, Epoch: 70, Loss: 0.2565, Train: 25.53%, Valid: 21.96%, Test: 4.04%\nHits@20\nRun: 02, Epoch: 70, Loss: 0.2565, Train: 35.84%, Valid: 31.15%, Test: 11.82%\n---\nHits@10\nRun: 02, Epoch: 75, Loss: 0.2494, Train: 26.15%, Valid: 22.46%, Test: 5.08%\nHits@20\nRun: 02, Epoch: 75, Loss: 0.2494, Train: 34.60%, Valid: 29.96%, Test: 12.96%\n---\nHits@10\nRun: 02, Epoch: 80, Loss: 0.2462, Train: 28.80%, Valid: 24.84%, Test: 17.24%\nHits@20\nRun: 02, Epoch: 80, Loss: 0.2462, Train: 36.27%, Valid: 31.46%, Test: 20.45%\n---\nHits@10\nRun: 02, Epoch: 85, Loss: 0.2428, Train: 21.47%, Valid: 18.14%, Test: 11.15%\nHits@20\nRun: 02, Epoch: 85, Loss: 0.2428, Train: 32.36%, Valid: 27.86%, Test: 22.54%\n---\nHits@10\nRun: 02, Epoch: 90, Loss: 0.2351, Train: 26.76%, Valid: 22.82%, Test: 16.05%\nHits@20\nRun: 02, Epoch: 90, Loss: 0.2351, Train: 42.76%, Valid: 36.95%, Test: 26.11%\n---\nHits@10\nRun: 02, Epoch: 95, Loss: 0.2312, Train: 34.38%, Valid: 29.48%, Test: 15.37%\nHits@20\nRun: 02, Epoch: 95, Loss: 0.2312, Train: 51.61%, Valid: 45.05%, Test: 30.93%\n---\nHits@10\nRun: 02, Epoch: 100, Loss: 0.2293, Train: 30.63%, Valid: 26.22%, Test: 15.85%\nHits@20\nRun: 02, Epoch: 100, Loss: 0.2293, Train: 45.04%, Valid: 38.95%, Test: 27.39%\n---\nVal_hits@20:  [0.3890507832106016, 0.450516521960611]\nTest_hits@20:  [0.30211478099319045, 0.30933634981159497]\nTrain_hits@20:  [0.4494902201679539, 0.5160724853733266]\nBest model val hits@20:  0.450516521960611\nBest model test hits@20:  0.30933634981159497\nBest model train hits@20:  0.450516521960611\nAverage best train hits@20: 0.48278135277064027; var: 0.0011082995099696458\nAverage best val hits@20: 0.4197836525856063; var: 0.0009445092600211012\nAverage best test hits@20: 0.3057255654023927; var: 1.3037764049738123e-05\n","output_type":"stream"}],"execution_count":91},{"cell_type":"code","source":"# Loading pretrained GCN (random weight init)\ngcn_model = GCN(gnn_args['hidden_size'], gnn_args['hidden_size'], gnn_args['hidden_size'],\n             gnn_args['num_layers'], gnn_args['dropout']).to(device)\nload_model_ckpt(gcn_model, model_name,0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T06:43:56.434526Z","iopub.execute_input":"2025-05-28T06:43:56.435152Z","iopub.status.idle":"2025-05-28T06:43:56.787511Z","shell.execute_reply.started":"2025-05-28T06:43:56.435111Z","shell.execute_reply":"2025-05-28T06:43:56.786744Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n  warnings.warn(msg)\n/usr/local/lib/python3.11/dist-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n  warnings.warn(out)\n","output_type":"stream"},{"name":"stdout","text":"Hits@10\nTrain: 34.38%, Valid: 29.48%, Test: 15.37%\nHits@20\nTrain: 51.61%, Valid: 45.05%, Test: 30.93%\n","output_type":"stream"}],"execution_count":94},{"cell_type":"code","source":"upload_file(\n    path_or_fileobj=\"/kaggle/working/training_outputs/gcn_rand_feat.pt\",\n    path_in_repo=\"gcn_rand_feat.pt\",\n    repo_id=\"Rzoro/ogb_ddi\",\n    repo_type=\"model\"\n)\nupload_file(\n    path_or_fileobj=\"/kaggle/working/training_outputs/gcn_rand_feat_init_emb.pt\",\n    path_in_repo=\"gcn_rand_feat_init_emb.pt\",\n    repo_id=\"Rzoro/ogb_ddi\",\n    repo_type=\"model\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T06:44:39.119361Z","iopub.execute_input":"2025-05-28T06:44:39.119968Z","iopub.status.idle":"2025-05-28T06:44:44.496251Z","shell.execute_reply.started":"2025-05-28T06:44:39.119947Z","shell.execute_reply":"2025-05-28T06:44:44.495599Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"gcn_rand_feat.pt:   0%|          | 0.00/11.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ffa5af801dca4adfb8bce66a8e30954d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"gcn_rand_feat_init_emb.pt:   0%|          | 0.00/4.37M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b03bea510679443c81f53548aad385d4"}},"metadata":{}},{"execution_count":95,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/Rzoro/ogb_ddi/commit/33d8bbbc71a33eabe6a5212d5c58519f5e192eba', commit_message='Upload gcn_rand_feat_init_emb.pt with huggingface_hub', commit_description='', oid='33d8bbbc71a33eabe6a5212d5c58519f5e192eba', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Rzoro/ogb_ddi', endpoint='https://huggingface.co', repo_type='model', repo_id='Rzoro/ogb_ddi'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":95},{"cell_type":"code","source":"upload_file(\n    path_or_fileobj=\"/kaggle/working/training_outputs/gcn_rand_feat_final_emb_0.pt\",\n    path_in_repo=\"gcn_rand_feat_final_emb_0.pt\",\n    repo_id=\"Rzoro/ogb_ddi\",\n    repo_type=\"model\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T06:56:52.515542Z","iopub.execute_input":"2025-05-28T06:56:52.515897Z","iopub.status.idle":"2025-05-28T06:56:55.877755Z","shell.execute_reply.started":"2025-05-28T06:56:52.515856Z","shell.execute_reply":"2025-05-28T06:56:55.876934Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"gcn_rand_feat_final_emb_0.pt:   0%|          | 0.00/4.37M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38a69ae30fb74752ada907f3af1c2917"}},"metadata":{}},{"execution_count":98,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/Rzoro/ogb_ddi/commit/70ee025d2758e3c6b4ebe7f807d4febb3a0ddc88', commit_message='Upload gcn_rand_feat_final_emb_0.pt with huggingface_hub', commit_description='', oid='70ee025d2758e3c6b4ebe7f807d4febb3a0ddc88', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Rzoro/ogb_ddi', endpoint='https://huggingface.co', repo_type='model', repo_id='Rzoro/ogb_ddi'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":98},{"cell_type":"code","source":"# Train GCN with Node2Vec features\nmodel_name = 'gcn_node2vec_feat_256'\ngcn_model2 = GCN(gnn_args['hidden_size'], gnn_args['hidden_size'], gnn_args['hidden_size'],\n             gnn_args['num_layers'], gnn_args['dropout']).to(device)\n\ngcn_predictor2 = LinkPredictor(gnn_args['hidden_size'], gnn_args['hidden_size'], 1,\n                          gnn_args['num_layers'], gnn_args['dropout']).to(device)\ngcn_emb_node2vec = torch.nn.Embedding(dataset.data.num_nodes, gnn_args['hidden_size']).to(device)\ngcn_emb_node2vec.weight.data.copy_(pretrained_weight)\ntrain_model(gcn_model2, gcn_emb_node2vec, gnn_args, gcn_predictor2, model_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T06:47:22.373916Z","iopub.execute_input":"2025-05-28T06:47:22.374234Z","iopub.status.idle":"2025-05-28T06:54:36.651143Z","shell.execute_reply.started":"2025-05-28T06:47:22.374212Z","shell.execute_reply":"2025-05-28T06:54:36.650333Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n  warnings.warn(out)\n","output_type":"stream"},{"name":"stdout","text":"Hits@10\nRun: 01, Epoch: 05, Loss: 0.6928, Train: 3.43%, Valid: 2.94%, Test: 0.90%\nHits@20\nRun: 01, Epoch: 05, Loss: 0.6928, Train: 5.28%, Valid: 4.58%, Test: 2.58%\n---\nHits@10\nRun: 01, Epoch: 10, Loss: 0.5178, Train: 18.36%, Valid: 16.35%, Test: 12.10%\nHits@20\nRun: 01, Epoch: 10, Loss: 0.5178, Train: 23.31%, Valid: 20.89%, Test: 14.44%\n---\nHits@10\nRun: 01, Epoch: 15, Loss: 0.4410, Train: 10.30%, Valid: 8.80%, Test: 11.78%\nHits@20\nRun: 01, Epoch: 15, Loss: 0.4410, Train: 20.63%, Valid: 18.38%, Test: 14.75%\n---\nHits@10\nRun: 01, Epoch: 20, Loss: 0.3777, Train: 18.04%, Valid: 15.68%, Test: 10.52%\nHits@20\nRun: 01, Epoch: 20, Loss: 0.3777, Train: 28.09%, Valid: 24.82%, Test: 18.29%\n---\nHits@10\nRun: 01, Epoch: 25, Loss: 0.3481, Train: 26.46%, Valid: 23.33%, Test: 8.78%\nHits@20\nRun: 01, Epoch: 25, Loss: 0.3481, Train: 36.67%, Valid: 32.69%, Test: 14.79%\n---\nHits@10\nRun: 01, Epoch: 30, Loss: 0.3320, Train: 13.74%, Valid: 11.71%, Test: 5.94%\nHits@20\nRun: 01, Epoch: 30, Loss: 0.3320, Train: 21.19%, Valid: 18.31%, Test: 12.29%\n---\nHits@10\nRun: 01, Epoch: 35, Loss: 0.3052, Train: 16.68%, Valid: 14.04%, Test: 2.85%\nHits@20\nRun: 01, Epoch: 35, Loss: 0.3052, Train: 27.21%, Valid: 23.51%, Test: 10.68%\n---\nHits@10\nRun: 01, Epoch: 40, Loss: 0.2939, Train: 13.10%, Valid: 11.08%, Test: 5.43%\nHits@20\nRun: 01, Epoch: 40, Loss: 0.2939, Train: 29.91%, Valid: 26.08%, Test: 10.52%\n---\nHits@10\nRun: 01, Epoch: 45, Loss: 0.2788, Train: 16.90%, Valid: 14.35%, Test: 4.79%\nHits@20\nRun: 01, Epoch: 45, Loss: 0.2788, Train: 28.37%, Valid: 24.43%, Test: 16.26%\n---\nHits@10\nRun: 01, Epoch: 50, Loss: 0.2715, Train: 28.05%, Valid: 24.16%, Test: 9.04%\nHits@20\nRun: 01, Epoch: 50, Loss: 0.2715, Train: 37.62%, Valid: 32.64%, Test: 17.97%\n---\nHits@10\nRun: 01, Epoch: 55, Loss: 0.2621, Train: 27.35%, Valid: 23.61%, Test: 8.51%\nHits@20\nRun: 01, Epoch: 55, Loss: 0.2621, Train: 35.88%, Valid: 31.23%, Test: 13.86%\n---\nHits@10\nRun: 01, Epoch: 60, Loss: 0.2541, Train: 21.49%, Valid: 18.18%, Test: 9.50%\nHits@20\nRun: 01, Epoch: 60, Loss: 0.2541, Train: 37.92%, Valid: 32.95%, Test: 20.65%\n---\nHits@10\nRun: 01, Epoch: 65, Loss: 0.2489, Train: 23.05%, Valid: 19.63%, Test: 9.98%\nHits@20\nRun: 01, Epoch: 65, Loss: 0.2489, Train: 33.15%, Valid: 28.29%, Test: 21.03%\n---\nHits@10\nRun: 01, Epoch: 70, Loss: 0.2471, Train: 26.88%, Valid: 22.97%, Test: 13.47%\nHits@20\nRun: 01, Epoch: 70, Loss: 0.2471, Train: 34.11%, Valid: 29.30%, Test: 19.73%\n---\nHits@10\nRun: 01, Epoch: 75, Loss: 0.2377, Train: 22.66%, Valid: 19.07%, Test: 14.15%\nHits@20\nRun: 01, Epoch: 75, Loss: 0.2377, Train: 29.49%, Valid: 25.10%, Test: 23.88%\n---\nHits@10\nRun: 01, Epoch: 80, Loss: 0.2368, Train: 26.31%, Valid: 22.33%, Test: 16.87%\nHits@20\nRun: 01, Epoch: 80, Loss: 0.2368, Train: 35.99%, Valid: 30.99%, Test: 22.88%\n---\nHits@10\nRun: 01, Epoch: 85, Loss: 0.2339, Train: 31.12%, Valid: 26.69%, Test: 16.91%\nHits@20\nRun: 01, Epoch: 85, Loss: 0.2339, Train: 51.21%, Valid: 44.87%, Test: 27.73%\n---\nHits@10\nRun: 01, Epoch: 90, Loss: 0.2288, Train: 26.70%, Valid: 22.56%, Test: 19.33%\nHits@20\nRun: 01, Epoch: 90, Loss: 0.2288, Train: 36.67%, Valid: 31.33%, Test: 31.80%\n---\nHits@10\nRun: 01, Epoch: 95, Loss: 0.2313, Train: 21.34%, Valid: 17.88%, Test: 13.03%\nHits@20\nRun: 01, Epoch: 95, Loss: 0.2313, Train: 33.24%, Valid: 28.45%, Test: 19.97%\n---\nHits@10\nRun: 01, Epoch: 100, Loss: 0.2226, Train: 22.22%, Valid: 18.57%, Test: 15.64%\nHits@20\nRun: 01, Epoch: 100, Loss: 0.2226, Train: 38.60%, Valid: 32.95%, Test: 31.21%\n---\nHits@10\nRun: 02, Epoch: 05, Loss: 0.6723, Train: 11.61%, Valid: 10.21%, Test: 5.12%\nHits@20\nRun: 02, Epoch: 05, Loss: 0.6723, Train: 13.81%, Valid: 12.26%, Test: 7.82%\n---\nHits@10\nRun: 02, Epoch: 10, Loss: 0.5227, Train: 15.92%, Valid: 14.13%, Test: 9.11%\nHits@20\nRun: 02, Epoch: 10, Loss: 0.5227, Train: 22.46%, Valid: 20.22%, Test: 14.70%\n---\nHits@10\nRun: 02, Epoch: 15, Loss: 0.4465, Train: 3.98%, Valid: 3.53%, Test: 5.45%\nHits@20\nRun: 02, Epoch: 15, Loss: 0.4465, Train: 15.31%, Valid: 13.31%, Test: 7.92%\n---\nHits@10\nRun: 02, Epoch: 20, Loss: 0.3820, Train: 23.47%, Valid: 20.90%, Test: 6.63%\nHits@20\nRun: 02, Epoch: 20, Loss: 0.3820, Train: 32.44%, Valid: 29.06%, Test: 17.68%\n---\nHits@10\nRun: 02, Epoch: 25, Loss: 0.3533, Train: 20.76%, Valid: 18.13%, Test: 5.27%\nHits@20\nRun: 02, Epoch: 25, Loss: 0.3533, Train: 26.81%, Valid: 23.70%, Test: 19.61%\n---\nHits@10\nRun: 02, Epoch: 30, Loss: 0.3349, Train: 18.82%, Valid: 16.35%, Test: 7.56%\nHits@20\nRun: 02, Epoch: 30, Loss: 0.3349, Train: 21.95%, Valid: 19.22%, Test: 18.23%\n---\nHits@10\nRun: 02, Epoch: 35, Loss: 0.3199, Train: 14.90%, Valid: 12.79%, Test: 3.99%\nHits@20\nRun: 02, Epoch: 35, Loss: 0.3199, Train: 22.02%, Valid: 19.19%, Test: 16.40%\n---\nHits@10\nRun: 02, Epoch: 40, Loss: 0.3053, Train: 17.61%, Valid: 15.19%, Test: 5.40%\nHits@20\nRun: 02, Epoch: 40, Loss: 0.3053, Train: 27.81%, Valid: 24.38%, Test: 13.55%\n---\nHits@10\nRun: 02, Epoch: 45, Loss: 0.2951, Train: 18.44%, Valid: 15.81%, Test: 6.24%\nHits@20\nRun: 02, Epoch: 45, Loss: 0.2951, Train: 26.89%, Valid: 23.34%, Test: 11.17%\n---\nHits@10\nRun: 02, Epoch: 50, Loss: 0.2865, Train: 20.70%, Valid: 17.86%, Test: 10.19%\nHits@20\nRun: 02, Epoch: 50, Loss: 0.2865, Train: 36.30%, Valid: 31.86%, Test: 16.15%\n---\nHits@10\nRun: 02, Epoch: 55, Loss: 0.2769, Train: 22.62%, Valid: 19.39%, Test: 7.19%\nHits@20\nRun: 02, Epoch: 55, Loss: 0.2769, Train: 31.47%, Valid: 27.28%, Test: 17.53%\n---\nHits@10\nRun: 02, Epoch: 60, Loss: 0.2674, Train: 18.92%, Valid: 16.02%, Test: 6.88%\nHits@20\nRun: 02, Epoch: 60, Loss: 0.2674, Train: 31.50%, Valid: 27.15%, Test: 12.99%\n---\nHits@10\nRun: 02, Epoch: 65, Loss: 0.2637, Train: 28.82%, Valid: 24.84%, Test: 6.77%\nHits@20\nRun: 02, Epoch: 65, Loss: 0.2637, Train: 36.77%, Valid: 31.84%, Test: 16.38%\n---\nHits@10\nRun: 02, Epoch: 70, Loss: 0.2588, Train: 20.84%, Valid: 17.84%, Test: 3.79%\nHits@20\nRun: 02, Epoch: 70, Loss: 0.2588, Train: 27.93%, Valid: 24.11%, Test: 14.12%\n---\nHits@10\nRun: 02, Epoch: 75, Loss: 0.2521, Train: 25.42%, Valid: 21.84%, Test: 6.51%\nHits@20\nRun: 02, Epoch: 75, Loss: 0.2521, Train: 40.54%, Valid: 35.40%, Test: 11.26%\n---\nHits@10\nRun: 02, Epoch: 80, Loss: 0.2491, Train: 25.71%, Valid: 21.94%, Test: 11.50%\nHits@20\nRun: 02, Epoch: 80, Loss: 0.2491, Train: 33.47%, Valid: 28.86%, Test: 19.96%\n---\nHits@10\nRun: 02, Epoch: 85, Loss: 0.2451, Train: 25.00%, Valid: 21.36%, Test: 7.70%\nHits@20\nRun: 02, Epoch: 85, Loss: 0.2451, Train: 38.02%, Valid: 32.94%, Test: 15.80%\n---\nHits@10\nRun: 02, Epoch: 90, Loss: 0.2425, Train: 20.45%, Valid: 17.17%, Test: 11.96%\nHits@20\nRun: 02, Epoch: 90, Loss: 0.2425, Train: 30.11%, Valid: 25.98%, Test: 19.87%\n---\nHits@10\nRun: 02, Epoch: 95, Loss: 0.2388, Train: 30.07%, Valid: 25.66%, Test: 13.88%\nHits@20\nRun: 02, Epoch: 95, Loss: 0.2388, Train: 40.23%, Valid: 34.66%, Test: 27.07%\n---\nHits@10\nRun: 02, Epoch: 100, Loss: 0.2355, Train: 29.12%, Valid: 24.73%, Test: 25.40%\nHits@20\nRun: 02, Epoch: 100, Loss: 0.2355, Train: 44.92%, Valid: 38.92%, Test: 35.20%\n---\nVal_hits@20:  [0.4487336035178929, 0.3891556607660556]\nTest_hits@20:  [0.2773037478743567, 0.351991549865532]\nTrain_hits@20:  [0.5121021207739963, 0.4491680962476309]\nBest model val hits@20:  0.4487336035178929\nBest model test hits@20:  0.2773037478743567\nBest model train hits@20:  0.4487336035178929\nAverage best train hits@20: 0.4806351085108136; var: 0.0009901728607712914\nAverage best val hits@20: 0.41894463214197425; var: 0.0008873828156352992\nAverage best test hits@20: 0.31464764886994434; var: 0.0013945669415682525\n","output_type":"stream"}],"execution_count":96},{"cell_type":"code","source":"# Loading GCN trained on Node2Vec Features\ngcn_model2 = GCN(gnn_args['hidden_size'], gnn_args['hidden_size'], gnn_args['hidden_size'],\n             gnn_args['num_layers'], gnn_args['dropout']).to(device)\nload_model_ckpt(gcn_model, model_name,0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T06:55:25.047503Z","iopub.execute_input":"2025-05-28T06:55:25.048198Z","iopub.status.idle":"2025-05-28T06:55:25.378598Z","shell.execute_reply.started":"2025-05-28T06:55:25.048176Z","shell.execute_reply":"2025-05-28T06:55:25.377788Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n  warnings.warn(msg)\n/usr/local/lib/python3.11/dist-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n  warnings.warn(out)\n","output_type":"stream"},{"name":"stdout","text":"Hits@10\nTrain: 29.12%, Valid: 24.73%, Test: 25.40%\nHits@20\nTrain: 44.92%, Valid: 38.92%, Test: 35.20%\n","output_type":"stream"}],"execution_count":97},{"cell_type":"code","source":"upload_file(\n    path_or_fileobj=\"/kaggle/working/training_outputs/gcn_node2vec_feat_256.pt\",\n    path_in_repo=\"gcn_node2vec_feat_256.pt\",\n    repo_id=\"Rzoro/ogb_ddi\",\n    repo_type=\"model\"\n)\nupload_file(\n    path_or_fileobj=\"/kaggle/working/training_outputs/gcn_node2vec_feat_256_final_emb_0.pt\",\n    path_in_repo=\"gcn_node2vec_feat_256_final_emb_0.pt\",\n    repo_id=\"Rzoro/ogb_ddi\",\n    repo_type=\"model\"\n)\nupload_file(\n    path_or_fileobj=\"/kaggle/working/training_outputs/gcn_node2vec_feat_256_init_emb.pt\",\n    path_in_repo=\"gcn_node2vec_feat_256_init_emb.pt\",\n    repo_id=\"Rzoro/ogb_ddi\",\n    repo_type=\"model\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T06:57:31.432907Z","iopub.execute_input":"2025-05-28T06:57:31.433733Z","iopub.status.idle":"2025-05-28T06:57:38.996933Z","shell.execute_reply.started":"2025-05-28T06:57:31.433695Z","shell.execute_reply":"2025-05-28T06:57:38.996320Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"gcn_node2vec_feat_256.pt:   0%|          | 0.00/11.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ad4b674b7ff446596fa8145ac65b9bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"gcn_node2vec_feat_256_final_emb_0.pt:   0%|          | 0.00/4.37M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1fb2e5f43ace481d8e2f5552cc90acd8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"gcn_node2vec_feat_256_init_emb.pt:   0%|          | 0.00/4.37M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07cf34479451495eb9e57288757db578"}},"metadata":{}},{"execution_count":99,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/Rzoro/ogb_ddi/commit/415af6201e75ccbc5a176cee7400353399be0017', commit_message='Upload gcn_node2vec_feat_256_init_emb.pt with huggingface_hub', commit_description='', oid='415af6201e75ccbc5a176cee7400353399be0017', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Rzoro/ogb_ddi', endpoint='https://huggingface.co', repo_type='model', repo_id='Rzoro/ogb_ddi'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":99},{"cell_type":"code","source":"# class using PyG's SAGEConv layer\nclass SAGE(torch.nn.Module):\n    ''' Define GCN network. '''\n    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n                 dropout):\n        super(SAGE, self).__init__()\n\n        self.convs = torch.nn.ModuleList()\n        self.convs.append(SAGEConv(in_channels, hidden_channels))\n        for _ in range(num_layers - 2):\n            self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n        self.convs.append(SAGEConv(hidden_channels, out_channels))\n\n        self.dropout = dropout\n\n    def reset_parameters(self):\n        for conv in self.convs:\n            conv.reset_parameters()\n\n    def forward(self, x, adj_t):\n      # Execute conv -> relu -> dropout sequence\n        for conv in self.convs[:-1]:\n            x = conv(x, adj_t)\n            x = F.relu(x)\n            x = F.dropout(x, p=self.dropout, training=self.training)\n        x = self.convs[-1](x, adj_t)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T07:10:23.796658Z","iopub.execute_input":"2025-05-28T07:10:23.797176Z","iopub.status.idle":"2025-05-28T07:10:23.802757Z","shell.execute_reply.started":"2025-05-28T07:10:23.797153Z","shell.execute_reply":"2025-05-28T07:10:23.801931Z"}},"outputs":[],"execution_count":107},{"cell_type":"code","source":"# TRAIN GRAPHSAGE with random features\nmodel_name = 'sage_rand_feat'\nsage_model = SAGE(gnn_args['hidden_size'], gnn_args['hidden_size'], gnn_args['hidden_size'],\n              gnn_args['num_layers'], gnn_args['dropout']).to(device)\nsage_predictor = LinkPredictor(gnn_args['hidden_size'], gnn_args['hidden_size'], 1,\n                          gnn_args['num_layers'], gnn_args['dropout']).to(device)\nsage_emb_rand = torch.nn.Embedding(dataset.data.num_nodes, gnn_args['hidden_size']).to(device)\ntrain_model(sage_model, sage_emb_rand, gnn_args, sage_predictor, model_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T07:10:59.826891Z","iopub.execute_input":"2025-05-28T07:10:59.827618Z","iopub.status.idle":"2025-05-28T07:18:08.510480Z","shell.execute_reply.started":"2025-05-28T07:10:59.827591Z","shell.execute_reply":"2025-05-28T07:18:08.509704Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n  warnings.warn(msg)\n/usr/local/lib/python3.11/dist-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n  warnings.warn(out)\n","output_type":"stream"},{"name":"stdout","text":"Hits@10\nRun: 01, Epoch: 05, Loss: 0.5733, Train: 8.38%, Valid: 7.35%, Test: 5.86%\nHits@20\nRun: 01, Epoch: 05, Loss: 0.5733, Train: 12.85%, Valid: 11.38%, Test: 9.00%\n---\nHits@10\nRun: 01, Epoch: 10, Loss: 0.4084, Train: 28.46%, Valid: 25.33%, Test: 9.78%\nHits@20\nRun: 01, Epoch: 10, Loss: 0.4084, Train: 32.24%, Valid: 28.78%, Test: 16.75%\n---\nHits@10\nRun: 01, Epoch: 15, Loss: 0.3221, Train: 32.78%, Valid: 28.39%, Test: 5.23%\nHits@20\nRun: 01, Epoch: 15, Loss: 0.3221, Train: 39.36%, Valid: 34.41%, Test: 12.56%\n---\nHits@10\nRun: 01, Epoch: 20, Loss: 0.2858, Train: 38.63%, Valid: 33.21%, Test: 5.82%\nHits@20\nRun: 01, Epoch: 20, Loss: 0.2858, Train: 43.74%, Valid: 37.96%, Test: 11.02%\n---\nHits@10\nRun: 01, Epoch: 25, Loss: 0.2560, Train: 42.11%, Valid: 36.05%, Test: 10.33%\nHits@20\nRun: 01, Epoch: 25, Loss: 0.2560, Train: 48.19%, Valid: 41.62%, Test: 21.04%\n---\nHits@10\nRun: 01, Epoch: 30, Loss: 0.2379, Train: 51.01%, Valid: 43.95%, Test: 11.48%\nHits@20\nRun: 01, Epoch: 30, Loss: 0.2379, Train: 56.50%, Valid: 48.91%, Test: 18.95%\n---\nHits@10\nRun: 01, Epoch: 35, Loss: 0.2227, Train: 51.57%, Valid: 44.29%, Test: 16.98%\nHits@20\nRun: 01, Epoch: 35, Loss: 0.2227, Train: 60.78%, Valid: 52.86%, Test: 33.90%\n---\nHits@10\nRun: 01, Epoch: 40, Loss: 0.2152, Train: 57.28%, Valid: 49.48%, Test: 26.03%\nHits@20\nRun: 01, Epoch: 40, Loss: 0.2152, Train: 63.19%, Valid: 55.04%, Test: 46.12%\n---\nHits@10\nRun: 01, Epoch: 45, Loss: 0.2071, Train: 59.47%, Valid: 51.46%, Test: 35.61%\nHits@20\nRun: 01, Epoch: 45, Loss: 0.2071, Train: 65.28%, Valid: 56.89%, Test: 46.93%\n---\nHits@10\nRun: 01, Epoch: 50, Loss: 0.2014, Train: 53.11%, Valid: 44.79%, Test: 32.13%\nHits@20\nRun: 01, Epoch: 50, Loss: 0.2014, Train: 65.73%, Valid: 56.78%, Test: 48.80%\n---\nHits@10\nRun: 01, Epoch: 55, Loss: 0.1979, Train: 51.29%, Valid: 42.83%, Test: 17.45%\nHits@20\nRun: 01, Epoch: 55, Loss: 0.1979, Train: 61.83%, Valid: 52.70%, Test: 38.07%\n---\nHits@10\nRun: 01, Epoch: 60, Loss: 0.1933, Train: 61.17%, Valid: 51.95%, Test: 29.45%\nHits@20\nRun: 01, Epoch: 60, Loss: 0.1933, Train: 68.38%, Valid: 59.10%, Test: 48.64%\n---\nHits@10\nRun: 01, Epoch: 65, Loss: 0.1894, Train: 60.89%, Valid: 51.80%, Test: 15.98%\nHits@20\nRun: 01, Epoch: 65, Loss: 0.1894, Train: 69.83%, Valid: 60.41%, Test: 39.73%\n---\nHits@10\nRun: 01, Epoch: 70, Loss: 0.1871, Train: 65.92%, Valid: 56.55%, Test: 39.95%\nHits@20\nRun: 01, Epoch: 70, Loss: 0.1871, Train: 70.21%, Valid: 60.83%, Test: 53.30%\n---\nHits@10\nRun: 01, Epoch: 75, Loss: 0.1845, Train: 63.25%, Valid: 54.09%, Test: 34.63%\nHits@20\nRun: 01, Epoch: 75, Loss: 0.1845, Train: 72.96%, Valid: 63.37%, Test: 53.72%\n---\nHits@10\nRun: 01, Epoch: 80, Loss: 0.1835, Train: 65.34%, Valid: 55.72%, Test: 50.92%\nHits@20\nRun: 01, Epoch: 80, Loss: 0.1835, Train: 71.72%, Valid: 61.96%, Test: 57.23%\n---\nHits@10\nRun: 01, Epoch: 85, Loss: 0.1801, Train: 66.85%, Valid: 57.23%, Test: 34.86%\nHits@20\nRun: 01, Epoch: 85, Loss: 0.1801, Train: 72.31%, Valid: 62.65%, Test: 56.54%\n---\nHits@10\nRun: 01, Epoch: 90, Loss: 0.1783, Train: 68.64%, Valid: 58.81%, Test: 41.65%\nHits@20\nRun: 01, Epoch: 90, Loss: 0.1783, Train: 72.00%, Valid: 62.18%, Test: 62.96%\n---\nHits@10\nRun: 01, Epoch: 95, Loss: 0.1763, Train: 67.77%, Valid: 57.96%, Test: 30.41%\nHits@20\nRun: 01, Epoch: 95, Loss: 0.1763, Train: 73.99%, Valid: 64.08%, Test: 53.65%\n---\nHits@10\nRun: 01, Epoch: 100, Loss: 0.1770, Train: 62.79%, Valid: 52.99%, Test: 45.39%\nHits@20\nRun: 01, Epoch: 100, Loss: 0.1770, Train: 74.73%, Valid: 64.75%, Test: 67.20%\n---\nHits@10\nRun: 02, Epoch: 05, Loss: 0.5760, Train: 11.21%, Valid: 9.71%, Test: 4.05%\nHits@20\nRun: 02, Epoch: 05, Loss: 0.5760, Train: 15.51%, Valid: 13.70%, Test: 8.60%\n---\nHits@10\nRun: 02, Epoch: 10, Loss: 0.4054, Train: 20.78%, Valid: 18.22%, Test: 6.97%\nHits@20\nRun: 02, Epoch: 10, Loss: 0.4054, Train: 27.05%, Valid: 23.95%, Test: 10.88%\n---\nHits@10\nRun: 02, Epoch: 15, Loss: 0.3299, Train: 29.72%, Valid: 25.80%, Test: 6.44%\nHits@20\nRun: 02, Epoch: 15, Loss: 0.3299, Train: 36.94%, Valid: 32.40%, Test: 14.16%\n---\nHits@10\nRun: 02, Epoch: 20, Loss: 0.2914, Train: 23.34%, Valid: 19.72%, Test: 4.74%\nHits@20\nRun: 02, Epoch: 20, Loss: 0.2914, Train: 30.83%, Valid: 26.38%, Test: 9.17%\n---\nHits@10\nRun: 02, Epoch: 25, Loss: 0.2619, Train: 40.05%, Valid: 34.53%, Test: 10.81%\nHits@20\nRun: 02, Epoch: 25, Loss: 0.2619, Train: 49.90%, Valid: 43.38%, Test: 19.49%\n---\nHits@10\nRun: 02, Epoch: 30, Loss: 0.2433, Train: 37.40%, Valid: 31.41%, Test: 14.07%\nHits@20\nRun: 02, Epoch: 30, Loss: 0.2433, Train: 47.95%, Valid: 41.04%, Test: 21.97%\n---\nHits@10\nRun: 02, Epoch: 35, Loss: 0.2259, Train: 54.01%, Valid: 46.47%, Test: 24.63%\nHits@20\nRun: 02, Epoch: 35, Loss: 0.2259, Train: 59.33%, Valid: 51.48%, Test: 34.66%\n---\nHits@10\nRun: 02, Epoch: 40, Loss: 0.2158, Train: 53.97%, Valid: 46.36%, Test: 13.14%\nHits@20\nRun: 02, Epoch: 40, Loss: 0.2158, Train: 60.88%, Valid: 52.77%, Test: 27.63%\n---\nHits@10\nRun: 02, Epoch: 45, Loss: 0.2096, Train: 55.45%, Valid: 47.39%, Test: 14.49%\nHits@20\nRun: 02, Epoch: 45, Loss: 0.2096, Train: 64.57%, Valid: 55.95%, Test: 32.53%\n---\nHits@10\nRun: 02, Epoch: 50, Loss: 0.2046, Train: 62.50%, Valid: 53.84%, Test: 29.40%\nHits@20\nRun: 02, Epoch: 50, Loss: 0.2046, Train: 67.47%, Valid: 58.54%, Test: 46.65%\n---\nHits@10\nRun: 02, Epoch: 55, Loss: 0.1995, Train: 60.47%, Valid: 51.86%, Test: 22.22%\nHits@20\nRun: 02, Epoch: 55, Loss: 0.1995, Train: 68.53%, Valid: 59.66%, Test: 54.72%\n---\nHits@10\nRun: 02, Epoch: 60, Loss: 0.1957, Train: 56.01%, Valid: 47.51%, Test: 36.24%\nHits@20\nRun: 02, Epoch: 60, Loss: 0.1957, Train: 67.81%, Valid: 58.75%, Test: 52.56%\n---\nHits@10\nRun: 02, Epoch: 65, Loss: 0.1912, Train: 66.28%, Valid: 57.09%, Test: 34.50%\nHits@20\nRun: 02, Epoch: 65, Loss: 0.1912, Train: 69.63%, Valid: 60.29%, Test: 53.69%\n---\nHits@10\nRun: 02, Epoch: 70, Loss: 0.1895, Train: 65.74%, Valid: 56.45%, Test: 30.72%\nHits@20\nRun: 02, Epoch: 70, Loss: 0.1895, Train: 69.54%, Valid: 60.17%, Test: 52.88%\n---\nHits@10\nRun: 02, Epoch: 75, Loss: 0.1860, Train: 53.91%, Valid: 44.77%, Test: 32.52%\nHits@20\nRun: 02, Epoch: 75, Loss: 0.1860, Train: 65.16%, Valid: 55.54%, Test: 44.64%\n---\nHits@10\nRun: 02, Epoch: 80, Loss: 0.1834, Train: 61.27%, Valid: 52.25%, Test: 39.57%\nHits@20\nRun: 02, Epoch: 80, Loss: 0.1834, Train: 68.18%, Valid: 58.80%, Test: 59.41%\n---\nHits@10\nRun: 02, Epoch: 85, Loss: 0.1819, Train: 63.01%, Valid: 53.25%, Test: 38.97%\nHits@20\nRun: 02, Epoch: 85, Loss: 0.1819, Train: 68.08%, Valid: 58.11%, Test: 66.78%\n---\nHits@10\nRun: 02, Epoch: 90, Loss: 0.1809, Train: 64.13%, Valid: 54.91%, Test: 42.32%\nHits@20\nRun: 02, Epoch: 90, Loss: 0.1809, Train: 70.61%, Valid: 61.22%, Test: 56.30%\n---\nHits@10\nRun: 02, Epoch: 95, Loss: 0.1777, Train: 62.49%, Valid: 52.65%, Test: 46.85%\nHits@20\nRun: 02, Epoch: 95, Loss: 0.1777, Train: 72.17%, Valid: 62.21%, Test: 67.13%\n---\nHits@10\nRun: 02, Epoch: 100, Loss: 0.1765, Train: 60.07%, Valid: 50.16%, Test: 27.03%\nHits@20\nRun: 02, Epoch: 100, Loss: 0.1765, Train: 68.69%, Valid: 58.74%, Test: 44.06%\n---\nVal_hits@20:  [0.647476571103237, 0.6220887114294061]\nTest_hits@20:  [0.6720254103334358, 0.6712987586992186]\nTrain_hits@20:  [0.747297530133569, 0.7216624590790253]\nBest model val hits@20:  0.647476571103237\nBest model test hits@20:  0.6720254103334358\nBest model train hits@20:  0.647476571103237\nAverage best train hits@20: 0.7344799946062972; var: 0.00016428921699287693\nAverage best val hits@20: 0.6347826412663216; var: 0.0001611358547045315\nAverage best test hits@20: 0.6716620845163273; var: 1.3200564937761218e-07\n","output_type":"stream"}],"execution_count":108},{"cell_type":"code","source":"# Loading pretrained graphsage (random weight init) [model with best tests hits@20]\nsage_model = SAGE(gnn_args['hidden_size'], gnn_args['hidden_size'], gnn_args['hidden_size'],\n             gnn_args['num_layers'], gnn_args['dropout']).to(device)\n\n\nload_model_ckpt(sage_model, model_name,0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T07:19:13.947686Z","iopub.execute_input":"2025-05-28T07:19:13.948302Z","iopub.status.idle":"2025-05-28T07:19:14.296997Z","shell.execute_reply.started":"2025-05-28T07:19:13.948270Z","shell.execute_reply":"2025-05-28T07:19:14.296184Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n  warnings.warn(msg)\n/usr/local/lib/python3.11/dist-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n  warnings.warn(out)\n","output_type":"stream"},{"name":"stdout","text":"Hits@10\nTrain: 62.49%, Valid: 52.65%, Test: 46.85%\nHits@20\nTrain: 72.17%, Valid: 62.21%, Test: 67.13%\n","output_type":"stream"}],"execution_count":109},{"cell_type":"code","source":"upload_file(\n    path_or_fileobj=\"/kaggle/working/training_outputs/sage_rand_feat.pt\",\n    path_in_repo=\"sage_rand_feat.pt\",\n    repo_id=\"Rzoro/ogb_ddi\",\n    repo_type=\"model\"\n)\nupload_file(\n    path_or_fileobj=\"/kaggle/working/training_outputs/sage_rand_feat_final_emb_0.pt\",\n    path_in_repo=\"sage_rand_feat_final_emb_0.pt\",\n    repo_id=\"Rzoro/ogb_ddi\",\n    repo_type=\"model\"\n)\nupload_file(\n    path_or_fileobj=\"/kaggle/working/training_outputs/sage_rand_feat_init_emb.pt\",\n    path_in_repo=\"sage_rand_feat_init_emb.pt\",\n    repo_id=\"Rzoro/ogb_ddi\",\n    repo_type=\"model\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T07:20:11.020161Z","iopub.execute_input":"2025-05-28T07:20:11.020449Z","iopub.status.idle":"2025-05-28T07:20:18.686623Z","shell.execute_reply.started":"2025-05-28T07:20:11.020427Z","shell.execute_reply":"2025-05-28T07:20:18.685974Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"sage_rand_feat.pt:   0%|          | 0.00/12.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8f1997cc3fa4f43a0dd54c075f7f516"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sage_rand_feat_final_emb_0.pt:   0%|          | 0.00/4.37M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"923db32850fd4f5fac212bf11274ed9b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sage_rand_feat_init_emb.pt:   0%|          | 0.00/4.37M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"448a4eab284c44aeb8ef0ef17e13e773"}},"metadata":{}},{"execution_count":110,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/Rzoro/ogb_ddi/commit/fc532e539b0b627237324971215810885c99710a', commit_message='Upload sage_rand_feat_init_emb.pt with huggingface_hub', commit_description='', oid='fc532e539b0b627237324971215810885c99710a', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Rzoro/ogb_ddi', endpoint='https://huggingface.co', repo_type='model', repo_id='Rzoro/ogb_ddi'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":110},{"cell_type":"code","source":"# Train GraphSAGE with node2vec = 256 features\nmodel_name = 'sage_node2vec_feat_256'\nsage_model2 = SAGE(gnn_args['hidden_size'], gnn_args['hidden_size'], gnn_args['hidden_size'],\n             gnn_args['num_layers'], gnn_args['dropout']).to(device)\n\nsage_predictor2 = LinkPredictor(gnn_args['hidden_size'], gnn_args['hidden_size'], 1,\n                          gnn_args['num_layers'], gnn_args['dropout']).to(device)\nsage_emb_node2vec = torch.nn.Embedding(dataset.data.num_nodes, gnn_args['hidden_size']).to(device)\nsage_emb_node2vec.weight.data.copy_(pretrained_weight)\ntrain_model(sage_model2, sage_emb_node2vec, gnn_args, sage_predictor2, 'sage_node2vec_feat_256')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T07:20:48.565898Z","iopub.execute_input":"2025-05-28T07:20:48.566379Z","iopub.status.idle":"2025-05-28T07:27:58.448136Z","shell.execute_reply.started":"2025-05-28T07:20:48.566344Z","shell.execute_reply":"2025-05-28T07:27:58.447298Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n  warnings.warn(out)\n","output_type":"stream"},{"name":"stdout","text":"Hits@10\nRun: 01, Epoch: 05, Loss: 0.5644, Train: 16.31%, Valid: 14.86%, Test: 3.31%\nHits@20\nRun: 01, Epoch: 05, Loss: 0.5644, Train: 20.05%, Valid: 18.39%, Test: 8.41%\n---\nHits@10\nRun: 01, Epoch: 10, Loss: 0.3869, Train: 30.05%, Valid: 26.28%, Test: 14.28%\nHits@20\nRun: 01, Epoch: 10, Loss: 0.3869, Train: 37.73%, Valid: 33.53%, Test: 23.35%\n---\nHits@10\nRun: 01, Epoch: 15, Loss: 0.3111, Train: 29.08%, Valid: 24.77%, Test: 12.80%\nHits@20\nRun: 01, Epoch: 15, Loss: 0.3111, Train: 37.67%, Valid: 32.40%, Test: 18.07%\n---\nHits@10\nRun: 01, Epoch: 20, Loss: 0.2733, Train: 48.02%, Valid: 41.63%, Test: 18.20%\nHits@20\nRun: 01, Epoch: 20, Loss: 0.2733, Train: 51.08%, Valid: 44.61%, Test: 23.13%\n---\nHits@10\nRun: 01, Epoch: 25, Loss: 0.2473, Train: 42.33%, Valid: 35.88%, Test: 11.22%\nHits@20\nRun: 01, Epoch: 25, Loss: 0.2473, Train: 52.28%, Valid: 45.01%, Test: 20.93%\n---\nHits@10\nRun: 01, Epoch: 30, Loss: 0.2309, Train: 46.44%, Valid: 39.12%, Test: 14.83%\nHits@20\nRun: 01, Epoch: 30, Loss: 0.2309, Train: 56.21%, Valid: 48.14%, Test: 21.72%\n---\nHits@10\nRun: 01, Epoch: 35, Loss: 0.2185, Train: 58.17%, Valid: 50.11%, Test: 15.18%\nHits@20\nRun: 01, Epoch: 35, Loss: 0.2185, Train: 62.41%, Valid: 54.23%, Test: 33.86%\n---\nHits@10\nRun: 01, Epoch: 40, Loss: 0.2151, Train: 57.06%, Valid: 48.85%, Test: 25.48%\nHits@20\nRun: 01, Epoch: 40, Loss: 0.2151, Train: 63.09%, Valid: 54.48%, Test: 40.73%\n---\nHits@10\nRun: 01, Epoch: 45, Loss: 0.2040, Train: 47.05%, Valid: 38.92%, Test: 19.88%\nHits@20\nRun: 01, Epoch: 45, Loss: 0.2040, Train: 57.58%, Valid: 48.74%, Test: 33.18%\n---\nHits@10\nRun: 01, Epoch: 50, Loss: 0.1993, Train: 57.00%, Valid: 48.32%, Test: 36.06%\nHits@20\nRun: 01, Epoch: 50, Loss: 0.1993, Train: 64.87%, Valid: 55.73%, Test: 47.32%\n---\nHits@10\nRun: 01, Epoch: 55, Loss: 0.1964, Train: 58.12%, Valid: 49.33%, Test: 8.04%\nHits@20\nRun: 01, Epoch: 55, Loss: 0.1964, Train: 65.79%, Valid: 56.70%, Test: 31.04%\n---\nHits@10\nRun: 01, Epoch: 60, Loss: 0.1907, Train: 57.14%, Valid: 47.85%, Test: 33.44%\nHits@20\nRun: 01, Epoch: 60, Loss: 0.1907, Train: 67.40%, Valid: 57.80%, Test: 44.93%\n---\nHits@10\nRun: 01, Epoch: 65, Loss: 0.1881, Train: 60.01%, Valid: 50.96%, Test: 38.46%\nHits@20\nRun: 01, Epoch: 65, Loss: 0.1881, Train: 68.32%, Valid: 59.07%, Test: 52.09%\n---\nHits@10\nRun: 01, Epoch: 70, Loss: 0.1855, Train: 58.50%, Valid: 48.87%, Test: 21.08%\nHits@20\nRun: 01, Epoch: 70, Loss: 0.1855, Train: 69.75%, Valid: 60.03%, Test: 40.53%\n---\nHits@10\nRun: 01, Epoch: 75, Loss: 0.1825, Train: 65.01%, Valid: 55.43%, Test: 31.35%\nHits@20\nRun: 01, Epoch: 75, Loss: 0.1825, Train: 72.21%, Valid: 62.51%, Test: 48.75%\n---\nHits@10\nRun: 01, Epoch: 80, Loss: 0.1802, Train: 61.55%, Valid: 51.70%, Test: 39.56%\nHits@20\nRun: 01, Epoch: 80, Loss: 0.1802, Train: 67.79%, Valid: 57.85%, Test: 60.46%\n---\nHits@10\nRun: 01, Epoch: 85, Loss: 0.1792, Train: 58.20%, Valid: 48.72%, Test: 45.67%\nHits@20\nRun: 01, Epoch: 85, Loss: 0.1792, Train: 71.53%, Valid: 61.88%, Test: 51.09%\n---\nHits@10\nRun: 01, Epoch: 90, Loss: 0.1758, Train: 64.91%, Valid: 55.04%, Test: 31.22%\nHits@20\nRun: 01, Epoch: 90, Loss: 0.1758, Train: 71.08%, Valid: 61.21%, Test: 54.08%\n---\nHits@10\nRun: 01, Epoch: 95, Loss: 0.1753, Train: 69.18%, Valid: 59.11%, Test: 31.36%\nHits@20\nRun: 01, Epoch: 95, Loss: 0.1753, Train: 73.62%, Valid: 63.61%, Test: 57.62%\n---\nHits@10\nRun: 01, Epoch: 100, Loss: 0.1766, Train: 66.86%, Valid: 56.95%, Test: 32.65%\nHits@20\nRun: 01, Epoch: 100, Loss: 0.1766, Train: 73.70%, Valid: 63.77%, Test: 49.05%\n---\nHits@10\nRun: 02, Epoch: 05, Loss: 0.5844, Train: 9.39%, Valid: 8.20%, Test: 3.61%\nHits@20\nRun: 02, Epoch: 05, Loss: 0.5844, Train: 13.87%, Valid: 12.32%, Test: 7.98%\n---\nHits@10\nRun: 02, Epoch: 10, Loss: 0.4186, Train: 24.14%, Valid: 21.31%, Test: 8.72%\nHits@20\nRun: 02, Epoch: 10, Loss: 0.4186, Train: 29.57%, Valid: 26.44%, Test: 15.74%\n---\nHits@10\nRun: 02, Epoch: 15, Loss: 0.3346, Train: 17.26%, Valid: 14.60%, Test: 3.98%\nHits@20\nRun: 02, Epoch: 15, Loss: 0.3346, Train: 25.22%, Valid: 21.63%, Test: 7.89%\n---\nHits@10\nRun: 02, Epoch: 20, Loss: 0.2941, Train: 41.81%, Valid: 36.35%, Test: 13.32%\nHits@20\nRun: 02, Epoch: 20, Loss: 0.2941, Train: 49.21%, Valid: 43.08%, Test: 23.63%\n---\nHits@10\nRun: 02, Epoch: 25, Loss: 0.2643, Train: 43.80%, Valid: 37.70%, Test: 5.82%\nHits@20\nRun: 02, Epoch: 25, Loss: 0.2643, Train: 51.77%, Valid: 45.08%, Test: 18.50%\n---\nHits@10\nRun: 02, Epoch: 30, Loss: 0.2418, Train: 52.12%, Valid: 45.07%, Test: 17.04%\nHits@20\nRun: 02, Epoch: 30, Loss: 0.2418, Train: 57.53%, Valid: 50.07%, Test: 23.85%\n---\nHits@10\nRun: 02, Epoch: 35, Loss: 0.2295, Train: 47.15%, Valid: 39.94%, Test: 19.64%\nHits@20\nRun: 02, Epoch: 35, Loss: 0.2295, Train: 54.62%, Valid: 47.04%, Test: 32.67%\n---\nHits@10\nRun: 02, Epoch: 40, Loss: 0.2192, Train: 52.07%, Valid: 44.40%, Test: 24.34%\nHits@20\nRun: 02, Epoch: 40, Loss: 0.2192, Train: 61.62%, Valid: 53.39%, Test: 38.55%\n---\nHits@10\nRun: 02, Epoch: 45, Loss: 0.2109, Train: 56.92%, Valid: 48.69%, Test: 28.64%\nHits@20\nRun: 02, Epoch: 45, Loss: 0.2109, Train: 62.44%, Valid: 53.96%, Test: 38.57%\n---\nHits@10\nRun: 02, Epoch: 50, Loss: 0.2054, Train: 55.47%, Valid: 47.35%, Test: 29.58%\nHits@20\nRun: 02, Epoch: 50, Loss: 0.2054, Train: 63.37%, Valid: 54.85%, Test: 37.91%\n---\nHits@10\nRun: 02, Epoch: 55, Loss: 0.1971, Train: 59.35%, Valid: 50.80%, Test: 30.20%\nHits@20\nRun: 02, Epoch: 55, Loss: 0.1971, Train: 64.49%, Valid: 55.79%, Test: 41.98%\n---\nHits@10\nRun: 02, Epoch: 60, Loss: 0.1960, Train: 56.92%, Valid: 47.92%, Test: 37.23%\nHits@20\nRun: 02, Epoch: 60, Loss: 0.1960, Train: 66.16%, Valid: 57.00%, Test: 48.91%\n---\nHits@10\nRun: 02, Epoch: 65, Loss: 0.1906, Train: 58.53%, Valid: 49.62%, Test: 36.61%\nHits@20\nRun: 02, Epoch: 65, Loss: 0.1906, Train: 65.75%, Valid: 56.68%, Test: 52.17%\n---\nHits@10\nRun: 02, Epoch: 70, Loss: 0.1876, Train: 65.76%, Valid: 56.86%, Test: 34.08%\nHits@20\nRun: 02, Epoch: 70, Loss: 0.1876, Train: 72.39%, Valid: 63.12%, Test: 53.15%\n---\nHits@10\nRun: 02, Epoch: 75, Loss: 0.1837, Train: 60.69%, Valid: 51.37%, Test: 42.66%\nHits@20\nRun: 02, Epoch: 75, Loss: 0.1837, Train: 71.97%, Valid: 62.51%, Test: 52.07%\n---\nHits@10\nRun: 02, Epoch: 80, Loss: 0.1812, Train: 65.81%, Valid: 56.48%, Test: 41.86%\nHits@20\nRun: 02, Epoch: 80, Loss: 0.1812, Train: 72.92%, Valid: 63.40%, Test: 58.12%\n---\nHits@10\nRun: 02, Epoch: 85, Loss: 0.1805, Train: 65.02%, Valid: 55.61%, Test: 30.52%\nHits@20\nRun: 02, Epoch: 85, Loss: 0.1805, Train: 70.39%, Valid: 60.91%, Test: 49.46%\n---\nHits@10\nRun: 02, Epoch: 90, Loss: 0.1776, Train: 63.97%, Valid: 54.52%, Test: 33.71%\nHits@20\nRun: 02, Epoch: 90, Loss: 0.1776, Train: 68.41%, Valid: 58.87%, Test: 43.91%\n---\nHits@10\nRun: 02, Epoch: 95, Loss: 0.1774, Train: 62.24%, Valid: 52.92%, Test: 43.22%\nHits@20\nRun: 02, Epoch: 95, Loss: 0.1774, Train: 72.13%, Valid: 62.69%, Test: 54.80%\n---\nHits@10\nRun: 02, Epoch: 100, Loss: 0.1745, Train: 69.30%, Valid: 59.60%, Test: 25.95%\nHits@20\nRun: 02, Epoch: 100, Loss: 0.1745, Train: 73.58%, Valid: 63.87%, Test: 49.52%\n---\nVal_hits@20:  [0.6376705196682873, 0.6386968214609443]\nTest_hits@20:  [0.4904673793346268, 0.4951868693300572]\nTrain_hits@20:  [0.7370494947149203, 0.7358434028271993]\nBest model val hits@20:  0.6386968214609443\nBest model test hits@20:  0.4951868693300572\nBest model train hits@20:  0.6386968214609443\nAverage best train hits@20: 0.7364464487710598; var: 3.6366441040661824e-07\nAverage best val hits@20: 0.6381836705646158; var: 2.6332384240277287e-07\nAverage best test hits@20: 0.492827124332342; var: 5.56839645424184e-06\n","output_type":"stream"}],"execution_count":112},{"cell_type":"code","source":"# Loading pretrained graphsage (random weight init) [model with best tests hits@20]\nsage_model2 = SAGE(gnn_args['hidden_size'], gnn_args['hidden_size'], gnn_args['hidden_size'],\n             gnn_args['num_layers'], gnn_args['dropout']).to(device)\n\n\nload_model_ckpt(sage_model2, model_name,0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T07:27:58.449192Z","iopub.execute_input":"2025-05-28T07:27:58.449396Z","iopub.status.idle":"2025-05-28T07:27:58.725956Z","shell.execute_reply.started":"2025-05-28T07:27:58.449380Z","shell.execute_reply":"2025-05-28T07:27:58.725288Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n  warnings.warn(msg)\n/usr/local/lib/python3.11/dist-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n  warnings.warn(out)\n","output_type":"stream"},{"name":"stdout","text":"Hits@10\nTrain: 69.30%, Valid: 59.60%, Test: 25.95%\nHits@20\nTrain: 73.58%, Valid: 63.87%, Test: 49.52%\n","output_type":"stream"}],"execution_count":113},{"cell_type":"code","source":"upload_file(\n    path_or_fileobj=\"/kaggle/working/training_outputs/sage_node2vec_feat_256.pt\",\n    path_in_repo=\"sage_node2vec_feat_256.pt\",\n    repo_id=\"Rzoro/ogb_ddi\",\n    repo_type=\"model\"\n)\nupload_file(\n    path_or_fileobj=\"/kaggle/working/training_outputs/sage_node2vec_feat_256_final_emb_0.pt\",\n    path_in_repo=\"sage_node2vec_feat_256_final_emb_0.pt\",\n    repo_id=\"Rzoro/ogb_ddi\",\n    repo_type=\"model\"\n)\nupload_file(\n    path_or_fileobj=\"/kaggle/working/training_outputs/sage_node2vec_feat_256_init_emb.pt\",\n    path_in_repo=\"sage_node2vec_feat_256_init_emb.pt\",\n    repo_id=\"Rzoro/ogb_ddi\",\n    repo_type=\"model\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T07:29:26.598902Z","iopub.execute_input":"2025-05-28T07:29:26.599221Z","iopub.status.idle":"2025-05-28T07:29:35.161055Z","shell.execute_reply.started":"2025-05-28T07:29:26.599201Z","shell.execute_reply":"2025-05-28T07:29:35.160482Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"sage_node2vec_feat_256.pt:   0%|          | 0.00/12.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dede9f97746346c58be300d15a4167f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sage_node2vec_feat_256_final_emb_0.pt:   0%|          | 0.00/4.37M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"533bd2f9bc944336932331e9a65a6224"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sage_node2vec_feat_256_init_emb.pt:   0%|          | 0.00/4.37M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d16c4ed8dd6849b9bac75510f5c1e9e2"}},"metadata":{}},{"execution_count":114,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/Rzoro/ogb_ddi/commit/3298fc3ecf1541707e10cdeb9fa01874ba932438', commit_message='Upload sage_node2vec_feat_256_init_emb.pt with huggingface_hub', commit_description='', oid='3298fc3ecf1541707e10cdeb9fa01874ba932438', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Rzoro/ogb_ddi', endpoint='https://huggingface.co', repo_type='model', repo_id='Rzoro/ogb_ddi'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":114},{"cell_type":"code","source":"# class using PyG's GINConv layer\nclass GIN(torch.nn.Module):\n    ''' Define graph isomorphic network. '''\n    def __init__(self, in_channels, dim, out_channels, num_layers,\n                 dropout):\n        super().__init__()\n         # # Initialize 2 GINConv layers (following num_layers=2 in model-agnostic hyperparams as specified in the blogpost)\n        self.conv1 = GINConv(\n            Sequential(Linear(in_channels, dim), BatchNorm1d(dim), ReLU(),\n                       Linear(dim, dim), ReLU())) # GINConv takes a neural network as input\n\n        self.conv2 = GINConv(\n            Sequential(Linear(dim, dim), BatchNorm1d(dim), ReLU(),\n                       Linear(dim, dim), ReLU()))\n\n        self.dropout = dropout\n\n    def reset_parameters(self):\n        self.conv1.reset_parameters()\n        self.conv2.reset_parameters()\n\n\n    def forward(self, x, adj_t):\n       # Execute conv -> relu -> dropout sequence\n        x = self.conv1(x, adj_t)\n        x = F.relu(x)\n        x = F.dropout(x, p=self.dropout, training=self.training)\n        x = self.conv2(x, adj_t)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T07:29:55.476522Z","iopub.execute_input":"2025-05-28T07:29:55.476809Z","iopub.status.idle":"2025-05-28T07:29:55.482978Z","shell.execute_reply.started":"2025-05-28T07:29:55.476789Z","shell.execute_reply":"2025-05-28T07:29:55.482250Z"}},"outputs":[],"execution_count":115},{"cell_type":"code","source":"# Train GIN with random features\nmodel_name = \"gin_rand_feat\"\ngin_model = GIN(gnn_args['hidden_size'], gnn_args['hidden_size'], gnn_args['hidden_size'],\n             gnn_args['num_layers'], gnn_args['dropout']).to(device)\n\ngin_predictor = LinkPredictor(gnn_args['hidden_size'], gnn_args['hidden_size'], 1,\n                          gnn_args['num_layers'], gnn_args['dropout']).to(device)\ngin_emb_rand = torch.nn.Embedding(dataset.data.num_nodes, gnn_args['hidden_size']).to(device)\ntrain_model(gin_model, gin_emb_rand, gnn_args, gin_predictor, model_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T07:31:04.696726Z","iopub.execute_input":"2025-05-28T07:31:04.696985Z","iopub.status.idle":"2025-05-28T07:37:56.450294Z","shell.execute_reply.started":"2025-05-28T07:31:04.696965Z","shell.execute_reply":"2025-05-28T07:37:56.449597Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n  warnings.warn(out)\n","output_type":"stream"},{"name":"stdout","text":"Hits@10\nRun: 01, Epoch: 05, Loss: 0.5476, Train: 2.48%, Valid: 2.08%, Test: 2.59%\nHits@20\nRun: 01, Epoch: 05, Loss: 0.5476, Train: 5.12%, Valid: 4.27%, Test: 3.19%\n---\nHits@10\nRun: 01, Epoch: 10, Loss: 0.4828, Train: 17.83%, Valid: 15.55%, Test: 9.29%\nHits@20\nRun: 01, Epoch: 10, Loss: 0.4828, Train: 25.39%, Valid: 22.50%, Test: 16.27%\n---\nHits@10\nRun: 01, Epoch: 15, Loss: 0.4137, Train: 20.83%, Valid: 18.24%, Test: 12.02%\nHits@20\nRun: 01, Epoch: 15, Loss: 0.4137, Train: 24.90%, Valid: 21.69%, Test: 23.88%\n---\nHits@10\nRun: 01, Epoch: 20, Loss: 0.3714, Train: 25.54%, Valid: 22.47%, Test: 25.96%\nHits@20\nRun: 01, Epoch: 20, Loss: 0.3714, Train: 35.17%, Valid: 31.09%, Test: 33.27%\n---\nHits@10\nRun: 01, Epoch: 25, Loss: 0.3431, Train: 9.52%, Valid: 8.05%, Test: 4.29%\nHits@20\nRun: 01, Epoch: 25, Loss: 0.3431, Train: 14.28%, Valid: 12.28%, Test: 5.87%\n---\nHits@10\nRun: 01, Epoch: 30, Loss: 0.3198, Train: 20.10%, Valid: 17.43%, Test: 17.89%\nHits@20\nRun: 01, Epoch: 30, Loss: 0.3198, Train: 28.06%, Valid: 24.55%, Test: 24.57%\n---\nHits@10\nRun: 01, Epoch: 35, Loss: 0.2958, Train: 32.20%, Valid: 28.20%, Test: 29.56%\nHits@20\nRun: 01, Epoch: 35, Loss: 0.2958, Train: 42.24%, Valid: 37.34%, Test: 33.93%\n---\nHits@10\nRun: 01, Epoch: 40, Loss: 0.2839, Train: 21.18%, Valid: 18.13%, Test: 24.71%\nHits@20\nRun: 01, Epoch: 40, Loss: 0.2839, Train: 26.90%, Valid: 23.19%, Test: 28.68%\n---\nHits@10\nRun: 01, Epoch: 45, Loss: 0.2674, Train: 31.57%, Valid: 27.25%, Test: 16.97%\nHits@20\nRun: 01, Epoch: 45, Loss: 0.2674, Train: 39.93%, Valid: 34.95%, Test: 36.50%\n---\nHits@10\nRun: 01, Epoch: 50, Loss: 0.2499, Train: 28.51%, Valid: 24.30%, Test: 17.58%\nHits@20\nRun: 01, Epoch: 50, Loss: 0.2499, Train: 43.83%, Valid: 37.98%, Test: 38.71%\n---\nHits@10\nRun: 01, Epoch: 55, Loss: 0.2400, Train: 39.57%, Valid: 34.14%, Test: 19.59%\nHits@20\nRun: 01, Epoch: 55, Loss: 0.2400, Train: 46.74%, Valid: 40.75%, Test: 30.09%\n---\nHits@10\nRun: 01, Epoch: 60, Loss: 0.2345, Train: 18.78%, Valid: 15.85%, Test: 15.66%\nHits@20\nRun: 01, Epoch: 60, Loss: 0.2345, Train: 24.26%, Valid: 20.68%, Test: 24.90%\n---\nHits@10\nRun: 01, Epoch: 65, Loss: 0.2246, Train: 29.19%, Valid: 24.95%, Test: 14.94%\nHits@20\nRun: 01, Epoch: 65, Loss: 0.2246, Train: 39.99%, Valid: 34.55%, Test: 20.51%\n---\nHits@10\nRun: 01, Epoch: 70, Loss: 0.2192, Train: 24.07%, Valid: 20.64%, Test: 21.17%\nHits@20\nRun: 01, Epoch: 70, Loss: 0.2192, Train: 36.64%, Valid: 31.55%, Test: 30.48%\n---\nHits@10\nRun: 01, Epoch: 75, Loss: 0.2132, Train: 24.09%, Valid: 20.39%, Test: 19.65%\nHits@20\nRun: 01, Epoch: 75, Loss: 0.2132, Train: 35.71%, Valid: 30.56%, Test: 29.96%\n---\nHits@10\nRun: 01, Epoch: 80, Loss: 0.2095, Train: 21.49%, Valid: 18.01%, Test: 14.84%\nHits@20\nRun: 01, Epoch: 80, Loss: 0.2095, Train: 36.31%, Valid: 30.97%, Test: 30.58%\n---\nHits@10\nRun: 01, Epoch: 85, Loss: 0.2053, Train: 29.07%, Valid: 24.75%, Test: 29.46%\nHits@20\nRun: 01, Epoch: 85, Loss: 0.2053, Train: 43.60%, Valid: 37.57%, Test: 40.39%\n---\nHits@10\nRun: 01, Epoch: 90, Loss: 0.2069, Train: 44.33%, Valid: 38.26%, Test: 49.63%\nHits@20\nRun: 01, Epoch: 90, Loss: 0.2069, Train: 56.66%, Valid: 49.54%, Test: 60.83%\n---\nHits@10\nRun: 01, Epoch: 95, Loss: 0.2012, Train: 32.88%, Valid: 27.77%, Test: 40.92%\nHits@20\nRun: 01, Epoch: 95, Loss: 0.2012, Train: 47.57%, Valid: 40.89%, Test: 51.62%\n---\nHits@10\nRun: 01, Epoch: 100, Loss: 0.1978, Train: 15.27%, Valid: 12.80%, Test: 13.93%\nHits@20\nRun: 01, Epoch: 100, Loss: 0.1978, Train: 21.95%, Valid: 18.59%, Test: 17.75%\n---\nHits@10\nRun: 02, Epoch: 05, Loss: 0.5199, Train: 12.67%, Valid: 10.91%, Test: 8.60%\nHits@20\nRun: 02, Epoch: 05, Loss: 0.5199, Train: 16.41%, Valid: 14.25%, Test: 12.26%\n---\nHits@10\nRun: 02, Epoch: 10, Loss: 0.4444, Train: 15.48%, Valid: 13.48%, Test: 17.06%\nHits@20\nRun: 02, Epoch: 10, Loss: 0.4444, Train: 23.33%, Valid: 20.78%, Test: 22.73%\n---\nHits@10\nRun: 02, Epoch: 15, Loss: 0.3815, Train: 4.24%, Valid: 3.61%, Test: 7.90%\nHits@20\nRun: 02, Epoch: 15, Loss: 0.3815, Train: 6.55%, Valid: 5.77%, Test: 9.48%\n---\nHits@10\nRun: 02, Epoch: 20, Loss: 0.3455, Train: 19.65%, Valid: 17.14%, Test: 20.20%\nHits@20\nRun: 02, Epoch: 20, Loss: 0.3455, Train: 29.42%, Valid: 26.09%, Test: 23.64%\n---\nHits@10\nRun: 02, Epoch: 25, Loss: 0.3157, Train: 24.68%, Valid: 21.46%, Test: 20.61%\nHits@20\nRun: 02, Epoch: 25, Loss: 0.3157, Train: 31.68%, Valid: 27.73%, Test: 25.22%\n---\nHits@10\nRun: 02, Epoch: 30, Loss: 0.2938, Train: 5.67%, Valid: 4.68%, Test: 6.31%\nHits@20\nRun: 02, Epoch: 30, Loss: 0.2938, Train: 10.80%, Valid: 9.08%, Test: 9.35%\n---\nHits@10\nRun: 02, Epoch: 35, Loss: 0.2680, Train: 22.77%, Valid: 19.49%, Test: 22.47%\nHits@20\nRun: 02, Epoch: 35, Loss: 0.2680, Train: 29.86%, Valid: 25.66%, Test: 28.61%\n---\nHits@10\nRun: 02, Epoch: 40, Loss: 0.2529, Train: 22.05%, Valid: 18.84%, Test: 23.66%\nHits@20\nRun: 02, Epoch: 40, Loss: 0.2529, Train: 30.67%, Valid: 26.34%, Test: 37.24%\n---\nHits@10\nRun: 02, Epoch: 45, Loss: 0.2437, Train: 31.15%, Valid: 26.82%, Test: 25.79%\nHits@20\nRun: 02, Epoch: 45, Loss: 0.2437, Train: 39.79%, Valid: 34.64%, Test: 34.15%\n---\nHits@10\nRun: 02, Epoch: 50, Loss: 0.2344, Train: 12.06%, Valid: 10.07%, Test: 14.96%\nHits@20\nRun: 02, Epoch: 50, Loss: 0.2344, Train: 21.60%, Valid: 18.32%, Test: 20.21%\n---\nHits@10\nRun: 02, Epoch: 55, Loss: 0.2276, Train: 33.43%, Valid: 28.60%, Test: 37.02%\nHits@20\nRun: 02, Epoch: 55, Loss: 0.2276, Train: 46.49%, Valid: 40.16%, Test: 48.04%\n---\nHits@10\nRun: 02, Epoch: 60, Loss: 0.2220, Train: 13.38%, Valid: 11.20%, Test: 20.70%\nHits@20\nRun: 02, Epoch: 60, Loss: 0.2220, Train: 24.71%, Valid: 21.02%, Test: 25.57%\n---\nHits@10\nRun: 02, Epoch: 65, Loss: 0.2168, Train: 31.52%, Valid: 26.93%, Test: 14.69%\nHits@20\nRun: 02, Epoch: 65, Loss: 0.2168, Train: 40.59%, Valid: 34.93%, Test: 34.00%\n---\nHits@10\nRun: 02, Epoch: 70, Loss: 0.2229, Train: 36.12%, Valid: 31.01%, Test: 22.30%\nHits@20\nRun: 02, Epoch: 70, Loss: 0.2229, Train: 44.75%, Valid: 38.79%, Test: 29.28%\n---\nHits@10\nRun: 02, Epoch: 75, Loss: 0.2070, Train: 31.73%, Valid: 27.63%, Test: 25.82%\nHits@20\nRun: 02, Epoch: 75, Loss: 0.2070, Train: 38.07%, Valid: 33.58%, Test: 34.40%\n---\nHits@10\nRun: 02, Epoch: 80, Loss: 0.2044, Train: 18.66%, Valid: 15.81%, Test: 23.97%\nHits@20\nRun: 02, Epoch: 80, Loss: 0.2044, Train: 28.96%, Valid: 24.65%, Test: 41.16%\n---\nHits@10\nRun: 02, Epoch: 85, Loss: 0.1995, Train: 32.43%, Valid: 27.71%, Test: 33.03%\nHits@20\nRun: 02, Epoch: 85, Loss: 0.1995, Train: 39.88%, Valid: 34.37%, Test: 54.09%\n---\nHits@10\nRun: 02, Epoch: 90, Loss: 0.1979, Train: 29.85%, Valid: 25.26%, Test: 33.54%\nHits@20\nRun: 02, Epoch: 90, Loss: 0.1979, Train: 42.36%, Valid: 36.39%, Test: 44.33%\n---\nHits@10\nRun: 02, Epoch: 95, Loss: 0.1957, Train: 21.91%, Valid: 18.51%, Test: 24.58%\nHits@20\nRun: 02, Epoch: 95, Loss: 0.1957, Train: 30.39%, Valid: 25.82%, Test: 31.18%\n---\nHits@10\nRun: 02, Epoch: 100, Loss: 0.1951, Train: 25.70%, Valid: 21.52%, Test: 30.06%\nHits@20\nRun: 02, Epoch: 100, Loss: 0.1951, Train: 34.52%, Valid: 29.12%, Test: 49.36%\n---\nVal_hits@20:  [0.4954265894568092, 0.40163608986508254]\nTest_hits@20:  [0.6083272779030482, 0.48042909902688613]\nTrain_hits@20:  [0.5665560458165092, 0.46487725580384903]\nBest model val hits@20:  0.4954265894568092\nBest model test hits@20:  0.6083272779030482\nBest model train hits@20:  0.4954265894568092\nAverage best train hits@20: 0.5157166508101791; var: 0.00258464408460966\nAverage best val hits@20: 0.4485313396609458; var: 0.0021991644534164185\nAverage best test hits@20: 0.5443781884649672; var: 0.004089486039959686\n","output_type":"stream"}],"execution_count":116},{"cell_type":"code","source":"# Load GIN with random features\ngin_model = GIN(gnn_args['hidden_size'], gnn_args['hidden_size'], gnn_args['hidden_size'],\n             gnn_args['num_layers'], gnn_args['dropout']).to(device)\nload_model_ckpt(gin_model, model_name,0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T07:37:56.451672Z","iopub.execute_input":"2025-05-28T07:37:56.452131Z","iopub.status.idle":"2025-05-28T07:37:56.732529Z","shell.execute_reply.started":"2025-05-28T07:37:56.452111Z","shell.execute_reply":"2025-05-28T07:37:56.731626Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n  warnings.warn(msg)\n/usr/local/lib/python3.11/dist-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n  warnings.warn(out)\n","output_type":"stream"},{"name":"stdout","text":"Hits@10\nTrain: 32.97%, Valid: 28.20%, Test: 37.31%\nHits@20\nTrain: 46.96%, Valid: 40.57%, Test: 48.98%\n","output_type":"stream"}],"execution_count":117},{"cell_type":"code","source":"upload_file(\n    path_or_fileobj=\"/kaggle/working/training_outputs/gin_rand_feat.pt\",\n    path_in_repo=\"gin_rand_feat.pt\",\n    repo_id=\"Rzoro/ogb_ddi\",\n    repo_type=\"model\"\n)\nupload_file(\n    path_or_fileobj=\"/kaggle/working/training_outputs/gin_rand_feat_final_emb_0.pt\",\n    path_in_repo=\"gin_rand_feat_final_emb_0.pt\",\n    repo_id=\"Rzoro/ogb_ddi\",\n    repo_type=\"model\"\n)\nupload_file(\n    path_or_fileobj=\"/kaggle/working/training_outputs/gin_rand_feat_init_emb.pt\",\n    path_in_repo=\"gin_rand_feat_init_emb.pt\",\n    repo_id=\"Rzoro/ogb_ddi\",\n    repo_type=\"model\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T07:38:50.518445Z","iopub.execute_input":"2025-05-28T07:38:50.518957Z","iopub.status.idle":"2025-05-28T07:38:58.485198Z","shell.execute_reply.started":"2025-05-28T07:38:50.518934Z","shell.execute_reply":"2025-05-28T07:38:58.484557Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"gin_rand_feat.pt:   0%|          | 0.00/12.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79b4d3c7a8804d69b914b8ea4c7e399e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"gin_rand_feat_final_emb_0.pt:   0%|          | 0.00/4.37M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98f9e7ac98604dc29bb35f95e35b93d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"gin_rand_feat_init_emb.pt:   0%|          | 0.00/4.37M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0a469d333974a939c479b4682e1f93a"}},"metadata":{}},{"execution_count":118,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/Rzoro/ogb_ddi/commit/7d8f494bde487f391c7290bffc32922bdf47c25f', commit_message='Upload gin_rand_feat_init_emb.pt with huggingface_hub', commit_description='', oid='7d8f494bde487f391c7290bffc32922bdf47c25f', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Rzoro/ogb_ddi', endpoint='https://huggingface.co', repo_type='model', repo_id='Rzoro/ogb_ddi'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":118},{"cell_type":"code","source":"# Train GIN with random features\nmodel_name = \"gin_node2vec_feat_256\"\ngin_model2 = GIN(gnn_args['hidden_size'], gnn_args['hidden_size'], gnn_args['hidden_size'],\n             gnn_args['num_layers'], gnn_args['dropout']).to(device)\n\ngin_predictor2 = LinkPredictor(gnn_args['hidden_size'], gnn_args['hidden_size'], 1,\n                          gnn_args['num_layers'], gnn_args['dropout']).to(device)\ngin_emb_node2vec = torch.nn.Embedding(dataset.data.num_nodes, gnn_args['hidden_size']).to(device)\ngin_emb_node2vec.weight.data.copy_(pretrained_weight)\ntrain_model(gin_model2, gin_emb_node2vec, gnn_args, gin_predictor2, model_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T07:40:36.495272Z","iopub.execute_input":"2025-05-28T07:40:36.495764Z","iopub.status.idle":"2025-05-28T07:47:27.678095Z","shell.execute_reply.started":"2025-05-28T07:40:36.495741Z","shell.execute_reply":"2025-05-28T07:47:27.677366Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n  warnings.warn(out)\n","output_type":"stream"},{"name":"stdout","text":"Hits@10\nRun: 01, Epoch: 05, Loss: 0.5503, Train: 12.04%, Valid: 10.52%, Test: 2.53%\nHits@20\nRun: 01, Epoch: 05, Loss: 0.5503, Train: 15.44%, Valid: 13.51%, Test: 3.09%\n---\nHits@10\nRun: 01, Epoch: 10, Loss: 0.4696, Train: 11.89%, Valid: 10.27%, Test: 7.60%\nHits@20\nRun: 01, Epoch: 10, Loss: 0.4696, Train: 15.87%, Valid: 13.85%, Test: 10.18%\n---\nHits@10\nRun: 01, Epoch: 15, Loss: 0.4036, Train: 15.58%, Valid: 13.39%, Test: 7.53%\nHits@20\nRun: 01, Epoch: 15, Loss: 0.4036, Train: 22.08%, Valid: 19.30%, Test: 11.30%\n---\nHits@10\nRun: 01, Epoch: 20, Loss: 0.3693, Train: 11.24%, Valid: 9.67%, Test: 20.48%\nHits@20\nRun: 01, Epoch: 20, Loss: 0.3693, Train: 15.69%, Valid: 13.68%, Test: 24.67%\n---\nHits@10\nRun: 01, Epoch: 25, Loss: 0.3399, Train: 28.28%, Valid: 25.04%, Test: 9.08%\nHits@20\nRun: 01, Epoch: 25, Loss: 0.3399, Train: 34.22%, Valid: 30.49%, Test: 17.62%\n---\nHits@10\nRun: 01, Epoch: 30, Loss: 0.3153, Train: 27.32%, Valid: 23.75%, Test: 21.96%\nHits@20\nRun: 01, Epoch: 30, Loss: 0.3153, Train: 43.37%, Valid: 38.20%, Test: 36.51%\n---\nHits@10\nRun: 01, Epoch: 35, Loss: 0.3023, Train: 19.23%, Valid: 16.25%, Test: 6.23%\nHits@20\nRun: 01, Epoch: 35, Loss: 0.3023, Train: 25.26%, Valid: 21.64%, Test: 13.14%\n---\nHits@10\nRun: 01, Epoch: 40, Loss: 0.2760, Train: 3.79%, Valid: 3.03%, Test: 5.66%\nHits@20\nRun: 01, Epoch: 40, Loss: 0.2760, Train: 9.37%, Valid: 7.68%, Test: 10.10%\n---\nHits@10\nRun: 01, Epoch: 45, Loss: 0.2562, Train: 18.18%, Valid: 15.54%, Test: 20.84%\nHits@20\nRun: 01, Epoch: 45, Loss: 0.2562, Train: 25.53%, Valid: 22.10%, Test: 26.74%\n---\nHits@10\nRun: 01, Epoch: 50, Loss: 0.2554, Train: 20.17%, Valid: 17.27%, Test: 23.83%\nHits@20\nRun: 01, Epoch: 50, Loss: 0.2554, Train: 34.62%, Valid: 30.26%, Test: 31.07%\n---\nHits@10\nRun: 01, Epoch: 55, Loss: 0.2371, Train: 25.69%, Valid: 21.63%, Test: 19.52%\nHits@20\nRun: 01, Epoch: 55, Loss: 0.2371, Train: 35.36%, Valid: 30.23%, Test: 30.68%\n---\nHits@10\nRun: 01, Epoch: 60, Loss: 0.2271, Train: 18.51%, Valid: 15.62%, Test: 20.46%\nHits@20\nRun: 01, Epoch: 60, Loss: 0.2271, Train: 35.47%, Valid: 30.42%, Test: 35.02%\n---\nHits@10\nRun: 01, Epoch: 65, Loss: 0.2252, Train: 17.18%, Valid: 14.68%, Test: 25.63%\nHits@20\nRun: 01, Epoch: 65, Loss: 0.2252, Train: 25.42%, Valid: 22.04%, Test: 29.35%\n---\nHits@10\nRun: 01, Epoch: 70, Loss: 0.2173, Train: 9.18%, Valid: 7.42%, Test: 14.20%\nHits@20\nRun: 01, Epoch: 70, Loss: 0.2173, Train: 26.97%, Valid: 22.80%, Test: 22.03%\n---\nHits@10\nRun: 01, Epoch: 75, Loss: 0.2125, Train: 29.84%, Valid: 25.27%, Test: 13.13%\nHits@20\nRun: 01, Epoch: 75, Loss: 0.2125, Train: 43.33%, Valid: 37.36%, Test: 23.23%\n---\nHits@10\nRun: 01, Epoch: 80, Loss: 0.2075, Train: 19.01%, Valid: 15.99%, Test: 11.12%\nHits@20\nRun: 01, Epoch: 80, Loss: 0.2075, Train: 26.27%, Valid: 22.26%, Test: 25.47%\n---\nHits@10\nRun: 01, Epoch: 85, Loss: 0.2048, Train: 19.75%, Valid: 16.43%, Test: 21.18%\nHits@20\nRun: 01, Epoch: 85, Loss: 0.2048, Train: 39.80%, Valid: 33.96%, Test: 38.83%\n---\nHits@10\nRun: 01, Epoch: 90, Loss: 0.2029, Train: 21.50%, Valid: 18.07%, Test: 33.06%\nHits@20\nRun: 01, Epoch: 90, Loss: 0.2029, Train: 35.38%, Valid: 30.17%, Test: 39.22%\n---\nHits@10\nRun: 01, Epoch: 95, Loss: 0.1996, Train: 28.49%, Valid: 24.08%, Test: 17.12%\nHits@20\nRun: 01, Epoch: 95, Loss: 0.1996, Train: 42.07%, Valid: 36.11%, Test: 25.50%\n---\nHits@10\nRun: 01, Epoch: 100, Loss: 0.1968, Train: 40.18%, Valid: 34.69%, Test: 38.76%\nHits@20\nRun: 01, Epoch: 100, Loss: 0.1968, Train: 47.97%, Valid: 41.64%, Test: 46.95%\n---\nHits@10\nRun: 02, Epoch: 05, Loss: 0.5181, Train: 19.57%, Valid: 17.42%, Test: 15.24%\nHits@20\nRun: 02, Epoch: 05, Loss: 0.5181, Train: 25.51%, Valid: 22.89%, Test: 19.01%\n---\nHits@10\nRun: 02, Epoch: 10, Loss: 0.4442, Train: 5.12%, Valid: 4.21%, Test: 3.72%\nHits@20\nRun: 02, Epoch: 10, Loss: 0.4442, Train: 8.84%, Valid: 7.47%, Test: 5.70%\n---\nHits@10\nRun: 02, Epoch: 15, Loss: 0.3846, Train: 11.97%, Valid: 10.16%, Test: 11.54%\nHits@20\nRun: 02, Epoch: 15, Loss: 0.3846, Train: 19.90%, Valid: 17.25%, Test: 14.83%\n---\nHits@10\nRun: 02, Epoch: 20, Loss: 0.3490, Train: 20.78%, Valid: 17.75%, Test: 18.58%\nHits@20\nRun: 02, Epoch: 20, Loss: 0.3490, Train: 31.68%, Valid: 27.81%, Test: 28.50%\n---\nHits@10\nRun: 02, Epoch: 25, Loss: 0.3272, Train: 11.80%, Valid: 9.92%, Test: 17.42%\nHits@20\nRun: 02, Epoch: 25, Loss: 0.3272, Train: 15.94%, Valid: 13.67%, Test: 22.56%\n---\nHits@10\nRun: 02, Epoch: 30, Loss: 0.2959, Train: 17.78%, Valid: 15.33%, Test: 5.85%\nHits@20\nRun: 02, Epoch: 30, Loss: 0.2959, Train: 22.90%, Valid: 19.93%, Test: 9.34%\n---\nHits@10\nRun: 02, Epoch: 35, Loss: 0.2691, Train: 22.14%, Valid: 19.00%, Test: 13.71%\nHits@20\nRun: 02, Epoch: 35, Loss: 0.2691, Train: 29.13%, Valid: 25.23%, Test: 18.78%\n---\nHits@10\nRun: 02, Epoch: 40, Loss: 0.2606, Train: 16.99%, Valid: 14.74%, Test: 8.35%\nHits@20\nRun: 02, Epoch: 40, Loss: 0.2606, Train: 26.64%, Valid: 23.34%, Test: 11.71%\n---\nHits@10\nRun: 02, Epoch: 45, Loss: 0.2411, Train: 20.44%, Valid: 17.20%, Test: 23.15%\nHits@20\nRun: 02, Epoch: 45, Loss: 0.2411, Train: 28.05%, Valid: 24.05%, Test: 29.93%\n---\nHits@10\nRun: 02, Epoch: 50, Loss: 0.2331, Train: 13.98%, Valid: 11.68%, Test: 20.93%\nHits@20\nRun: 02, Epoch: 50, Loss: 0.2331, Train: 24.28%, Valid: 20.49%, Test: 26.89%\n---\nHits@10\nRun: 02, Epoch: 55, Loss: 0.2261, Train: 18.20%, Valid: 15.24%, Test: 11.48%\nHits@20\nRun: 02, Epoch: 55, Loss: 0.2261, Train: 29.38%, Valid: 25.06%, Test: 17.80%\n---\nHits@10\nRun: 02, Epoch: 60, Loss: 0.2212, Train: 21.44%, Valid: 18.31%, Test: 18.68%\nHits@20\nRun: 02, Epoch: 60, Loss: 0.2212, Train: 31.18%, Valid: 26.99%, Test: 25.13%\n---\nHits@10\nRun: 02, Epoch: 65, Loss: 0.2189, Train: 28.26%, Valid: 23.89%, Test: 21.22%\nHits@20\nRun: 02, Epoch: 65, Loss: 0.2189, Train: 40.85%, Valid: 34.92%, Test: 32.16%\n---\nHits@10\nRun: 02, Epoch: 70, Loss: 0.2090, Train: 27.77%, Valid: 23.57%, Test: 19.00%\nHits@20\nRun: 02, Epoch: 70, Loss: 0.2090, Train: 43.87%, Valid: 37.44%, Test: 37.00%\n---\nHits@10\nRun: 02, Epoch: 75, Loss: 0.2058, Train: 24.28%, Valid: 20.43%, Test: 17.33%\nHits@20\nRun: 02, Epoch: 75, Loss: 0.2058, Train: 41.09%, Valid: 35.39%, Test: 30.06%\n---\nHits@10\nRun: 02, Epoch: 80, Loss: 0.2027, Train: 35.14%, Valid: 29.76%, Test: 17.83%\nHits@20\nRun: 02, Epoch: 80, Loss: 0.2027, Train: 45.61%, Valid: 39.08%, Test: 25.84%\n---\nHits@10\nRun: 02, Epoch: 85, Loss: 0.1999, Train: 27.77%, Valid: 23.93%, Test: 24.47%\nHits@20\nRun: 02, Epoch: 85, Loss: 0.1999, Train: 42.23%, Valid: 36.58%, Test: 33.67%\n---\nHits@10\nRun: 02, Epoch: 90, Loss: 0.1959, Train: 27.97%, Valid: 23.52%, Test: 23.09%\nHits@20\nRun: 02, Epoch: 90, Loss: 0.1959, Train: 40.40%, Valid: 34.26%, Test: 31.63%\n---\nHits@10\nRun: 02, Epoch: 95, Loss: 0.1941, Train: 20.71%, Valid: 17.48%, Test: 7.16%\nHits@20\nRun: 02, Epoch: 95, Loss: 0.1941, Train: 24.65%, Valid: 20.78%, Test: 13.81%\n---\nHits@10\nRun: 02, Epoch: 100, Loss: 0.1965, Train: 32.60%, Valid: 27.52%, Test: 29.94%\nHits@20\nRun: 02, Epoch: 100, Loss: 0.1965, Train: 49.08%, Valid: 42.25%, Test: 45.24%\n---\nVal_hits@20:  [0.41637887766033155, 0.422491740892508]\nTest_hits@20:  [0.46950685075174736, 0.4524417742285881]\nTrain_hits@20:  [0.47970244739266904, 0.4907745207470278]\nBest model val hits@20:  0.422491740892508\nBest model test hits@20:  0.4524417742285881\nBest model train hits@20:  0.422491740892508\nAverage best train hits@20: 0.48523848406984843; var: 3.064770209107542e-05\nAverage best val hits@20: 0.4194353092764198; var: 9.341774223823735e-06\nAverage best test hits@20: 0.46097431249016774; var: 7.280420918532026e-05\n","output_type":"stream"}],"execution_count":119},{"cell_type":"code","source":"# Load GIN with random features\ngin_model2 = GIN(gnn_args['hidden_size'], gnn_args['hidden_size'], gnn_args['hidden_size'],\n             gnn_args['num_layers'], gnn_args['dropout']).to(device)\nload_model_ckpt(gin_model2, model_name,0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T07:47:27.679176Z","iopub.execute_input":"2025-05-28T07:47:27.679433Z","iopub.status.idle":"2025-05-28T07:47:27.992533Z","shell.execute_reply.started":"2025-05-28T07:47:27.679415Z","shell.execute_reply":"2025-05-28T07:47:27.991884Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n  warnings.warn(msg)\n/usr/local/lib/python3.11/dist-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n  warnings.warn(out)\n","output_type":"stream"},{"name":"stdout","text":"Hits@10\nTrain: 35.26%, Valid: 29.80%, Test: 30.34%\nHits@20\nTrain: 49.21%, Valid: 42.34%, Test: 46.49%\n","output_type":"stream"}],"execution_count":120},{"cell_type":"code","source":"upload_file(\n    path_or_fileobj=\"/kaggle/working/training_outputs/gin_node2vec_feat_256.pt\",\n    path_in_repo=\"gin_node2vec_feat_256.pt\",\n    repo_id=\"Rzoro/ogb_ddi\",\n    repo_type=\"model\"\n)\nupload_file(\n    path_or_fileobj=\"/kaggle/working/training_outputs/gin_node2vec_feat_256_final_emb_0.pt\",\n    path_in_repo=\"gin_node2vec_feat_256_final_emb_0.pt\",\n    repo_id=\"Rzoro/ogb_ddi\",\n    repo_type=\"model\"\n)\nupload_file(\n    path_or_fileobj=\"/kaggle/working/training_outputs/gin_node2vec_feat_256_init_emb.pt\",\n    path_in_repo=\"gin_node2vec_feat_256_init_emb.pt\",\n    repo_id=\"Rzoro/ogb_ddi\",\n    repo_type=\"model\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T07:48:22.049192Z","iopub.execute_input":"2025-05-28T07:48:22.049484Z","iopub.status.idle":"2025-05-28T07:48:29.942502Z","shell.execute_reply.started":"2025-05-28T07:48:22.049466Z","shell.execute_reply":"2025-05-28T07:48:29.941829Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"gin_node2vec_feat_256.pt:   0%|          | 0.00/12.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8dc6cb05e3c242ce9dbbb61f68053d10"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"gin_node2vec_feat_256_final_emb_0.pt:   0%|          | 0.00/4.37M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a37c169b0923465b80906bc107e0015a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"gin_node2vec_feat_256_init_emb.pt:   0%|          | 0.00/4.37M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b14868fadd9d49c190d0bc8d527dc19c"}},"metadata":{}},{"execution_count":121,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/Rzoro/ogb_ddi/commit/73c5c4f0b4718e53e29226ecf164d2aeddd832a6', commit_message='Upload gin_node2vec_feat_256_init_emb.pt with huggingface_hub', commit_description='', oid='73c5c4f0b4718e53e29226ecf164d2aeddd832a6', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Rzoro/ogb_ddi', endpoint='https://huggingface.co', repo_type='model', repo_id='Rzoro/ogb_ddi'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":121},{"cell_type":"markdown","source":"# Graph Attention Network","metadata":{}},{"cell_type":"code","source":"from torch_geometric.nn import GATConv\n# class using PyG's GATConv layer\nclass GAN(torch.nn.Module):\n    ''' Define graph isomorphic network. '''\n    def __init__(self, in_channels, dim, out_channels, attn_size,\n                 num_layers, dropout):\n        super(GAN, self).__init__()\n        self.in_head = attn_size\n        self.out_head = 1\n        self.dropout = dropout\n        # Initialize 2 GATConv layers\n        self.conv1 = GATConv(in_channels, dim, heads=self.in_head,\n                             dropout=self.dropout)\n        self.conv2 = GATConv(dim*self.in_head, out_channels, concat=False,\n                             heads=self.out_head, dropout=self.dropout)\n\n\n    def reset_parameters(self):\n        self.conv1.reset_parameters()\n        self.conv2.reset_parameters()\n\n\n    def forward(self, x, adj_t):\n        # Execute conv -> relu -> dropout sequence\n        x = F.dropout(x, p=self.dropout, training=self.training)\n        x = self.conv1(x, adj_t)\n        x = F.relu(x)\n        x = F.dropout(x, p=self.dropout, training=self.training)\n        x = self.conv2(x, adj_t)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T07:05:02.332644Z","iopub.execute_input":"2025-05-28T07:05:02.333314Z","iopub.status.idle":"2025-05-28T07:05:02.339389Z","shell.execute_reply.started":"2025-05-28T07:05:02.333289Z","shell.execute_reply":"2025-05-28T07:05:02.338569Z"}},"outputs":[],"execution_count":102},{"cell_type":"code","source":"class GAT(torch.nn.Module):\n    '''Define GAT network using PyG's GATConv layer.'''\n    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n                dropout=0.5, heads=1):\n        super(GAT, self).__init__()\n\n        self.convs = torch.nn.ModuleList()\n        self.convs.append(GATConv(in_channels, hidden_channels, heads=heads, dropout=dropout))\n        \n        for _ in range(num_layers - 2):\n            self.convs.append(\n                GATConv(hidden_channels * heads, hidden_channels, heads=heads, dropout=dropout)\n            )\n        \n        self.convs.append(GATConv(hidden_channels * heads, out_channels, heads=1, concat=False, dropout=dropout))\n\n        self.dropout = dropout\n\n    def reset_parameters(self):\n        for conv in self.convs:\n            conv.reset_parameters()\n\n    def forward(self, x, edge_index):\n        for conv in self.convs[:-1]:\n            x = conv(x, edge_index)\n            x = F.elu(x)\n            x = F.dropout(x, p=self.dropout, training=self.training)\n        x = self.convs[-1](x, edge_index)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T07:56:55.698265Z","iopub.execute_input":"2025-05-28T07:56:55.698531Z","iopub.status.idle":"2025-05-28T07:56:55.705504Z","shell.execute_reply.started":"2025-05-28T07:56:55.698511Z","shell.execute_reply":"2025-05-28T07:56:55.704950Z"}},"outputs":[],"execution_count":132},{"cell_type":"code","source":"os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True \"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T07:58:18.304074Z","iopub.execute_input":"2025-05-28T07:58:18.304667Z","iopub.status.idle":"2025-05-28T07:58:18.307462Z","shell.execute_reply.started":"2025-05-28T07:58:18.304644Z","shell.execute_reply":"2025-05-28T07:58:18.306813Z"}},"outputs":[],"execution_count":137},{"cell_type":"code","source":"# TRAIN gcn with random features\nmodel_name = 'gan_rand_feat'\ngan_model = GAT(in_channels = gnn_args['hidden_size'],hidden_channels = 32,out_channels = gnn_args['hidden_size'],\n              num_layers = gnn_args['num_layers'], dropout = gnn_args['dropout'], heads = 1).to(device)\ngan_predictor = LinkPredictor(gnn_args['hidden_size'], gnn_args['hidden_size'], 1,\n                          gnn_args['num_layers'], gnn_args['dropout']).to(device)\ngan_emb_rand = torch.nn.Embedding(dataset.data.num_nodes, gnn_args['hidden_size']).to(device)\ntrain_model(gan_model, gan_emb_rand, gnn_args, gan_predictor, model_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T07:58:20.054350Z","iopub.execute_input":"2025-05-28T07:58:20.055025Z","iopub.status.idle":"2025-05-28T08:24:25.506314Z","shell.execute_reply.started":"2025-05-28T07:58:20.055000Z","shell.execute_reply":"2025-05-28T08:24:25.505360Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n  warnings.warn(msg)\n/usr/local/lib/python3.11/dist-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n  warnings.warn(out)\n","output_type":"stream"},{"name":"stdout","text":"Hits@10\nRun: 01, Epoch: 05, Loss: 0.3521, Train: 25.70%, Valid: 23.26%, Test: 0.96%\nHits@20\nRun: 01, Epoch: 05, Loss: 0.3521, Train: 35.73%, Valid: 32.73%, Test: 10.85%\n---\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/3774775010.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                           gnn_args['num_layers'], gnn_args['dropout']).to(device)\n\u001b[1;32m      7\u001b[0m \u001b[0mgan_emb_rand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_nodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgnn_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hidden_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgan_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgan_emb_rand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgnn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgan_predictor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_35/1011019181.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, emb, gnn_args, predictor, model_name)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgnn_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         loss = train(model, predictor, emb.weight, adj_t, split_edge,\n\u001b[0m\u001b[1;32m     21\u001b[0m                       optimizer, gnn_args['batch_size'])\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_35/2376493552.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, predictor, x, adj_t, split_edge, optimizer, batch_size)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mnum_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mtotal_examples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnum_examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":138},{"cell_type":"code","source":"# # TRAIN gcn with random features\n# model_name = 'gan_rand_feat'\n# gan_model = GAN(gnn_args['hidden_size'], gnn_args['hidden_size'], gnn_args['hidden_size'],gnn_args['attn_size'],\n#               gnn_args['num_layers'], gnn_args['dropout']).to(device)\n# gan_predictor = LinkPredictor(gnn_args['hidden_size'], gnn_args['hidden_size'], 1,\n#                           gnn_args['num_layers'], gnn_args['dropout']).to(device)\n# gan_emb_rand = torch.nn.Embedding(dataset.data.num_nodes, gnn_args['hidden_size']).to(device)\n# train_model(gan_model, gan_emb_rand, gnn_args, gan_predictor, model_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T07:52:29.600848Z","iopub.execute_input":"2025-05-28T07:52:29.601384Z","iopub.status.idle":"2025-05-28T07:52:29.884073Z","shell.execute_reply.started":"2025-05-28T07:52:29.601362Z","shell.execute_reply":"2025-05-28T07:52:29.883061Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n  warnings.warn(msg)\n/usr/local/lib/python3.11/dist-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n  warnings.warn(out)\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/3604142798.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                           gnn_args['num_layers'], gnn_args['dropout']).to(device)\n\u001b[1;32m      7\u001b[0m \u001b[0mgan_emb_rand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_nodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgnn_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hidden_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgan_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgan_emb_rand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgnn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgan_predictor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_35/1011019181.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, emb, gnn_args, predictor, model_name)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgnn_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         loss = train(model, predictor, emb.weight, adj_t, split_edge,\n\u001b[0m\u001b[1;32m     21\u001b[0m                       optimizer, gnn_args['batch_size'])\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_35/2376493552.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, predictor, x, adj_t, split_edge, optimizer, batch_size)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0medge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos_train_edge\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mperm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_35/4204896518.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, adj_t)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# Execute conv -> relu -> dropout sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch_geometric/nn/conv/gat_conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index, edge_attr, size, return_attention_weights)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;31m# propagate_type: (x: OptPairTensor, alpha: Tensor)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpropagate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/torch_geometric.nn.conv.gat_conv_GATConv_propagate_o1vxn7wz.py\u001b[0m in \u001b[0;36mpropagate\u001b[0;34m(self, edge_index, x, alpha, size)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         kwargs = self.collect(\n\u001b[0m\u001b[1;32m    151\u001b[0m             \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/torch_geometric.nn.conv.gat_conv_GATConv_propagate_o1vxn7wz.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self, edge_index, x, alpha, size)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_x_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_x_0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0mx_j\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_x_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index_j\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mx_j\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch_geometric/nn/conv/message_passing.py\u001b[0m in \u001b[0;36m_index_select\u001b[0;34m(self, src, index)\u001b[0m\n\u001b[1;32m    265\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index_select_safe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_index_select_safe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch_geometric/nn/conv/message_passing.py\u001b[0m in \u001b[0;36m_index_select_safe\u001b[0;34m(self, src, index)\u001b[0m\n\u001b[1;32m    288\u001b[0m                     f\"your node feature matrix and try again.\")\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m     def _lift(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch_geometric/nn/conv/message_passing.py\u001b[0m in \u001b[0;36m_index_select_safe\u001b[0;34m(self, src, index)\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_index_select_safe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mIndexError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 65.31 GiB. GPU 0 has a total capacity of 14.74 GiB of which 2.84 GiB is free. Process 2558 has 11.89 GiB memory in use. Of the allocated memory 11.01 GiB is allocated by PyTorch, and 772.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 65.31 GiB. GPU 0 has a total capacity of 14.74 GiB of which 2.84 GiB is free. Process 2558 has 11.89 GiB memory in use. Of the allocated memory 11.01 GiB is allocated by PyTorch, and 772.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error"}],"execution_count":124},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}